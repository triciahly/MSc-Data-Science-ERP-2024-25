{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl3cjoU9BjrX"
      },
      "source": [
        "# **01. Import Libraries and Load Data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTheBB-Bqpd0",
        "outputId": "ee1db9ff-27d6-4831-ccc5-fce4d7f1e76e"
      },
      "outputs": [],
      "source": [
        "pip install wrds --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI5fhCBl4-ab"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wrds\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import kurtosis, skew\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqgDuMzerBMD"
      },
      "source": [
        "## Connect to WRDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtZdFaD2wkDH",
        "outputId": "fab092c1-dee7-45cd-8d1d-b6f0b69f5f0b"
      },
      "outputs": [],
      "source": [
        "# Establish a connection to the WRDS\n",
        "db = wrds.Connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ph1EzpeJJiV"
      },
      "source": [
        "# **02. Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOuv3x3sLxIJ"
      },
      "source": [
        "## Select 50 Top Stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMDOukbMN0uP"
      },
      "outputs": [],
      "source": [
        "# Get the earliest trading date for each permno\n",
        "query_earliest_date = \"\"\"\n",
        "SELECT\n",
        "    permno,\n",
        "    MIN(date) as first_trade_date\n",
        "FROM\n",
        "    crsp.dsf\n",
        "GROUP BY\n",
        "    permno\n",
        "HAVING\n",
        "    MIN(date) <= '2000-01-01'\n",
        "\"\"\"\n",
        "\n",
        "earliest_dates = db.raw_sql(query_earliest_date)\n",
        "\n",
        "# Ensure stocks are still active until December 31, 2024 (latest available date)\n",
        "query_active_stocks = \"\"\"\n",
        "SELECT\n",
        "    permno\n",
        "FROM\n",
        "    crsp.dsf\n",
        "WHERE\n",
        "    date BETWEEN '2000-01-01' AND '2024-12-31'\n",
        "GROUP BY\n",
        "    permno\n",
        "HAVING\n",
        "    COUNT(DISTINCT date) = (SELECT COUNT(DISTINCT date)\n",
        "                            FROM crsp.dsf\n",
        "                            WHERE date BETWEEN '2000-01-01' AND '2024-12-31')\n",
        "\"\"\"\n",
        "\n",
        "active_stocks = db.raw_sql(query_active_stocks)\n",
        "\n",
        "# Combine the two sets of stocks to get those listed before 2000 and still active in 2024\n",
        "filtered_permnos = earliest_dates.merge(active_stocks, on='permno', how='inner')\n",
        "\n",
        "# Get the list of permnos as a comma-separated string\n",
        "permnos_str = ','.join([str(permno) for permno in filtered_permnos['permno'].tolist()])\n",
        "\n",
        "# Get market capitalisation, company name, and sector information for IT sector\n",
        "query_main = f\"\"\"\n",
        "SELECT\n",
        "    a.permco,\n",
        "    a.permno,\n",
        "    a.date,\n",
        "    a.shrout,\n",
        "    a.prc * a.shrout as market_cap,\n",
        "    b.shrcd,\n",
        "    b.exchcd,\n",
        "    b.siccd,\n",
        "    b.ncusip,\n",
        "    b.comnam,\n",
        "    b.ticker\n",
        "FROM\n",
        "    crsp.dsf AS a\n",
        "JOIN\n",
        "    crsp.dsenames AS b\n",
        "ON\n",
        "    a.permno = b.permno\n",
        "WHERE\n",
        "    (\n",
        "        (b.siccd BETWEEN 3570 AND 3579) OR  -- IT-related services (programming, software, etc.)\n",
        "        (b.siccd BETWEEN 3600 AND 3674) OR\n",
        "        (b.siccd BETWEEN 7370 AND 7379) OR\n",
        "        (b.siccd BETWEEN 4810 AND 4813)\n",
        "    )\n",
        "    AND a.permno IN ({permnos_str})\n",
        "    AND a.date = '2024-12-31'\n",
        "    AND b.exchcd IN (1, 3)\n",
        "\"\"\"\n",
        "\n",
        "# Execute query\n",
        "crsp_data = db.raw_sql(query_main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "k73TOTxuVBiB",
        "outputId": "a0811270-be53-491b-d807-36021d8f3930"
      },
      "outputs": [],
      "source": [
        "# Check the results from crsp_data\n",
        "crsp_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WxOmABSjyUs",
        "outputId": "1c52481c-696a-47ee-dd53-6ffd135ec0cc"
      },
      "outputs": [],
      "source": [
        "print(\"Original dataset size: \", len(crsp_data))\n",
        "print(\"Original number of stocks: \", len(set(crsp_data['permno'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "akpGi0aH3GLq",
        "outputId": "eb8eb56a-cee2-4e13-aef7-7d337b7bbd92"
      },
      "outputs": [],
      "source": [
        "# Group by ticker and calculate statistics for market_cap for each stock\n",
        "ticker_stats = crsp_data.groupby('ticker')['market_cap'].describe(percentiles=[.25, .5, .75])\n",
        "\n",
        "# Calculate kurtosis and skewness for each ticker\n",
        "ticker_stats['kurtosis'] = crsp_data.groupby('ticker')['market_cap'].apply(lambda x: kurtosis(x, nan_policy='omit'))\n",
        "ticker_stats['skewness'] = crsp_data.groupby('ticker')['market_cap'].apply(lambda x: skew(x, nan_policy='omit'))\n",
        "\n",
        "# Step 2: Select and display only the desired statistics (Min, Max, Mean, STD, Kurtosis, Skewness, and Variance)\n",
        "ticker_stats['variance'] = crsp_data.groupby('ticker')['market_cap'].var()\n",
        "\n",
        "# Filter desired stats\n",
        "desired_stats = ticker_stats[['min', 'max', 'mean', 'std', 'variance', 'kurtosis', 'skewness']]\n",
        "\n",
        "# Rename columns for clarity\n",
        "desired_stats = desired_stats.rename(columns={\n",
        "    'min': 'Min',\n",
        "    'max': 'Max',\n",
        "    'mean': 'Mean',\n",
        "    'std': 'STD',\n",
        "    'variance': 'Variance',\n",
        "    'kurtosis': 'Kurtosis',\n",
        "    'skewness': 'Skewness'\n",
        "})\n",
        "\n",
        "# Get the top 50 tickers by market capitalization (mean)\n",
        "top_50_tickers = desired_stats.sort_values(by='Mean', ascending=False).head(50)\n",
        "\n",
        "# Create plots for each of the statistics\n",
        "\n",
        "# Set up the figure size for multiple plots (adjusted for 4x2 grid)\n",
        "plt.figure(figsize=(25, 25))  # Adjust the figure size to fit 7 plots\n",
        "\n",
        "# Plot for Min values\n",
        "plt.subplot(4, 2, 1)  # 4 rows, 2 columns, position 1\n",
        "sns.barplot(x=top_50_tickers.index, y=top_50_tickers['Min'], color='skyblue')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Min (Market Capitalisation)')\n",
        "\n",
        "# Plot for Max values\n",
        "plt.subplot(4, 2, 2)  # 4 rows, 2 columns, position 2\n",
        "sns.barplot(x=top_50_tickers.index, y=top_50_tickers['Max'], color='lightgreen')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Max (Market Capitalisation)')\n",
        "\n",
        "# Plot for Mean\n",
        "plt.subplot(4, 2, 3)  # 4 rows, 2 columns, position 3\n",
        "sns.barplot(x=top_50_tickers.index, y=top_50_tickers['Mean'], color='cyan')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Mean (Market Capitalisation)')\n",
        "\n",
        "# Plot for STD (Standard Deviation)\n",
        "plt.subplot(4, 2, 4)  # 4 rows, 2 columns, position 4\n",
        "sns.barplot(x=top_50_tickers.index, y=top_50_tickers['STD'], color='pink')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('STD (Market Capitalisation)')\n",
        "\n",
        "# Plot for Variance\n",
        "plt.subplot(4, 2, 5)  # 4 rows, 2 columns, position 5\n",
        "sns.barplot(x=top_50_tickers.index, y=top_50_tickers['Variance'], color='lightcoral')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Variance (Market Capitalisation)')\n",
        "\n",
        "# Plot for Skewness\n",
        "plt.subplot(4, 2, 6)  # 4 rows, 2 columns, position 6\n",
        "sns.barplot(x=top_50_tickers.index, y=top_50_tickers['skewness'], color='lightblue')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Skewness (Market Capitalisation)')\n",
        "\n",
        "# Plot for Kurtosis\n",
        "plt.subplot(4, 2, 7)  # 4 rows, 2 columns, position 7\n",
        "sns.barplot(x=top_50_tickers.index, y=top_50_tickers['kurtosis'], color='yellow')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Kurtosis (Market Capitalisation)')\n",
        "\n",
        "# Adjust layout to prevent overlap (increased padding between plots)\n",
        "plt.tight_layout(pad=5.0)  # Add more padding between subplots\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n",
        "\n",
        "# Print the top 50 stats table\n",
        "print(\"Top 50 Tickers Market Capitalisation Stats:\")\n",
        "display(top_50_tickers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkk21ag-MYPH",
        "outputId": "4b236563-abf6-4ea0-a87b-aaa6cce5f4e8"
      },
      "outputs": [],
      "source": [
        "# Filter data for the latest date\n",
        "latest_date = crsp_data['date'].max()\n",
        "latest_data = crsp_data[crsp_data['date'] == latest_date]\n",
        "\n",
        "# Group by permco and permno and select the entry with the highest market capitalisation within each group\n",
        "top_50_IT_stocks = latest_data.groupby(['permco', 'permno']).apply(lambda x: x.nlargest(1, 'market_cap'))\n",
        "\n",
        "# Sort by market capitalization and get the top 50 stocks\n",
        "top_50_IT_stocks = top_50_IT_stocks.sort_values(by='market_cap', ascending=False).head(50)\n",
        "top_50_IT_stocks.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlfVcnfoMknx",
        "outputId": "c2270e5c-2699-403e-e22d-83eb61f1ff3f"
      },
      "outputs": [],
      "source": [
        "print(top_50_IT_stocks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA-iZPmShsZg"
      },
      "source": [
        "## Word Cloud for Top 50 stocks (Tickers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3Jg3T-7hu-F",
        "outputId": "89d19144-6166-4a27-8390-1e60064ce7ee"
      },
      "outputs": [],
      "source": [
        "pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2GVKGyi-hy35",
        "outputId": "73dd4625-9182-4392-b711-d18745544749"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the tickers from the top 50 IT stocks\n",
        "tickers = top_50_IT_stocks['ticker'].dropna().tolist()  # Drop any NaN tickers\n",
        "\n",
        "# Join the tickers into a single string\n",
        "tickers_string = ' '.join(tickers)\n",
        "\n",
        "# Create the WordCloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(tickers_string)\n",
        "\n",
        "# Display the WordCloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMI8Vd3vQ5LS",
        "outputId": "ce826b48-011d-4692-9064-585d9ca7135f"
      },
      "outputs": [],
      "source": [
        "# Check for missing values in important columns\n",
        "missing_data = crsp_data[crsp_data[['market_cap', 'comnam', 'ncusip', 'ticker']].isna().any(axis=1)]\n",
        "\n",
        "# Display the rows with missing data\n",
        "print(missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHf_X0ybkTCz",
        "outputId": "b48efd28-85db-49b2-98ad-2ae72ce328bf"
      },
      "outputs": [],
      "source": [
        "# Before removing duplicates\n",
        "print(f\"Data size before removing duplicates: {crsp_data.shape}\")\n",
        "\n",
        "# Remove duplicates\n",
        "crsp_data.drop_duplicates(subset=['permno', 'date', 'date'], keep='first', inplace=True)\n",
        "\n",
        "# After removing duplicates\n",
        "print(f\"Data size after removing duplicates: {crsp_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5YSSrkHsj8z"
      },
      "source": [
        "## Collect Price and Return Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-vvDR2TQK9m"
      },
      "outputs": [],
      "source": [
        "# Get permno of the top 50 stocks\n",
        "top_50_permnos = top_50_IT_stocks['permno'].tolist()\n",
        "\n",
        "# Convert permno list to a string for the SQL IN clause\n",
        "permnos_str = ', '.join(map(str, top_50_permnos))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJfu8ceddkmV"
      },
      "source": [
        "### Download train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypUhrylXPpaH"
      },
      "outputs": [],
      "source": [
        "# Define the date range\n",
        "start_date = '2000-01-01'\n",
        "end_date = '2015-12-31'\n",
        "\n",
        "# Query to get data for the specified date range and variables for the top 50 stocks\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    a.permco,\n",
        "    a.permno,\n",
        "    b.comnam,\n",
        "    b.ticker,\n",
        "    a.date,\n",
        "    a.prc,\n",
        "    a.cfacpr,\n",
        "    a.ret\n",
        "FROM\n",
        "    crsp.dsf AS a\n",
        "JOIN\n",
        "    (SELECT permno, comnam, ticker, namedt, nameendt\n",
        "     FROM crsp.dsenames\n",
        "     WHERE permno IN ({permnos_str}) -- filter for the top 50 stocks\n",
        "       AND namedt <= '{end_date}'\n",
        "       AND (nameendt IS NULL OR nameendt >= '{start_date}')) AS b\n",
        "ON\n",
        "    a.permno = b.permno\n",
        "WHERE\n",
        "    a.permno IN ({permnos_str})     -- filter for the top 50 stocks\n",
        "    AND a.date BETWEEN '{start_date}' AND '{end_date}'\n",
        "    AND a.date >= b.namedt\n",
        "    AND (a.date <= b.nameendt OR b.nameendt IS NULL)\n",
        "\"\"\"\n",
        "\n",
        "# Execute query\n",
        "crsp_train = db.raw_sql(query)\n",
        "crsp_train.sort_values(by=['permco', 'date'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-SmkYRGSk05",
        "outputId": "cd915fdc-c581-4807-fc3a-8a853bbc013a"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(crsp_train.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNu2PQioW6xX"
      },
      "outputs": [],
      "source": [
        "# Drop rows where 'prc' or 'ret' are missing (NaN)\n",
        "crsp_train = crsp_train.dropna(subset=['prc', 'ret'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-K_ntcqTgSMe",
        "outputId": "4e1a22c1-6f6d-470a-83fe-8b25a78b8bf8"
      },
      "outputs": [],
      "source": [
        "crsp_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYbmPvkS1kbz"
      },
      "source": [
        "## Merge the risk-free rate with stock returns (calculate excess returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "X4yuJmrzE_x1",
        "outputId": "321c4ba1-49c1-4a85-e339-77a2a3fd49fa"
      },
      "outputs": [],
      "source": [
        "# Query to fetch the daily risk-free rate for the period 2000-2015\n",
        "query_risk_free = \"\"\"\n",
        "SELECT\n",
        "    date,\n",
        "    rf\n",
        "FROM\n",
        "    ff.factors_daily\n",
        "WHERE\n",
        "    date BETWEEN '2000-01-01' AND '2015-12-31'\n",
        "\"\"\"\n",
        "rf_data = db.raw_sql(query_risk_free)\n",
        "\n",
        "# Ensure both 'date' columns are in datetime format before merging\n",
        "crsp_train['date'] = pd.to_datetime(crsp_train['date'], errors='coerce')\n",
        "rf_data['date'] = pd.to_datetime(rf_data['date'], errors='coerce')\n",
        "\n",
        "# Merge the risk-free rate with stock data\n",
        "crsp_train = pd.merge(crsp_train, rf_data, how='left', on='date')\n",
        "\n",
        "# Adjust the returns by factoring in the price adjustment factor (cfacpr)\n",
        "crsp_train['adjusted_ret'] = crsp_train['ret'] / crsp_train['cfacpr']\n",
        "\n",
        "# Calculate excess returns using the adjusted returns\n",
        "crsp_train['excess_ret'] = crsp_train['adjusted_ret'] - crsp_train['rf']\n",
        "\n",
        "# Clip abnormal returns to +100% and -100%\n",
        "crsp_train['excess_ret'] = crsp_train['excess_ret'].clip(lower=-1.0, upper=1.0)\n",
        "\n",
        "# Convert the excess return to a binary target for directional forecasting\n",
        "crsp_train['directional_target'] = np.where(crsp_train['excess_ret'] > 0, 1, 0)\n",
        "\n",
        "# Check the results for train data\n",
        "crsp_train[['permco', 'permno', 'date', 'adjusted_ret', 'excess_ret']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uecdeumUFoBt"
      },
      "source": [
        "### Download test data (2016-2024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA-6blOlFAlu"
      },
      "outputs": [],
      "source": [
        "# Define the date range\n",
        "start_date = '2016-01-01'\n",
        "end_date = '2024-12-31'\n",
        "\n",
        "# Query to get data for the specified date range and variables for the top 50 stocks\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "    a.permco,\n",
        "    a.permno,\n",
        "    b.comnam,\n",
        "    b.ticker,\n",
        "    a.date,\n",
        "    a.prc,\n",
        "    a.cfacpr,\n",
        "    a.ret\n",
        "FROM\n",
        "    crsp.dsf AS a\n",
        "JOIN\n",
        "    (SELECT permno, comnam, ticker, namedt, nameendt\n",
        "     FROM crsp.dsenames\n",
        "     WHERE permno IN ({permnos_str}) -- filter for the top 50 stocks\n",
        "       AND namedt <= '{end_date}'\n",
        "       AND (nameendt IS NULL OR nameendt >= '{start_date}')) AS b\n",
        "ON\n",
        "    a.permno = b.permno\n",
        "WHERE\n",
        "    a.permno IN ({permnos_str})       -- filter for the top 50 stocks\n",
        "    AND a.date BETWEEN '{start_date}' AND '{end_date}'\n",
        "    AND a.date >= b.namedt\n",
        "    AND (a.date <= b.nameendt OR b.nameendt IS NULL)\n",
        "\"\"\"\n",
        "# Execute query\n",
        "crsp_test = db.raw_sql(query)\n",
        "crsp_test.sort_values(by=['permco', 'date'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MfWmLFhGUHEx",
        "outputId": "9509e128-1867-43ce-8688-314634432f18"
      },
      "outputs": [],
      "source": [
        "crsp_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYyrO8_33AyO",
        "outputId": "b916df4b-100f-4d90-f68f-05b444a0e279"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(crsp_test.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km8AIbFrnBS8"
      },
      "source": [
        "### Calculate Excess Returns for Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2wdXWmdQnAcr",
        "outputId": "3201b064-c6f8-43de-f7d6-9f4e59747a5f"
      },
      "outputs": [],
      "source": [
        "# Use the Fama French data to get the daily risk-free rate for the test period (2016-2024)\n",
        "query_risk_free_test = \"\"\"\n",
        "SELECT\n",
        "    date,\n",
        "    rf\n",
        "FROM\n",
        "    ff.factors_daily\n",
        "WHERE\n",
        "    date BETWEEN '2016-01-01' AND '2024-12-31'\n",
        "\"\"\"\n",
        "rf_data_test = db.raw_sql(query_risk_free_test)\n",
        "\n",
        "# Merge risk-free rate with test data\n",
        "crsp_test['date'] = pd.to_datetime(crsp_test['date'], errors='coerce')\n",
        "rf_data_test['date'] = pd.to_datetime(rf_data_test['date'], errors='coerce')\n",
        "\n",
        "# Merge the test data with the risk-free rate data\n",
        "crsp_test = pd.merge(crsp_test, rf_data_test, how='left', on='date')\n",
        "\n",
        "# Adjust the returns by factoring in the price adjustment factor (cfacpr)\n",
        "crsp_test['adjusted_ret'] = crsp_test['ret'] / crsp_test['cfacpr']\n",
        "\n",
        "# Calculate excess returns using the adjusted returns\n",
        "crsp_test['excess_ret'] = crsp_test['adjusted_ret'] - crsp_test['rf']\n",
        "\n",
        "# Clip abnormal returns to +100% and -100%\n",
        "crsp_test['excess_ret'] = crsp_test['excess_ret'].clip(lower=-1.0, upper=1.0)\n",
        "\n",
        "# Convert the excess return to a binary target for directional forecasting\n",
        "crsp_test['directional_target'] = np.where(crsp_test['excess_ret'] > 0, 1, 0)\n",
        "\n",
        "# Check the results for test data\n",
        "crsp_test[['permco', 'permno', 'date', 'adjusted_ret', 'excess_ret']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEcnZmbkpE_T"
      },
      "source": [
        "This is because the risk-free rate (rf) is very close to zero around those years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcOzD8ynMhaA"
      },
      "source": [
        "## Descriptive Statistics for Excess Returns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cauk1tPXKOtX",
        "outputId": "f2f91364-cdbf-48fc-9ed5-efbd7298b323"
      },
      "outputs": [],
      "source": [
        "# Calculate descriptive statistics for excess returns in the training dataset\n",
        "in_sample_stats = crsp_train[\"excess_ret\"].describe()\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "skewness = stats.skew(crsp_train[\"excess_ret\"])\n",
        "kurtosis = stats.kurtosis(crsp_train[\"excess_ret\"])\n",
        "\n",
        "# Print the statistics in the desired format\n",
        "print(\"In-Sample Excess Return Stats:\")\n",
        "print(in_sample_stats)\n",
        "\n",
        "# Print skewness and kurtosis\n",
        "print(f\"Skewness: {skewness:.4f}\")\n",
        "print(f\"Kurtosis: {kurtosis:.4f}\")\n",
        "\n",
        "# Display the dtype\n",
        "print(f\"Name: excess_ret, dtype: {crsp_train['excess_ret'].dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQWsArAJKxRx",
        "outputId": "f0406e4d-bf1a-42a3-ab4f-c26f21da1047"
      },
      "outputs": [],
      "source": [
        "# Calculate descriptive statistics for excess returns in the testing dataset\n",
        "out_sample_stats = crsp_test[\"excess_ret\"].describe()\n",
        "\n",
        "# Calculate skewness and kurtosis\n",
        "skewness = stats.skew(crsp_test[\"excess_ret\"])\n",
        "kurtosis = stats.kurtosis(crsp_test[\"excess_ret\"])\n",
        "\n",
        "# Print the statistics in the desired format\n",
        "print(\"Out-Sample Excess Return Stats:\")\n",
        "print(out_sample_stats)\n",
        "\n",
        "# Print skewness and kurtosis\n",
        "print(f\"Skewness: {skewness:.4f}\")\n",
        "print(f\"Kurtosis: {kurtosis:.4f}\")\n",
        "\n",
        "# Display the dtype\n",
        "print(f\"Name: excess_ret, dtype: {crsp_test['excess_ret'].dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuvJLW92MgQy",
        "outputId": "5125d0d1-9a41-4298-abfe-61c6e848ab66"
      },
      "outputs": [],
      "source": [
        "train_stats = crsp_train.groupby('permno')['excess_ret'].describe()\n",
        "test_stats = crsp_test.groupby('permno')['excess_ret'].describe()\n",
        "\n",
        "# Print descriptive statistics\n",
        "print(\"Descriptive Statistics for Excess Returns (Training Period):\")\n",
        "print(train_stats)\n",
        "\n",
        "print(\"\\nDescriptive Statistics for Excess Returns (Test Period):\")\n",
        "print(test_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k19gsrJPAZZu"
      },
      "source": [
        "## Create Rolling Windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpbEEU6ScDn3",
        "outputId": "5f4dfd5a-b1f4-4bb6-edd3-38d7f1b7db9e"
      },
      "outputs": [],
      "source": [
        "def create_lag_features(df, lags):\n",
        "    # Sort the data by stock ID ('permno') and date to ensure correct time order\n",
        "    df_sorted = df.sort_values(by=[\"permno\", \"date\"])\n",
        "\n",
        "    # Loop through each lag value provided (e.g., 5, 21, 252, 512)\n",
        "    for lag in lags:\n",
        "        # Create lag features by shifting excess returns and applying a rolling window\n",
        "        df[f\"lag_{lag}\"] = (\n",
        "            df_sorted.groupby(\"permno\")[\"excess_ret\"]  # Group by stock\n",
        "            .shift(1)  # Shift by 1 day to avoid lookahead bias\n",
        "            .rolling(window=lag, min_periods=1)  # Rolling window over past 'lag' days\n",
        "            .mean()  # Calculate the mean of the rolling window\n",
        "        )\n",
        "\n",
        "    # Return the DataFrame with added lag features\n",
        "    return df\n",
        "\n",
        "# Example usage for both crsp_train and crsp_test\n",
        "lag_days_list = [5, 21, 252, 512]  # Example list of lag days\n",
        "\n",
        "# Apply the function to both crsp_train and crsp_test\n",
        "crsp_train_lagged = create_lag_features(crsp_train, lag_days_list)\n",
        "crsp_test_lagged = create_lag_features(crsp_test, lag_days_list)\n",
        "\n",
        "# Drop rows where any of the lag columns are NaN in crsp_test_lagged\n",
        "crsp_test_lagged = crsp_test_lagged.dropna(subset=[f'lag_{lag}' for lag in lag_days_list])\n",
        "\n",
        "# Verify that the lag features are correctly added\n",
        "print(crsp_train_lagged.head())\n",
        "print(crsp_test_lagged.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **03. Linear Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jabpJqYeBNM"
      },
      "source": [
        "## OLS + H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZmFmA-4eDKm"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy (correct classification of direction)\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# OLS+Huber model to predict excess returns\n",
        "def ols_huber_model(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512], threshold=0.5):\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Hyperparameter grid for Huber\n",
        "    param_grid = {\n",
        "        'epsilon': [1.1, 1.2, 1.35, 1.5],\n",
        "        'alpha': [0.0001, 0.001, 0.01],\n",
        "        'max_iter': [100, 500, 1000]\n",
        "    }\n",
        "\n",
        "    huber = HuberRegressor()\n",
        "    grid_search = GridSearchCV(huber, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Predicting excess returns using the best model\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Convert continuous predictions to binary outcomes (0 or 1)\n",
        "    y_pred_binary = np.where(y_pred > threshold, 1, 0)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)  # Accuracy of prediction\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])  # Accuracy for 'up' direction\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])  # Accuracy for 'down' direction\n",
        "    mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error (continuous predictions)\n",
        "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
        "    mase_value = mase(y_test, y_pred)  # Mean Absolute Scaled Error\n",
        "    r2 = r2_score(y_test, y_pred)  # R-squared\n",
        "\n",
        "    # Return the results\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'OLS + H',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame to a CSV file with the correct structure\n",
        "    pred_df.to_csv(f\"ols_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test, pred_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymbuQhmYpoAe"
      },
      "source": [
        "## Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-SE4ZVsppoi"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors (absolute differences)\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Lasso Model with Multiple Features (Lags) and Tuning\n",
        "def lasso_model(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512], threshold=0.5):\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Hyperparameter tuning for Lasso\n",
        "    param_grid = {'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
        "    grid_search = GridSearchCV(Lasso(max_iter=10000), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Predicting excess returns using the best model\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Convert continuous predictions to binary outcomes (0 or 1)\n",
        "    y_pred_binary = np.where(y_pred > threshold, 1, 0)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'Lasso',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Store predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"lasso_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO8mfrXfC63P"
      },
      "source": [
        "## Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kNIRT8DC8EI"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Ridge Regression with Tuning\n",
        "def ridge_model(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "    # Prepare dataset\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Hyperparameter tuning for Ridge\n",
        "    param_grid = {'alpha': [0.01, 0.1, 1.0, 10, 100]}\n",
        "    grid_search_ridge = GridSearchCV(Ridge(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search_ridge.fit(X_train, y_train)\n",
        "\n",
        "    # Corrected: Use grid_search_ridge to get the best model\n",
        "    best_model = grid_search_ridge.best_estimator_\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'Ridge',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"ridge_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKFIgChhBf7i"
      },
      "source": [
        "## ElasticNet Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRFEiZLodI-A"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# ElasticNet Model with MSE loss and tuning\n",
        "def elasticnet_model_with_tuning(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Hyperparameter grid for ElasticNet\n",
        "    param_grid = {\n",
        "        'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
        "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 1.0]  # 1.0 = Lasso, 0 = Ridge\n",
        "    }\n",
        "\n",
        "    # Grid search for ElasticNet\n",
        "    grid_search = GridSearchCV(ElasticNet(max_iter=10000), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Predict using the best model\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Convert predictions to binary for directional accuracy\n",
        "    threshold = 0.5\n",
        "    y_pred_binary = np.where(y_pred > threshold, 1, 0)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'ElasticNet',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"enet_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj7Z6BUxBkFC"
      },
      "source": [
        "## Principal Components Regression (PCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdbGtWFvBmJt"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "def pcr_model(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "    # Prepare dataset\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Check the number of features\n",
        "    n_samples, n_features = X_train.shape\n",
        "    max_components = min(n_samples, n_features)\n",
        "\n",
        "    # If there is only one feature, avoid PCA with multiple components\n",
        "    param_grid = {'pca__n_components': [1, min(2, max_components), min(3, max_components), min(5, max_components)]}\n",
        "\n",
        "    pipeline = make_pipeline(\n",
        "        SimpleImputer(strategy='mean'),\n",
        "        PCA(),\n",
        "        LinearRegression()\n",
        "    )\n",
        "\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'PCR',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"pcr_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xABfRPS16w3"
      },
      "source": [
        "## Generalized Linear Model (GLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOtRLKk-1-dO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Generalized Linear Model (GLM)\n",
        "def glm_model(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Hyperparameter grid for GLM\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'max_iter': [100, 500, 1000]\n",
        "    }\n",
        "\n",
        "    # Logistic Regression model (GLM)\n",
        "    glm = LogisticRegression(solver='liblinear')\n",
        "    grid_search = GridSearchCV(glm, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best model from GridSearchCV\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Get best hyperparameters\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'GLM - Logistic Regression',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"glm_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SyG3edoBtib"
      },
      "source": [
        "# **04. Nonlinear Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqwUR9v2Bwhs"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAsxXsk8CAvl"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Tuning Random Forest globally\n",
        "def random_forest_model_with_tuning(X_train, y_train):\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        RandomForestRegressor(),\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=10,\n",
        "        cv=5,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train, y_train)\n",
        "    return random_search.best_estimator_\n",
        "\n",
        "# Global Random Forest Model\n",
        "def random_forest_model(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['excess_ret'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['excess_ret'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['excess_ret']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['excess_ret']\n",
        "\n",
        "    # Impute missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_imputed = imputer.fit_transform(X_train)\n",
        "    X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "    # Tune and train the model\n",
        "    model = random_forest_model_with_tuning(X_train_imputed, y_train)\n",
        "    y_pred = model.predict(X_test_imputed)\n",
        "    # Convert continuous predictions to binary outcomes (0 or 1)\n",
        "    y_pred_binary = np.where(y_pred > 0, 1, 0)\n",
        "\n",
        "    # Convert actual excess returns to binary for directional accuracy\n",
        "    y_test_binary = np.where(y_test > 0, 1, 0)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test_binary, y_pred_binary)  # Accuracy of prediction\n",
        "    up_accuracy = directional_accuracy(y_test_binary[y_test_binary == 1], y_pred_binary[y_test_binary == 1])  # Accuracy for 'up' direction\n",
        "    down_accuracy = directional_accuracy(y_test_binary[y_test_binary == 0], y_pred_binary[y_test_binary == 0])  # Accuracy for 'down' direction\n",
        "    mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error (continuous predictions)\n",
        "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
        "    mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
        "    mase_value = mase(y_test, y_pred)  # Mean Absolute Scaled Error\n",
        "    r2 = r2_score(y_test, y_pred)  # R-squared\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'Random Forest',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Add the predicted returns for portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results to a DataFrame\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,        # Stock Identifier\n",
        "        'date': df_test['date'].values,            # Date corresponding to each stock's excess return\n",
        "        'excess_ret': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                           # Predicted values\n",
        "    })\n",
        "\n",
        "    # Save the predictions to a CSV file\n",
        "    pred_df.to_csv(f\"rf_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkuK1ztgCCDX"
      },
      "source": [
        "###Gradient Boosted Regression Trees (GBRT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLru8QHeCFGq"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Utility: Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# GBRT with hyperparameter tuning (with Huber loss)\n",
        "def gbrt_model_with_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'loss': ['huber']\n",
        "    }\n",
        "    grid_search = GridSearchCV(\n",
        "        GradientBoostingRegressor(),\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring='neg_mean_squared_error'\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# Global GBRT Model\n",
        "def gbrt_model(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Tune and train\n",
        "    model = gbrt_model_with_tuning(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'GBRT',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"gbrt_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnZrU9HpCH-l"
      },
      "source": [
        "## Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsWnpSH8ilDO"
      },
      "source": [
        "### NN1 (One Hidden Layer, 32 Neurons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liqOQrXwCJTi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Define NN1 architecture\n",
        "def create_nn_1model(input_dim, learning_rate=0.0001, l2_reg=0.0001):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Global Neural Network Model (NN1)\n",
        "def neural_network_nn1(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    model = create_nn_1model(input_dim=X_train.shape[1])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=0)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'NN1',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"nn1_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiGuYw30iqU3"
      },
      "source": [
        "### NN2 (Two Hidden Layers, 32 and 16 Neurons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlZ969mWivQy"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Define NN2 architecture\n",
        "def create_nn_2model(input_dim, learning_rate=0.00001, l2_reg=0.0001):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Global Neural Network Model (NN2)\n",
        "def neural_network_nn2(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    model = create_nn_1model(input_dim=X_train.shape[1])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=0)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'NN2',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"nn2_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ3VDgrXjAtr"
      },
      "source": [
        "### NN3 (Three Hidden Layers, 32, 16, and 8 Neurons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMlwaWGyjAd1"
      },
      "outputs": [],
      "source": [
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Define NN3 architecture\n",
        "def create_nn_3model(input_dim, learning_rate=0.00001, l2_reg=0.0001):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Global Neural Network Model (NN3)\n",
        "def neural_network_nn3(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    model = create_nn_1model(input_dim=X_train.shape[1])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=0)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'NN3',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"nn3_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UoHxaNyjNR4"
      },
      "source": [
        "### NN4 (Four Hidden Layers, 32, 16, 8, and 4 Neurons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNogrLqSjTCg"
      },
      "outputs": [],
      "source": [
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Define NN4 architecture\n",
        "def create_nn_4model(input_dim, learning_rate=0.00001, l2_reg=0.0001):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(4, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Global Neural Network Model (NN4)\n",
        "def neural_network_nn4(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    model = create_nn_1model(input_dim=X_train.shape[1])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=0)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'NN4',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"nn4_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey5S0Pa3jXJA"
      },
      "source": [
        "### NN5 (Five Hidden Layers, 32, 16, 8, 4, and 2 Neurons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeOswOVevbto"
      },
      "outputs": [],
      "source": [
        "# Function to calculate MASE\n",
        "def mase(y_true, y_pred):\n",
        "    # Calculate the prediction errors\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "\n",
        "    # Naïve forecast\n",
        "    naive_errors = np.abs(np.diff(y_true))\n",
        "\n",
        "    # Calculate MASE\n",
        "    mase_value = np.mean(errors) / np.mean(naive_errors)\n",
        "    return mase_value\n",
        "\n",
        "# Directional accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Define NN1 architecture\n",
        "def create_nn_5model(input_dim, learning_rate=0.00001, l2_reg=0.0001):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(16, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(4, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(2, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Global Neural Network Model (NN5)\n",
        "def neural_network_nn5(crsp_train_lagged, crsp_test_lagged, lags=[5, 21, 252, 512]):\n",
        "\n",
        "    # Prepare combined data\n",
        "    lag_columns = [f'lag_{lag}' for lag in lags]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_columns + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_columns]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_columns]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    model = create_nn_1model(input_dim=X_train.shape[1])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32,\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=0)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Return a DataFrame with metrics\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Model': 'NN2',\n",
        "        'Lags': str(lags),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'R-squared': r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "    # Predicted returns for use in portfolio construction later\n",
        "    crsp_test_lagged.loc[crsp_test_lagged.index, 'predicted_excess_returns'] = y_pred\n",
        "\n",
        "    # Save results\n",
        "    pred_df = pd.DataFrame({\n",
        "        'permno': df_test['permno'].values,   # Stock identifier\n",
        "        'date': df_test['date'].values,       # Date corresponding to each stock's excess return\n",
        "        'y_true': df_test['excess_ret'].values,  # Actual excess returns (true values)\n",
        "        'y_pred': y_pred                        # Predicted excess returns\n",
        "    })\n",
        "\n",
        "    pred_df.to_csv(f\"nn5_results_lag{'_'.join(map(str, lags))}_full.csv\", index=False)\n",
        "\n",
        "    # Store predicted excess returns for later use in portfolio construction\n",
        "    crsp_test_lagged['predicted_excess_returns'] = y_pred\n",
        "\n",
        "    return results_df, y_pred_binary, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O3RXsKRvb49"
      },
      "source": [
        "# **05. Run Rolling Forecast**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tZy0oW4sxu"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "def rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ols', lag=5, alpha=1.0):\n",
        "    results = []\n",
        "\n",
        "    # Prepare full-dataset input\n",
        "    lag_features = [f'lag_{i}' for i in [5, 21, 252, 512]]\n",
        "    df_train = crsp_train_lagged.dropna(subset=lag_features + ['directional_target'])\n",
        "    df_test = crsp_test_lagged.dropna(subset=lag_features + ['directional_target'])\n",
        "\n",
        "    X_train = df_train[lag_features]  # Use multiple lag features\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[lag_features]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # DIRECT MODELS\n",
        "    direct_models = {\n",
        "        'ols_h': lambda: ols_huber_model(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'lasso': lambda: lasso_model(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'ridge': lambda: ridge_model(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'elasticnet': lambda: elasticnet_model_with_tuning(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'pcr': lambda: pcr_model(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'glm': lambda: glm_model(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'randomforest': lambda: random_forest_model(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'gbrt': lambda: gbrt_model(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'nn1': lambda: neural_network_nn1(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'nn2': lambda: neural_network_nn2(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'nn3': lambda: neural_network_nn3(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'nn4': lambda: neural_network_nn4(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "        'nn5': lambda: neural_network_nn5(crsp_train_lagged, crsp_test_lagged, lag),\n",
        "    }\n",
        "\n",
        "    if model_type in direct_models:\n",
        "        return direct_models[model_type]()\n",
        "\n",
        "    # Fit and predict\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_directional_accuracy = np.mean((y_pred_binary == 1) & (y_test == 1))\n",
        "    down_directional_accuracy = np.mean((y_pred_binary == 0) & (y_test == 0))\n",
        "    n = len(y_test)\n",
        "    p = X_train.shape[1]\n",
        "    r2_value = r2_score(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mase_value = mase(y_test, y_pred)\n",
        "\n",
        "    # Store result\n",
        "    results.append({\n",
        "        'Model': model_type.upper(),\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_directional_accuracy,\n",
        "        'Down Directional Accuracy': down_directional_accuracy,\n",
        "        'R-squared': adjusted_r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value,\n",
        "        'Lag': lag\n",
        "    })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib6sKn64BN2B"
      },
      "source": [
        "# **06. Rename Market Cap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On4fu85IhTqZ"
      },
      "outputs": [],
      "source": [
        "# Merge crsp_data and crsp_test_lagged on the stock ID (permno)\n",
        "merged_df = crsp_test_lagged.merge(crsp_data[['permno', 'market_cap']], on='permno', how='left')\n",
        "\n",
        "# Rename 'market_cap' in merged_df to avoid conflict during merge\n",
        "merged_df = merged_df.rename(columns={'market_cap': 'market_cap_merged'})\n",
        "\n",
        "# Merge 'market_cap' (now renamed to 'market_cap_merged') from merged_df into crsp_test_lagged based on 'permco' and 'date'\n",
        "crsp_test_lagged = crsp_test_lagged.merge(merged_df[['permco', 'date', 'market_cap_merged']], how='left', on=['permco', 'date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZk4GHFm8T2a"
      },
      "source": [
        "# **07. Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D3k98PD96Ot"
      },
      "source": [
        "## OLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "vDUnFllYAQBT",
        "outputId": "79a2ef91-6e93-4a33-8ae9-cbb2e9c2bb11"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "ols_h_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ols_h', lag=[5])\n",
        "print(\"OLS + H Results (Lag 5):\")\n",
        "display(ols_h_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "mWcEx22W-NxR",
        "outputId": "709f26b7-2fa4-4f64-fe5e-61030b5bfaa8"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "ols_h_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ols_h', lag=[21])\n",
        "print(\"OLS + H Results (Lag 21):\")\n",
        "display(ols_h_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "13cLh6MS-VCp",
        "outputId": "78490e0e-b779-4878-e4b8-28813d2fedb0"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "ols_h_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ols_h', lag=[252])\n",
        "print(\"OLS + H Results (Lag 252):\")\n",
        "display(ols_h_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "LpfIPc_APTBg",
        "outputId": "f7ed8a3c-00a8-4f47-9591-45e3181d63d8"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "ols_h_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ols_h', lag=[512])\n",
        "print(\"OLS + H Results (Lag 512):\")\n",
        "display(ols_h_lag512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "IsMWnf1j-Y31",
        "outputId": "09689d82-d9f2-4ed8-a14b-eaa14ef7e75e"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "ols_h_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ols_h', lag=[512])\n",
        "print(\"OLS + H Results (Lag 512):\")\n",
        "display(ols_h_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blP4QT7fQWny"
      },
      "source": [
        "## OLS+H Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJHH1_NCrR8u"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "Ksf7vSEucmIP",
        "outputId": "b19372e5-3e8a-44f2-e71c-63c6e7c5d13d"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "def ols_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the OLS + Huber model\n",
        "    result_df = ols_huber_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    crsp_test_lagged['ols_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using OLS+Huber model\n",
        "crsp_test_lagged = ols_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'ols_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'ols_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_ols5  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_ols5['date'].append(date)\n",
        "    cumulative_log_returns_by_date_ols5['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ols5['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_ols_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_ols5)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_ols_lag_5.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccl8jOBqPlQZ"
      },
      "source": [
        "#### With/ Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "faPLjhDkPIka",
        "outputId": "4e5cfd40-a2f0-4515-bbd3-780c543fc057"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_ols5_c = cumulative_log_returns_ols_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ols5_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ols5_c = pd.DataFrame(metrics)\n",
        "display(metrics_ols5_c)\n",
        "\n",
        "# Do same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_ols5_wc = cumulative_log_returns_ols_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ols5_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ols5_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_ols5_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cjLVHOSADrs"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2P3nrFsAFZP",
        "outputId": "a998dd9d-3d82-4bcf-b028-0e30fdc8f599"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `ols_huber_model` to predict excess returns\n",
        "def ols_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the OLS + Huber model\n",
        "    result_df = ols_huber_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['ols_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using OLS+Huber model\n",
        "crsp_test_lagged = ols_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'ols_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'ols_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_ols21  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_ols21['date'].append(date)\n",
        "    cumulative_log_returns_by_date_ols21['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ols21['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_ols_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_ols21)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_ols_lag_21.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQe9YCfGSs6F"
      },
      "source": [
        "#### With/ Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5C5nu67-BVZ",
        "outputId": "36b5eb69-f556-43d2-8563-c3c08284d86d"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_ols21_c = cumulative_log_returns_ols_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ols21_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ols21_c = pd.DataFrame(metrics)\n",
        "display(metrics_ols21_c)\n",
        "\n",
        "\n",
        "# Do same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_ols21_wc = cumulative_log_returns_ols_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ols21_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ols21_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_ols21_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB2UN7FCYrrC"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPr14HuYYuqN",
        "outputId": "ddf1118c-eb99-48de-b8f6-f8c5fc94eeea"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `ols_huber_model` to predict excess returns\n",
        "def ols_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the OLS + Huber model\n",
        "    result_df = ols_huber_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['ols_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using OLS+Huber model\n",
        "crsp_test_lagged = ols_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'ols_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'ols_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_ols252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_ols252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_ols252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ols252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_ols_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_ols252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_ols_lag_252.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rglENNXBZyip"
      },
      "source": [
        "#### With/ Without Trasaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEM7Jr_bZxpZ",
        "outputId": "d0b07b39-d42d-4c20-eb65-71897dd0693d"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_ols252_c = cumulative_log_returns_ols_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ols252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ols252_c = pd.DataFrame(metrics)\n",
        "display(metrics_ols252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_ols252_wc = cumulative_log_returns_ols_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ols252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ols252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_ols252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shn99eEEbuHZ"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9rU46f2bwb6",
        "outputId": "ca836364-efa2-446e-fffd-59d817844498"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `ols_huber_model` to predict excess returns\n",
        "def ols_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the OLS + Huber model\n",
        "    result_df = ols_huber_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['ols_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using OLS+Huber model\n",
        "crsp_test_lagged = ols_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'ols_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'ols_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_ols512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_ols512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_ols512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ols512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_ols_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_ols512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_ols_lag_512.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1WgV_35c6px"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEB738lbAxni",
        "outputId": "4426c411-c405-4e25-86cf-c4457a825060"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_ols512_c = cumulative_log_returns_ols_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ols512_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ols512_c = pd.DataFrame(metrics)\n",
        "display(metrics_ols512_c)\n",
        "\n",
        "\n",
        "# the same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_ols512_wc = cumulative_log_returns_ols_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ols512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ols512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_ols512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9tmsSYGAGOd"
      },
      "source": [
        "## Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "v5b5zE36AHQR",
        "outputId": "e7e8ffb5-2f1b-431e-9d06-bd67acd1fdcd"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "lasso_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='lasso', lag=[5])\n",
        "print(\"Lasso Results (Lag 5):\")\n",
        "display(lasso_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6Bw8iEWZAMxx",
        "outputId": "856f35a6-6906-4c3e-f877-af47f23a6c4d"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "lasso_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='lasso', lag=[21])\n",
        "print(\"Lasso Results (Lag 21):\")\n",
        "display(lasso_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "pyx2n5neAPdp",
        "outputId": "ed7e9c8a-5dde-4e7e-d9fd-4ad3722eda86"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "lasso_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='lasso', lag=[252])\n",
        "print(\"Lasso Results (Lag 252):\")\n",
        "display(lasso_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "-MBcXvEoASnZ",
        "outputId": "6d0b13d9-b3ce-4f1e-8b7e-5cf6f8f1bd0d"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "lasso_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='lasso', lag=[512])\n",
        "print(\"Lasso Results (Lag 512):\")\n",
        "display(lasso_lag512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JCMlYM0Kafw"
      },
      "outputs": [],
      "source": [
        "# Merge crsp_data and crsp_test_lagged on the stock ID (permno)\n",
        "merged_df = crsp_test_lagged.merge(crsp_data[['permno', 'market_cap']], on='permno', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkBrCu-yIz2J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Predefined valid lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Create an empty list to store results for each lag\n",
        "r2_results_per_lag = []\n",
        "\n",
        "# Calculate the quantiles for the market cap\n",
        "top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "# Filter top 25% and bottom 25% based on market cap\n",
        "top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "# Loop over each lag and calculate R² for each group (All, Top 25%, Bottom 25%)\n",
        "for lag in valid_lags:\n",
        "    forecast_col = f'lag_{lag}'  # Column name for the forecasted values for each lag\n",
        "    actual_col = 'excess_ret'  # Actual values column\n",
        "\n",
        "    # Check if forecast_col exists in merged_df\n",
        "    if forecast_col not in merged_df.columns:\n",
        "        print(f\"Warning: {forecast_col} not found in merged_df. Skipping lag {lag}.\")\n",
        "        continue\n",
        "\n",
        "    # Drop NaN values for both actual and forecasted columns before calculating R²\n",
        "    merged_df_clean = merged_df.dropna(subset=[forecast_col, actual_col])\n",
        "\n",
        "    # Prepare data for model fitting\n",
        "    X_train = merged_df_clean[[forecast_col]]  # Use the lag as the feature\n",
        "    y_train = merged_df_clean[actual_col]\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Fit the Lasso model with GridSearchCV for alpha tuning\n",
        "    param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 1000]}\n",
        "    grid_search = GridSearchCV(Lasso(max_iter=10000), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Make predictions for the entire dataset\n",
        "    merged_df_clean_copy = merged_df_clean.copy()  # Make a copy to avoid SettingWithCopyWarning\n",
        "    merged_df_clean_copy.loc[:, 'predicted_ret'] = best_model.predict(X_train_scaled)\n",
        "\n",
        "    # Calculate R² for All stocks (drop rows with NaN)\n",
        "    all_r2 = r2_score(merged_df_clean_copy[actual_col], merged_df_clean_copy['predicted_ret'])\n",
        "\n",
        "    # Scale and prepare data for Top 25% stocks\n",
        "    top_25_clean = top_25_stocks.dropna(subset=[forecast_col, actual_col]).copy()  # Explicit copy\n",
        "    X_top_25 = top_25_clean[[forecast_col]]\n",
        "    y_top_25 = top_25_clean[actual_col]\n",
        "    X_top_25_scaled = scaler.transform(X_top_25)  # Scale the top 25% data\n",
        "    top_25_model = Lasso(alpha=grid_search.best_params_['alpha'], max_iter=10000)\n",
        "    top_25_model.fit(X_top_25_scaled, y_top_25)\n",
        "    top_25_clean.loc[:, 'predicted_ret'] = top_25_model.predict(X_top_25_scaled)  # .loc[] to avoid warning\n",
        "    top_25_r2 = r2_score(top_25_clean[actual_col], top_25_clean['predicted_ret'])\n",
        "\n",
        "    # Scale and prepare data for Bottom 25% stocks\n",
        "    bottom_25_clean = bottom_25_stocks.dropna(subset=[forecast_col, actual_col]).copy()  # Explicit copy\n",
        "    X_bottom_25 = bottom_25_clean[[forecast_col]]\n",
        "    y_bottom_25 = bottom_25_clean[actual_col]\n",
        "    X_bottom_25_scaled = scaler.transform(X_bottom_25)  # Scale the bottom 25% data\n",
        "    bottom_25_model = Lasso(alpha=grid_search.best_params_['alpha'], max_iter=10000)\n",
        "    bottom_25_model.fit(X_bottom_25_scaled, y_bottom_25)\n",
        "    bottom_25_clean.loc[:, 'predicted_ret'] = bottom_25_model.predict(X_bottom_25_scaled)  # .loc[] to avoid warning\n",
        "    bottom_25_r2 = r2_score(bottom_25_clean[actual_col], bottom_25_clean['predicted_ret'])\n",
        "\n",
        "    # Store the results in the list\n",
        "    r2_results_per_lag.append({\n",
        "        'Lag': lag,\n",
        "        'All Stocks R²': all_r2,\n",
        "        'Top 25% R²': top_25_r2,\n",
        "        'Bottom 25% R²': bottom_25_r2\n",
        "    })\n",
        "\n",
        "# Convert the results into a DataFrame\n",
        "r2_results_df = pd.DataFrame(r2_results_per_lag)\n",
        "\n",
        "# Display the results\n",
        "display(r2_results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwtHNmY4RdOu"
      },
      "source": [
        "## Lasso Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbi-6Dc__3fS"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uCLvbgvoMnG",
        "outputId": "bf9a3046-d23a-4269-c988-2bebed3deafa"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `lasso` to predict excess returns\n",
        "def lasso_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the Lasso model\n",
        "    result_df = lasso_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values  # Ensure that the column 'predicted_excess_returns' exists\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['lasso_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Lasso model\n",
        "crsp_test_lagged = lasso_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'lasso_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'lasso_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_lasso5  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_lasso5['date'].append(date)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso5['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_lasso_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_lasso5)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_lasso_lag_5.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6PLQ3c-TKjV"
      },
      "source": [
        "#### With/ Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE0QRtMzTMe8",
        "outputId": "4dcbc07f-9904-4537-f801-73a0f89bbbed"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_lasso5_c = cumulative_log_returns_lasso_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_lasso5_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_lasso5_c = pd.DataFrame(metrics)\n",
        "display(metrics_lasso5_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_lasso5_wc = cumulative_log_returns_lasso_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_lasso5_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_lasso5_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_lasso5_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nh3aEIZVn1F"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2AGCI9dVpCm",
        "outputId": "3287b1a5-fed8-4eb2-f844-b3dd37b73296"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `lasso` to predict excess returns\n",
        "def lasso_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the Lasso model\n",
        "    result_df = lasso_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['lasso_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Lasso model (this should generate binary outcomes)\n",
        "crsp_test_lagged = lasso_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'lasso_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'lasso_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_lasso21  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_lasso21['date'].append(date)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso21['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_lasso_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_lasso21)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_lasso_lag_21.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tl32xPQXvxb"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4U9Vf4zXzHd",
        "outputId": "640cd6d7-1f98-4e83-ef3a-01c68cb39506"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_lasso21_c = cumulative_log_returns_lasso_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_lasso21_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_lasso21_c = pd.DataFrame(metrics)\n",
        "display(metrics_lasso21_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_lasso21_wc = cumulative_log_returns_lasso_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_lasso21_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_lasso21_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_lasso21_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advmQ-j5YMg8"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw-OhHJ4YOGF",
        "outputId": "146cae2e-ab92-4019-ffbc-084d2509648b"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `lasso` to predict excess returns\n",
        "def lasso_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the Lasso model\n",
        "    result_df = lasso_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['lasso_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Lasso model (this should generate binary outcomes)\n",
        "crsp_test_lagged = lasso_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'lasso_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'lasso_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_lasso252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_lasso252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_lasso_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_lasso252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_lasso_lag_252.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPSuKW32aPRm"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwPMr7qnaRVA",
        "outputId": "217fddd6-207a-4238-af06-06e188e8f617"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_lasso252_c = cumulative_log_returns_lasso_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_lasso252_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_lasso252_c = pd.DataFrame(metrics)\n",
        "display(metrics_lasso252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_lasso252_wc = cumulative_log_returns_lasso_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_lasso252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_lasso252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_lasso252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocdCdCqpb6VA"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMuviLYub8Yu",
        "outputId": "628bfb75-3a4c-4ef4-aaa3-127d0bab2156"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `lasso` to predict excess returns\n",
        "def lasso_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the Lasso model\n",
        "    result_df = lasso_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['lasso_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Lasso model (this should generate binary outcomes)\n",
        "crsp_test_lagged = lasso_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'lasso_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'lasso_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_lasso512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_lasso512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_lasso512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_lasso_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_lasso512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_lasso_lag_512.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iB0K3FfcVPm"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN8882hHcXtx",
        "outputId": "41ea5f13-a566-46b1-f374-0f1ea477a72f"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_lasso512_c = cumulative_log_returns_lasso_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_lasso512_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_lasso512_c = pd.DataFrame(metrics)\n",
        "display(metrics_lasso512_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_lasso512_wc = cumulative_log_returns_lasso_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_lasso512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_lasso512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_lasso512_wc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVqfPnYNAB6P"
      },
      "source": [
        "## Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "V9IrlYsfVtty",
        "outputId": "aedf24be-1fed-45ce-e69a-18734f5a8092"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "ridge_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ridge', lag=[5])\n",
        "print(\"Ridge Results (Lag 5):\")\n",
        "display(ridge_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "_8r5mcrcADwS",
        "outputId": "c8b60123-a273-4693-b365-cb3b239bc450"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "ridge_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ridge', lag=[21])\n",
        "print(\"Ridge Results (Lag 21):\")\n",
        "display(ridge_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ZvS_9AjJAcRf",
        "outputId": "6a9680fb-11c3-4ca5-e9b3-213f8d282589"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "ridge_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ridge', lag=[252])\n",
        "print(\"Ridge Results (Lag 252):\")\n",
        "display(ridge_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "du-9ybDhAe-O",
        "outputId": "075452a9-01f6-4d62-c7eb-2b5473c17adb"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "ridge_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='ridge', lag=[512])\n",
        "print(\"Ridge Results (Lag 512):\")\n",
        "display(ridge_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI72bfsCKQLI"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kidyl3VYKRtR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Predefined valid lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Create an empty list to store results for each lag\n",
        "r2_results_per_lag = []\n",
        "\n",
        "# Calculate the quantiles for the market cap\n",
        "top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "# Filter top 25% and bottom 25% based on market cap\n",
        "top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "# Loop over each lag and calculate R² for each group (All, Top 25%, Bottom 25%)\n",
        "for lag in valid_lags:\n",
        "    forecast_col = f'lag_{lag}'  # Column name for the forecasted values for each lag\n",
        "    actual_col = 'excess_ret'  # Actual values column\n",
        "\n",
        "    # Check if forecast_col exists in merged_df\n",
        "    if forecast_col not in merged_df.columns:\n",
        "        print(f\"Warning: {forecast_col} not found in merged_df. Skipping lag {lag}.\")\n",
        "        continue\n",
        "\n",
        "    # Drop NaN values for both actual and forecasted columns before calculating R²\n",
        "    merged_df_clean = merged_df.dropna(subset=[forecast_col, actual_col])\n",
        "\n",
        "    # Prepare data for model fitting\n",
        "    X_train = merged_df_clean[[forecast_col]]  # Use the lag as the feature\n",
        "    y_train = merged_df_clean[actual_col]\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Fit the Ridge model with GridSearchCV for alpha tuning\n",
        "    param_grid = {'alpha': [0.01, 0.1, 1.0, 10, 100]}\n",
        "    grid_search = GridSearchCV(Ridge(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Make predictions for the entire dataset\n",
        "    merged_df_clean_copy = merged_df_clean.copy()  # Make a copy to avoid SettingWithCopyWarning\n",
        "    merged_df_clean_copy.loc[:, 'predicted_ret'] = best_model.predict(X_train_scaled)\n",
        "\n",
        "    # Calculate R² for All stocks (drop rows with NaN)\n",
        "    all_r2 = r2_score(merged_df_clean_copy[actual_col], merged_df_clean_copy['predicted_ret'])\n",
        "\n",
        "    # Scale and prepare data for Top 25% stocks\n",
        "    top_25_clean = top_25_stocks.dropna(subset=[forecast_col, actual_col]).copy()  # Explicit copy\n",
        "    X_top_25 = top_25_clean[[forecast_col]]\n",
        "    y_top_25 = top_25_clean[actual_col]\n",
        "    X_top_25_scaled = scaler.transform(X_top_25)  # Scale the top 25% data\n",
        "    top_25_model = Ridge(alpha=grid_search.best_params_['alpha'])\n",
        "    top_25_model.fit(X_top_25_scaled, y_top_25)\n",
        "    top_25_clean.loc[:, 'predicted_ret'] = top_25_model.predict(X_top_25_scaled)  # .loc[] to avoid warning\n",
        "    top_25_r2 = r2_score(top_25_clean[actual_col], top_25_clean['predicted_ret'])\n",
        "\n",
        "    # Scale and prepare data for Bottom 25% stocks\n",
        "    bottom_25_clean = bottom_25_stocks.dropna(subset=[forecast_col, actual_col]).copy()  # Explicit copy\n",
        "    X_bottom_25 = bottom_25_clean[[forecast_col]]\n",
        "    y_bottom_25 = bottom_25_clean[actual_col]\n",
        "    X_bottom_25_scaled = scaler.transform(X_bottom_25)  # Scale the bottom 25% data\n",
        "    bottom_25_model = Ridge(alpha=grid_search.best_params_['alpha'])\n",
        "    bottom_25_model.fit(X_bottom_25_scaled, y_bottom_25)\n",
        "    bottom_25_clean.loc[:, 'predicted_ret'] = bottom_25_model.predict(X_bottom_25_scaled)  # .loc[] to avoid warning\n",
        "    bottom_25_r2 = r2_score(bottom_25_clean[actual_col], bottom_25_clean['predicted_ret'])\n",
        "\n",
        "    # Store the results in the list\n",
        "    r2_results_per_lag.append({\n",
        "        'Lag': lag,\n",
        "        'All Stocks R²': all_r2,\n",
        "        'Top 25% R²': top_25_r2,\n",
        "        'Bottom 25% R²': bottom_25_r2\n",
        "    })\n",
        "\n",
        "# Convert the results into a DataFrame\n",
        "r2_results_df = pd.DataFrame(r2_results_per_lag)\n",
        "\n",
        "# Display the results\n",
        "display(r2_results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEOZeZBtQxdl"
      },
      "source": [
        "## Ridge Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmGeNXfLOeDV"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akpEX_cNOdSr",
        "outputId": "a3fddfc8-4e43-4d97-e705-5d528921ec2b"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `ridge` to predict excess returns\n",
        "def ridge_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the Ridge model\n",
        "    result_df = ridge_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['ridge_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Ridge model\n",
        "crsp_test_lagged = ridge_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'ridge_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'ridge_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_ridge5  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_ridge5['date'].append(date)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge5['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_ridge_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_ridge5)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_ridge_lag_5.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Rv9wFnO8bC"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulgpgKP-O_Pu",
        "outputId": "3a02bbbb-c33f-416d-9f6a-a8c70830b9d0"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_ridge5_c = cumulative_log_returns_ridge_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ridge5_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ridge5_c = pd.DataFrame(metrics)\n",
        "display(metrics_ridge5_c)\n",
        "\n",
        "\n",
        "# the same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_ridge5_wc = cumulative_log_returns_ridge_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ridge5_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ridge5_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_ridge5_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osH5MoSOhPtD"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVOe6n1HhSYn",
        "outputId": "b146c117-ec9b-46ef-f475-eb16cabfc32c"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `ridge` to predict excess returns\n",
        "def ridge_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the Ridge model\n",
        "    result_df = ridge_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['ridge_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Ridge model\n",
        "crsp_test_lagged = ridge_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'ridge_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'ridge_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_ridge21  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_ridge21['date'].append(date)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge21['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_ridge_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_ridge21)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_ridge_lag_21.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvEHAj5DlZrP"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m0ubpVElb6_",
        "outputId": "9af382eb-5e88-4e9d-ab98-0abbc073b4e5"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_ridge21_c = cumulative_log_returns_ridge_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ridge21_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ridge21_c = pd.DataFrame(metrics)\n",
        "display(metrics_ridge21_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_ridge21_wc = cumulative_log_returns_ridge_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ridge21_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ridge21_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_ridge21_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHY2KkiCnldq"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9fMOJYQnnYP",
        "outputId": "e01bf266-aff4-4424-e43a-620ec05df9be"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `ridge` to predict excess returns\n",
        "def ridge_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the Ridge model\n",
        "    result_df = ridge_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['ridge_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Ridge model\n",
        "crsp_test_lagged = ridge_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'ridge_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'ridge_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_ridge252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_ridge252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_ridge_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_ridge252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_ridge_lag_252.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8raEld_oTyh",
        "outputId": "eed2a4c6-caf8-4ef8-9d7f-b8991b294db7"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_ridge252_c = cumulative_log_returns_ridge_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ridge252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ridge252_c = pd.DataFrame(metrics)\n",
        "display(metrics_ridge252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_ridge252_wc = cumulative_log_returns_ridge_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ridge252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ridge252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_ridge252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PSGh5GLlx4R"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFaZw-yWlzHW",
        "outputId": "f51343c9-5460-4f34-b70d-cab998ebb726"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `ridge` to predict excess returns\n",
        "def ridge_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the Ridge model\n",
        "    result_df = ridge_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['ridge_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Ridge model\n",
        "crsp_test_lagged = ridge_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'ridge_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'ridge_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_ridge512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 512 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_ridge512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_ridge512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_ridge_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_ridge512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_ridge_lag_512.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd4DU-9RmZf5"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1DXoYw8mb1U",
        "outputId": "0f44e9fa-e996-48c2-bef3-99317fec6f3d"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_ridge512_c = cumulative_log_returns_ridge_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ridge512_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ridge512_c = pd.DataFrame(metrics)\n",
        "display(metrics_ridge512_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_ridge512_wc = cumulative_log_returns_ridge_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_ridge512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_ridge512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_ridge512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOn8b_shAnig"
      },
      "source": [
        "## ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "CKwMwVjKAnOz",
        "outputId": "0258649e-9804-4ea5-eb0c-82a128d19930"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "elasticnet_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='elasticnet', lag=[5])\n",
        "print(\"ElasticNet Results (Lag 5):\")\n",
        "display(elasticnet_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "A0Rjz3fgAu4Y",
        "outputId": "1c576ac8-d612-4f32-8e47-cab62c2099e1"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "elasticnet_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='elasticnet', lag=[21])\n",
        "print(\"ElasticNet Results (Lag 21):\")\n",
        "display(elasticnet_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "lmmduXa5AxRH",
        "outputId": "b33e6cbd-b1fe-4963-ebfa-eb2e64bbcd22"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "elasticnet_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='elasticnet', lag=[252])\n",
        "print(\"ElasticNet Results (Lag 252):\")\n",
        "display(elasticnet_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "xrssQ5ocFfCQ",
        "outputId": "6aeec751-618d-402d-80e4-fa52a1140ec8"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "elasticnet_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='elasticnet', lag=[512])\n",
        "print(\"ElasticNet Results (Lag 512):\")\n",
        "display(elasticnet_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLw8eZkpKw65"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9BttWhGLxhJ"
      },
      "outputs": [],
      "source": [
        "# Merge crsp_data and crsp_test_lagged on the stock ID (permno)\n",
        "merged_df = crsp_test_lagged.merge(crsp_data[['permno', 'market_cap']], on='permno', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ATq-LJ700B8"
      },
      "source": [
        "## ElasticNet Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtivQAVll5Rz"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBtk3zPZ3s57",
        "outputId": "ca230b4d-5c60-49f4-d676-59edd825dae3"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `elasticnet` to predict excess returns\n",
        "def enet_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the ElasticNet model\n",
        "    result_df = elasticnet_model_with_tuning(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['enet_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using ElasticNet model (this should generate binary outcomes)\n",
        "crsp_test_lagged = enet_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'enet_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'enet_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_enet5  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_enet5['date'].append(date)\n",
        "    cumulative_log_returns_by_date_enet5['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_enet5['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_enet_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_enet5)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_enet_lag_5.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4vzk6TzJjNc"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysdseQ4hJp-g",
        "outputId": "555c1af6-4b91-4234-a0b4-e39298471a3e"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_enet5_c = cumulative_log_returns_enet_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_enet5_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_enet5_c = pd.DataFrame(metrics)\n",
        "display(metrics_enet5_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_enet5_wc = cumulative_log_returns_enet_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_enet5_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_enet5_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_enet5_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIsu0CWal8AG"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwF6qoN2l-UT",
        "outputId": "0a598f12-829e-484a-fc00-b818facd162e"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `Elasticnet` to predict excess returns\n",
        "def enet_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the ElasticNet model\n",
        "    result_df = elasticnet_model_with_tuning(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['enet_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using OLS+Huber model\n",
        "crsp_test_lagged = enet_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'enet_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'enet_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_enet21  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_enet21['date'].append(date)\n",
        "    cumulative_log_returns_by_date_enet21['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_enet21['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_enet_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_enet21)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_enet_lag_21.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHdzYua5mpyZ"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHCxnpZMms5I",
        "outputId": "baa4f90e-d16b-4798-9f01-2e78dcec5685"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_enet21_c = cumulative_log_returns_enet_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_enet21_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_enet21_c = pd.DataFrame(metrics)\n",
        "display(metrics_enet21_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_enet21_wc = cumulative_log_returns_enet_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_enet21_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_enet21_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_enet21_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WexPilVknlY9"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYmn89FanomA",
        "outputId": "d55bef62-c730-4b78-e385-b1e839e326f6"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Step 4: Use the previously defined `Enet` to predict excess returns\n",
        "def enet_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the ElastcNet model\n",
        "    result_df = elasticnet_model_with_tuning(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['enet_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using Enet model\n",
        "crsp_test_lagged = enet_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'enet_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'enet_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_enet252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_enet252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_enet252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_enet252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_enet_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_enet252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_enet_lag_252.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiH_pOcPoO-_"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BgUM0jFoRs4",
        "outputId": "2099ba0e-e750-4479-9a95-36adc76fe622"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_enet252_c = cumulative_log_returns_enet_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_enet252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_enet252_c = pd.DataFrame(metrics)\n",
        "display(metrics_enet252_c)\n",
        "\n",
        "\n",
        "#  same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_enet252_wc = cumulative_log_returns_enet_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_enet252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_enet252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_enet252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m71pcJtv4rsD"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qamNaNPR4u8b",
        "outputId": "71cbc959-1eaf-485c-8661-2cb4ca1b2191"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `enet` to predict excess returns\n",
        "def enet_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the enet model\n",
        "    result_df = elasticnet_model_with_tuning(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['enet_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using enet model (this should generate binary outcomes)\n",
        "crsp_test_lagged = enet_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'enet_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'enet_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_enet512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 512 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_enet512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_enet512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_enet512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_enet_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_enet512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_enet_lag_512.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05-lMcBc6BAA",
        "outputId": "407e9fc3-1458-4632-f21e-821cca3c3248"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_enet512_c = cumulative_log_returns_enet_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_enet512_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_enet512_c = pd.DataFrame(metrics)\n",
        "display(metrics_enet512_c)\n",
        "\n",
        "\n",
        "# Now, the same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_enet512_wc = cumulative_log_returns_enet_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_enet512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_enet512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_enet512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R38gneeHF0fI"
      },
      "source": [
        "## PCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "xqReRIy6FwXg",
        "outputId": "036b685c-5a27-48af-d3ae-fed169233df4"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "pcr_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='pcr', lag=[5])\n",
        "print(\"PCR Results (Lag 5):\")\n",
        "display(pcr_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "A51GgfNkF6x7",
        "outputId": "98a0c06d-6df0-4d7f-bff6-ac460eb69b45"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "pcr_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='pcr', lag=[21])\n",
        "print(\"PCR Results (Lag 5):\")\n",
        "display(pcr_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Z3jY5DuUF9iM",
        "outputId": "62f31f92-de01-4e4a-8ec6-ef8597e4ae5f"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "pcr_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='pcr', lag=[252])\n",
        "print(\"PCR Results (Lag 252):\")\n",
        "display(pcr_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "yAXxtiNhGAIX",
        "outputId": "b0b9d3fa-ed9f-4550-be01-f21d25b0313f"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "pcr_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='pcr', lag=[512])\n",
        "print(\"PCR Results (Lag 5):\")\n",
        "display(pcr_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEk26NF7GFAf"
      },
      "source": [
        "## PCR Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJEXfBddGQAx"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P_sKGOLGHNe",
        "outputId": "0e7912a9-327d-4024-befe-3ef8e4c8064a"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `pcr` to predict excess returns\n",
        "def pcr_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the PCR model\n",
        "    result_df = pcr_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values  # Ensure that the column 'predicted_excess_returns' exists\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['pcr_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using PCR model (this should generate binary outcomes)\n",
        "crsp_test_lagged = pcr_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Step 5: Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'pcr_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'pcr_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_pcr5  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_pcr5['date'].append(date)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr5['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_pcr_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_pcr5)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_pcr_lag_5.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej_ICfceGqls"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H5BNc7KGtEJ",
        "outputId": "3b95512c-2eb5-46c3-8f4c-8cc2699f30cd"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_pcr5_c = cumulative_log_returns_pcr_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_pcr5_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_pcr5_c = pd.DataFrame(metrics)\n",
        "display(metrics_pcr5_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_pcr5_wc = cumulative_log_returns_pcr_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_pcr5_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_pcr5_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_pcr5_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nmvwXrvnPn3"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjGucMlanRmW",
        "outputId": "f1d80fd3-4fc7-423b-c555-7d74b0e79523"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `pcr` to predict excess returns\n",
        "def pcr_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the PCR model\n",
        "    result_df = pcr_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['pcr_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using PCR model\n",
        "crsp_test_lagged = pcr_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'pcr_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'pcr_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_pcr21  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_pcr21['date'].append(date)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr21['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_pcr_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_pcr21)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_pcr_lag_21.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZzUItfVns1O"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwnASjhqnvoz",
        "outputId": "b05ee272-8a3b-44f7-ef13-07b6a6a90c65"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_pcr21_c = cumulative_log_returns_pcr_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_pcr21_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_pcr21_c = pd.DataFrame(metrics)\n",
        "display(metrics_pcr21_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_pcr21_wc = cumulative_log_returns_pcr_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_pcr21_wc)\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_pcr21_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_pcr21_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5_dO5cLoHKV"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbkutPLXoIiw",
        "outputId": "9507e19a-1ab0-41d1-b34b-95ae5a12d156"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `pcr` to predict excess returns\n",
        "def pcr_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the PCR model\n",
        "    result_df = pcr_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['pcr_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using PCR model (this should generate binary outcomes)\n",
        "crsp_test_lagged = pcr_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'pcr_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'pcr_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_pcr252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_pcr252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_pcr_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_pcr252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_pcr_lag_252.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKPfgHDLoeXg"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwSFrzf6oh_A",
        "outputId": "2e79a96e-9d93-4107-8ed5-6f18c95b4847"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_pcr252_c = cumulative_log_returns_pcr_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_pcr252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_pcr252_c = pd.DataFrame(metrics)\n",
        "display(metrics_pcr252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_pcr252_wc = cumulative_log_returns_pcr_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_pcr252_wc)\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_pcr252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_pcr252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzz7WaJZozSo"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knon36iqoy1_",
        "outputId": "0194cdea-33cc-4403-9340-c74f0d38ff53"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `pcr` to predict excess returns\n",
        "def pcr_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the PCR model\n",
        "    result_df = pcr_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['pcr_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using PCR model\n",
        "crsp_test_lagged = pcr_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'pcr_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'pcr_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_pcr512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_pcr512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_pcr512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_pcr_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_pcr512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_pcr_lag_512.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVfkmzPDpaAM",
        "outputId": "12c4be45-0eee-413c-8925-fd114aa55146"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_pcr512_c = cumulative_log_returns_pcr_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_pcr512_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_pcr512_c = pd.DataFrame(metrics)\n",
        "display(metrics_pcr512_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_pcr512_wc = cumulative_log_returns_pcr_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_pcr512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_pcr512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_pcr512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQqMMcC5Gf-x"
      },
      "source": [
        "## GLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "z4k-oFWDGiNw",
        "outputId": "744d5877-031c-4a0d-9a14-4e13bf667ad4"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "glm_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='glm', lag=[5])\n",
        "print(\"GLM Results (Lag 5):\")\n",
        "display(glm_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "leruaaiKGiKX",
        "outputId": "862908f1-17c0-4157-87db-050eb60a27b6"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "glm_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='glm', lag=[21])\n",
        "print(\"GLM Results (Lag 21):\")\n",
        "display(glm_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "C8c_eQc_GrxH",
        "outputId": "de66d7a3-b2cb-4353-8fbb-18c738207d22"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "glm_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='glm', lag=[252])\n",
        "print(\"GLM Results (Lag 252):\")\n",
        "display(glm_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "fkX2MinbGujV",
        "outputId": "716e1504-244b-4848-de29-bd2e0c798b86"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "glm_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='glm', lag=[512])\n",
        "print(\"GLM Results (Lag 512):\")\n",
        "display(glm_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIitSHhNs_x"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pik_LMMpNuU0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Utility: Adjusted R²\n",
        "def adjusted_r2_score(y_true, y_pred, n, p):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Utility: Directional Accuracy\n",
        "def directional_accuracy(y_true, y_pred_binary):\n",
        "    return np.mean(y_true == y_pred_binary)\n",
        "\n",
        "# Logistic Regression (GLM) with tuning — Global Model\n",
        "def glm_model_with_tuning(train_df, test_df, lag=5):\n",
        "    feature = f'lag_{lag}'\n",
        "    df_train = train_df.dropna(subset=[feature, 'directional_target'])\n",
        "    df_test = test_df.dropna(subset=[feature, 'directional_target'])\n",
        "\n",
        "    X_train = df_train[[feature]]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[[feature]]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'solver': ['lbfgs', 'liblinear', 'saga'],\n",
        "        'max_iter': [100, 200, 500]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_prob = best_model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "    y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    up_accuracy = directional_accuracy(y_test[y_test == 1], y_pred_binary[y_test == 1])\n",
        "    down_accuracy = directional_accuracy(y_test[y_test == 0], y_pred_binary[y_test == 0])\n",
        "    mse = mean_squared_error(y_test, y_pred_prob)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred_prob)\n",
        "    mase_value = mase(y_test, y_pred_binary)\n",
        "    adjusted_r2 = adjusted_r2_score(y_test, y_pred_prob, len(y_test), X_train.shape[1])\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        'Model': 'GLM',\n",
        "        'Lag': lag,\n",
        "        'Directional Accuracy': accuracy,\n",
        "        'Up Directional Accuracy': up_accuracy,\n",
        "        'Down Directional Accuracy': down_accuracy,\n",
        "        'Adjusted R-squared': adjusted_r2,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'MASE': mase_value\n",
        "    }])\n",
        "\n",
        "# Main function to calculate OOS R² for GLM with market cap segmentation\n",
        "def calculate_oos_r2_market_cap_glm(train_df, test_df, valid_lags):\n",
        "    # Calculate the quantiles for the market cap\n",
        "    top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "    bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "    # Filter top 25% and bottom 25% based on market cap\n",
        "    top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "    bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "    # Create empty DataFrames to store results\n",
        "    glm_oos_r2_all = pd.DataFrame()\n",
        "    glm_oos_r2_top_25 = pd.DataFrame()\n",
        "    glm_oos_r2_bottom_25 = pd.DataFrame()\n",
        "\n",
        "    # Loop through each lag and calculate OOS R² for all, top 25% and bottom 25% stocks\n",
        "    for lag in valid_lags:\n",
        "        # For All Stocks\n",
        "        glm_oos_r2_all = pd.concat([glm_oos_r2_all, glm_model_with_tuning(train_df, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Top 25% Stocks\n",
        "        glm_oos_r2_top_25 = pd.concat([glm_oos_r2_top_25, glm_model_with_tuning(top_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Bottom 25% Stocks\n",
        "        glm_oos_r2_bottom_25 = pd.concat([glm_oos_r2_bottom_25, glm_model_with_tuning(bottom_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "    # Merge the results into a single DataFrame with Lag as index\n",
        "    results_df = pd.DataFrame({\n",
        "        'Lag': valid_lags,\n",
        "        'All Stocks R²': glm_oos_r2_all['Adjusted R-squared'].values,\n",
        "        'Top 25% R²': glm_oos_r2_top_25['Adjusted R-squared'].values,\n",
        "        'Bottom 25% R²': glm_oos_r2_bottom_25['Adjusted R-squared'].values\n",
        "    })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Example: Define lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Calculate OOS R² for GLM with all stocks, top 25%, and bottom 25%\n",
        "glm_oos_r2_results = calculate_oos_r2_market_cap_glm(crsp_train_lagged, crsp_test_lagged, valid_lags)\n",
        "\n",
        "# Display the results in the required format\n",
        "display(glm_oos_r2_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heMnYrFCD-Hn"
      },
      "source": [
        "## GLM Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vml32EztEAX_"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "593E7pA0EBba",
        "outputId": "852f9dab-373f-47d0-80ce-de511fef3b66"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `glm` to predict excess returns\n",
        "def glm_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the ElasticNet model\n",
        "    result_df = glm_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['glm_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using ElasticNet model\n",
        "crsp_test_lagged = glm_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'glm_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'glm_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_glm5  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_glm5['date'].append(date)\n",
        "    cumulative_log_returns_by_date_glm5['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_glm5['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_glm_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_glm5)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_glm_lag_5.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_glm_lag_5.to_csv(\"cumulative_log_returns_glm_lag_5.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWChnGC2vecP"
      },
      "source": [
        "#### With/Without Transaction Cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T9XneB5vhc2",
        "outputId": "669601fa-a7d6-4752-f0ea-ffd04e33409b"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_glm5_c = cumulative_log_returns_glm_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_glm5_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_glm5_c = pd.DataFrame(metrics)\n",
        "display(metrics_glm5_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_glm5_wc = cumulative_log_returns_glm_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_glm5_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_glm5_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_glm5_wc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmPdnaXWp3-_"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSjCzRRip5Ya",
        "outputId": "f53a444b-5bd3-4a36-9c67-c20732cc9732"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `glm` to predict excess returns\n",
        "def glm_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the GLM model\n",
        "    result_df = glm_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['glm_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using GLM model\n",
        "crsp_test_lagged = glm_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'glm_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'glm_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_glm21  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_glm21['date'].append(date)\n",
        "    cumulative_log_returns_by_date_glm21['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_glm21['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_glm_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_glm21)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_glm_lag_21.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_glm_lag_21.to_csv(\"cumulative_log_returns_glm_lag_21.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoLJqToFq8xL",
        "outputId": "0d428310-9ace-41bf-89da-983905c52efc"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_glm21_c = cumulative_log_returns_glm_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_glm21_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_glm21_c = pd.DataFrame(metrics)\n",
        "display(metrics_glm21_c)\n",
        "\n",
        "# the same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_glm21_wc = cumulative_log_returns_glm_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_glm21_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_glm21_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_glm21_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFzxWV9irR4B"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2GQN_kwrTBz",
        "outputId": "9c84dea7-82a3-447f-baa4-2a9e27ca745c"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `glm` to predict excess returns\n",
        "def glm_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the GLM model\n",
        "    result_df = glm_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['glm_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using GLM model\n",
        "crsp_test_lagged = glm_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'glm_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'glm_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_glm252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_glm252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_glm252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_glm252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_glm_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_glm252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_glm_lag_252.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_glm_lag_252.to_csv(\"cumulative_log_returns_glm_lag_252.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfGZdLKEroTe",
        "outputId": "4e39f0f7-bca4-4463-de89-4e5bfdf8e6f3"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_glm252_c = cumulative_log_returns_glm_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_glm252_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_glm252_c = pd.DataFrame(metrics)\n",
        "display(metrics_glm252_c)\n",
        "\n",
        "\n",
        "# Now, the same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_glm252_wc = cumulative_log_returns_glm_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_glm252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_glm252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_glm252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dJx2TJMr6lJ"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0WUI_IOr71w",
        "outputId": "48ef6574-760f-4931-983f-80abab3a78ad"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `GLM` to predict excess returns\n",
        "def glm_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the GLM model\n",
        "    result_df = glm_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['glm_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using GLM model (this should generate binary outcomes)\n",
        "crsp_test_lagged = glm_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'glm_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'glm_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_glm512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_glm512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_glm512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_glm512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_glm_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_glm512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_glm_lag_512.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_glm_lag_512.to_csv(\"cumulative_log_returns_glm_lag_512.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuPQL4YXsOJf",
        "outputId": "933a2c1f-5154-4f7f-ab65-f8a1fed768fd"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_glm512_c = cumulative_log_returns_glm_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_glm512_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_glm512_c = pd.DataFrame(metrics)\n",
        "display(metrics_glm512_c)\n",
        "\n",
        "\n",
        "# Now, the same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_glm512_wc = cumulative_log_returns_glm_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_glm512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_glm512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_glm512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddHmDWzKG1aI"
      },
      "source": [
        "## RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "AXD8Ov83G2kR",
        "outputId": "85efc56d-7543-4a5a-ca3e-f9a4c90068e5"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "rf_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='randomforest', lag=[5])\n",
        "print(\"Random Forest Results (Lag 5):\")\n",
        "display(rf_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "tEUwju20G8_8",
        "outputId": "3a2f76de-3bd5-4a5b-ac08-75e695347d00"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "rf_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='randomforest', lag=[21])\n",
        "print(\"Random Forest Results (Lag 21):\")\n",
        "display(rf_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "l73o4X3Ly67N",
        "outputId": "109832b3-0388-4e3c-dcae-23eb2c49bee2"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "rf_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='randomforest', lag=[252])\n",
        "print(\"Random Forest Results (Lag 252):\")\n",
        "display(rf_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "t7mTijg2HAJX",
        "outputId": "9e5aa2cd-8b97-468c-e5da-011f244f3387"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "rf_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='randomforest', lag=[252])\n",
        "print(\"Random Forest Results (Lag 252):\")\n",
        "display(rf_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "WZNTQa9ty8kT",
        "outputId": "bb3a7be5-b1a3-49b5-e9a0-51a3846ccee3"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "rf_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='randomforest', lag=[512])\n",
        "print(\"Random Forest Results (Lag 512):\")\n",
        "display(rf_lag512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "S7x18rKVHAAN",
        "outputId": "e458a9b7-2706-47f6-8c1a-e929556c9986"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "rf_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='randomforest', lag=[512])\n",
        "print(\"Random Forest Results (Lag 512):\")\n",
        "display(rf_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otN9eLyIOLxq"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rvKyyOtOMua"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Utility: Adjusted R²\n",
        "def adjusted_r2_score(y_true, y_pred, n, p):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Utility: Calculate OOS R² for Random Forest\n",
        "def calculate_oos_r2_rf(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return r2_score(y_test, y_pred)\n",
        "\n",
        "# Tuning Random Forest model\n",
        "def random_forest_model_with_tuning(X_train, y_train):\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        RandomForestRegressor(),\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=10,\n",
        "        cv=5,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train, y_train)\n",
        "    return random_search.best_estimator_\n",
        "\n",
        "# Random Forest model with OOS R² loss and tuning\n",
        "def random_forest_model_with_tuning_loss(train_df, test_df, lag=5):\n",
        "    feature = f'lag_{lag}'\n",
        "    df_train = train_df.dropna(subset=[feature, 'directional_target'])\n",
        "    df_test = test_df.dropna(subset=[feature, 'directional_target'])\n",
        "\n",
        "    X_train = df_train[[feature]]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[[feature]]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Impute missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_imputed = imputer.fit_transform(X_train)\n",
        "    X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "    # Tune and train Random Forest\n",
        "    model = random_forest_model_with_tuning(X_train_imputed, y_train)\n",
        "\n",
        "    # Calculate OOS R²\n",
        "    oos_r2 = calculate_oos_r2_rf(model, X_test_imputed, y_test)\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        'Lag': lag,\n",
        "        'OOS R²': oos_r2\n",
        "    }])\n",
        "\n",
        "# Main function to calculate OOS R² for Random Forest with market cap segmentation\n",
        "def calculate_oos_r2_market_cap_rf(train_df, test_df, valid_lags):\n",
        "    # Calculate the quantiles for the market cap\n",
        "    top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "    bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "    # Filter top 25% and bottom 25% based on market cap\n",
        "    top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "    bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "    # Create empty DataFrames to store results\n",
        "    rf_oos_r2_all = pd.DataFrame()\n",
        "    rf_oos_r2_top_25 = pd.DataFrame()\n",
        "    rf_oos_r2_bottom_25 = pd.DataFrame()\n",
        "\n",
        "    # Loop through each lag and calculate OOS R² for all, top 25% and bottom 25% stocks\n",
        "    for lag in valid_lags:\n",
        "        # For All Stocks\n",
        "        rf_oos_r2_all = pd.concat([rf_oos_r2_all, random_forest_model_with_tuning_loss(train_df, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Top 25% Stocks\n",
        "        rf_oos_r2_top_25 = pd.concat([rf_oos_r2_top_25, random_forest_model_with_tuning_loss(top_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Bottom 25% Stocks\n",
        "        rf_oos_r2_bottom_25 = pd.concat([rf_oos_r2_bottom_25, random_forest_model_with_tuning_loss(bottom_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "    # Merge the results into a single DataFrame with Lag as index\n",
        "    results_df = pd.DataFrame({\n",
        "        'Lag': valid_lags,\n",
        "        'All Stocks R²': rf_oos_r2_all['OOS R²'].values,\n",
        "        'Top 25% R²': rf_oos_r2_top_25['OOS R²'].values,\n",
        "        'Bottom 25% R²': rf_oos_r2_bottom_25['OOS R²'].values\n",
        "    })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Example: Define lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Calculate OOS R² for Random Forest with all stocks, top 25%, and bottom 25%\n",
        "rf_oos_r2_results = calculate_oos_r2_market_cap_rf(crsp_train_lagged, crsp_test_lagged, valid_lags)\n",
        "\n",
        "# Display the results in the required format\n",
        "display(rf_oos_r2_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEosYqNk0uOf"
      },
      "source": [
        "## RF Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWoYAg5VJUgL"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "hyWjt3Qz0wAx",
        "outputId": "0dd8692f-9ad1-4de5-f85c-b1b66d07991c"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `RF` to predict excess returns\n",
        "def rf_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the RF model\n",
        "    result_df = random_forest_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['rf_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using RF model\n",
        "crsp_test_lagged = rf_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'rf_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'rf_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_rf5  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_rf5['date'].append(date)\n",
        "    cumulative_log_returns_by_date_rf5['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_rf5['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_rf_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_rf5)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_rf_lag_5.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "Q44fYPAOJ795",
        "outputId": "9756c98a-9056-4b10-dcb5-0dd9efacaa56"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_rf5_c = cumulative_log_returns_rf_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_rf5_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_rf5_c = pd.DataFrame(metrics)\n",
        "display(metrics_rf5_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_rf5_wc = cumulative_log_returns_rf_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_rf5_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_rf5_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_rf5_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaaDUpNyspSL"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K496LRTfsqSE",
        "outputId": "121d8a9a-5c15-479c-aae2-90dec4aa2d01"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `Random Forest` to predict excess returns\n",
        "def rf_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the RF model\n",
        "    result_df = random_forest_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['rf_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using RF model\n",
        "crsp_test_lagged = rf_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'rf_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'rf_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_rf21  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_rf21['date'].append(date)\n",
        "    cumulative_log_returns_by_date_rf21['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_rf21['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_rf_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_rf21)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_rf_lag_21.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNi59dCxtTEz",
        "outputId": "9c3184fe-6b1d-4cad-cbcd-d27c633ccc5f"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_rf21_c = cumulative_log_returns_rf_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_rf21_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_rf21_c = pd.DataFrame(metrics)\n",
        "display(metrics_rf21_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_rf21_wc = cumulative_log_returns_rf_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_rf21_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_rf21_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_rf21_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPrtBdFetdZ8"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXB0F0tRtfJG",
        "outputId": "a40a3024-6b3e-4158-bc5b-e51e86fa7f83"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `rf` to predict excess returns\n",
        "def rf_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the RF model\n",
        "    result_df = random_forest_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['rf_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using RF model (this should generate binary outcomes)\n",
        "crsp_test_lagged = rf_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'rf_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'rf_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_rf252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_rf252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_rf252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_rf252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_rf_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_rf252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_rf_lag_252.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMzIt3ufVzU0",
        "outputId": "cc0d2645-8c9e-454c-ed8d-8774c2c1a436"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_rf252_c = cumulative_log_returns_rf_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_rf252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_rf252_c = pd.DataFrame(metrics)\n",
        "display(metrics_rf252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_rf252_wc = cumulative_log_returns_rf_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_rf252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_rf252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_rf252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Cpo6FIuPTk"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUR5gCzVV2xO",
        "outputId": "1f702b72-da4a-47f8-9e0a-3ee81be42e7c"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `rf` to predict excess returns\n",
        "def rf_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the RF model\n",
        "    result_df = random_forest_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['rf_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using RF model (this should generate binary outcomes)\n",
        "crsp_test_lagged = rf_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'rf_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'rf_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_rf512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_rf512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_rf512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_rf512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_rf_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_rf512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_rf_lag_512.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkM5bORwMeP9",
        "outputId": "2a9c79bf-b1db-49ec-8266-924236d65d62"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_rf512_c = cumulative_log_returns_rf_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_rf512_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_rf512_c = pd.DataFrame(metrics)\n",
        "display(metrics_rf512_c)\n",
        "\n",
        "\n",
        "# Now, the same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_rf512_wc = cumulative_log_returns_rf_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_rf512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_rf512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_rf512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BEjzPtKHFHG"
      },
      "source": [
        "## GBRT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "N-04jdXyHG_I",
        "outputId": "8684d423-590e-4917-fc53-d86754691de1"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "gbrt_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='gbrt', lag=[5])\n",
        "print(\"GBRT Results (Lag 5):\")\n",
        "display(gbrt_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "H6TTUuLsIKt2",
        "outputId": "a2458662-91d1-4261-f6a8-896d8e2eddf8"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "gbrt_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='gbrt', lag=[21])\n",
        "print(\"GBRT Results (Lag 21):\")\n",
        "display(gbrt_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "4rIkemFuIN6V",
        "outputId": "ea1328c2-7c2a-4a04-8ccd-24a355b11acb"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "gbrt_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='gbrt', lag=[252])\n",
        "print(\"GBRT Results (Lag 252):\")\n",
        "display(gbrt_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "Gg1CM-vwIQhI",
        "outputId": "23192423-1853-4fcf-e1fe-24dc2ae9bdbe"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "gbrt_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='gbrt', lag=[512])\n",
        "print(\"GBRT Results (Lag 512):\")\n",
        "display(gbrt_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOW3y_4zOc99"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "jSieU5coOeO-",
        "outputId": "001fb5c9-3835-40c6-8951-a54e0a7134f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Utility: Adjusted R²\n",
        "def adjusted_r2_score(y_true, y_pred, n, p):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Utility: Calculate OOS R² for GBRT\n",
        "def calculate_oos_r2_gbrt(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return r2_score(y_test, y_pred)\n",
        "\n",
        "# GBRT model with hyperparameter tuning (with Huber loss)\n",
        "def gbrt_model_with_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'loss': ['huber']  # Use Huber loss for robustness\n",
        "    }\n",
        "    grid_search = GridSearchCV(\n",
        "        GradientBoostingRegressor(),\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring='neg_mean_squared_error'\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# GBRT model with OOS R² loss and tuning\n",
        "def gbrt_model_with_tuning_loss(train_df, test_df, lag=5):\n",
        "    feature = f'lag_{lag}'\n",
        "    df_train = train_df.dropna(subset=[feature, 'directional_target'])\n",
        "    df_test = test_df.dropna(subset=[feature, 'directional_target'])\n",
        "\n",
        "    X_train = df_train[[feature]]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[[feature]]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Tune and train GBRT\n",
        "    model = gbrt_model_with_tuning(X_train, y_train)\n",
        "\n",
        "    # Calculate OOS R²\n",
        "    oos_r2 = calculate_oos_r2_gbrt(model, X_test, y_test)\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        'Lag': lag,\n",
        "        'OOS R²': oos_r2\n",
        "    }])\n",
        "\n",
        "# Main function to calculate OOS R² for GBRT with market cap segmentation\n",
        "def calculate_oos_r2_market_cap_gbrt(train_df, test_df, valid_lags):\n",
        "    # Calculate the quantiles for the market cap\n",
        "    top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "    bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "    # Filter top 25% and bottom 25% based on market cap\n",
        "    top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "    bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "    # Create empty DataFrames to store results\n",
        "    gbrt_oos_r2_all = pd.DataFrame()\n",
        "    gbrt_oos_r2_top_25 = pd.DataFrame()\n",
        "    gbrt_oos_r2_bottom_25 = pd.DataFrame()\n",
        "\n",
        "    # Loop through each lag and calculate OOS R² for all, top 25% and bottom 25% stocks\n",
        "    for lag in valid_lags:\n",
        "        # For All Stocks\n",
        "        gbrt_oos_r2_all = pd.concat([gbrt_oos_r2_all, gbrt_model_with_tuning_loss(train_df, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Top 25% Stocks\n",
        "        gbrt_oos_r2_top_25 = pd.concat([gbrt_oos_r2_top_25, gbrt_model_with_tuning_loss(top_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Bottom 25% Stocks\n",
        "        gbrt_oos_r2_bottom_25 = pd.concat([gbrt_oos_r2_bottom_25, gbrt_model_with_tuning_loss(bottom_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "    # Merge the results into a single DataFrame with Lag as index\n",
        "    results_df = pd.DataFrame({\n",
        "        'Lag': valid_lags,\n",
        "        'All Stocks R²': gbrt_oos_r2_all['OOS R²'].values,\n",
        "        'Top 25% R²': gbrt_oos_r2_top_25['OOS R²'].values,\n",
        "        'Bottom 25% R²': gbrt_oos_r2_bottom_25['OOS R²'].values\n",
        "    })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Example: Define lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Calculate OOS R² for GBRT with all stocks, top 25%, and bottom 25%\n",
        "gbrt_oos_r2_results = calculate_oos_r2_market_cap_gbrt(crsp_train_lagged, crsp_test_lagged, valid_lags)\n",
        "\n",
        "# Display the results in the required format\n",
        "display(gbrt_oos_r2_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93-uaoLAuueB"
      },
      "source": [
        "## GBRT Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmDpen5GuwMZ"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "fr8oLrq2uxie",
        "outputId": "836ee1ce-05bf-4410-9077-53406ab5c84b"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `gbrt` to predict excess returns\n",
        "def gbrt_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the GBRT model\n",
        "    result_df = gbrt_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values  # Ensure that the column 'predicted_excess_returns' exists\n",
        "\n",
        "    # can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['gbrt_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using GBRT model\n",
        "crsp_test_lagged = gbrt_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'gbrt_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'gbrt_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_gbrt5  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_gbrt5['date'].append(date)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt5['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_gbrt_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_gbrt5)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_gbrt_lag_5.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "_vGWqo7n3ek0",
        "outputId": "e0a28e16-7e68-49f2-950a-68ce0001d87e"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_gbrt5_c = cumulative_log_returns_gbrt_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_gbrt5_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_gbrt5_c = pd.DataFrame(metrics)\n",
        "display(metrics_gbrt5_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_gbrt5_wc = cumulative_log_returns_gbrt_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_gbrt5_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_gbrt5_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_gbrt5_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_TL4Tl83vda"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "zVITtKTL3xvi",
        "outputId": "8252b06e-af85-4879-826e-3ced36a9d924"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `gbrt` to predict excess returns\n",
        "def gbrt_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the GBRT model\n",
        "    result_df = gbrt_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['gbrt_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using GBRT model (this should generate binary outcomes)\n",
        "crsp_test_lagged = gbrt_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'gbrt_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'gbrt_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_gbrt21  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_gbrt21['date'].append(date)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt21['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_gbrt_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_gbrt21)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_gbrt_lag_21.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "rSNBVc1548TS",
        "outputId": "82e33919-9b2e-4516-b5fe-b025220c0976"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_gbrt21_c = cumulative_log_returns_gbrt_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_gbrt21_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_gbrt21_c = pd.DataFrame(metrics)\n",
        "display(metrics_gbrt21_c)\n",
        "\n",
        "#  the same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_gbrt21_wc = cumulative_log_returns_gbrt_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_gbrt21_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_gbrt21_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_gbrt21_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RanTRU9S5T8s"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "iPnJadI15Vd2",
        "outputId": "47832e50-ae6c-481b-9946-d493478b50f7"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `gbrt` to predict excess returns\n",
        "def gbrt_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the GBRT model\n",
        "    result_df = gbrt_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['gbrt_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using GBRT model (this should generate binary outcomes)\n",
        "crsp_test_lagged = gbrt_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "#  Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'gbrt_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'gbrt_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_gbrt252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_gbrt252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_gbrt_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_gbrt252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_gbrt_lag_252.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_gbrt_lag_252.to_csv(\"cumulative_log_returns_gbrt_lag_252.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "dP9bG-x958uZ",
        "outputId": "4e9158ea-6e5e-4a5e-f36f-cc85562bfa5d"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_gbrt252_c = cumulative_log_returns_gbrt_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_gbrt252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_gbrt252_c = pd.DataFrame(metrics)\n",
        "display(metrics_gbrt252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_gbrt252_wc = cumulative_log_returns_gbrt_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_gbrt252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_gbrt252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_gbrt252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYUHPsRS6luO"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "DYr8IlZx6m2G",
        "outputId": "c437729d-a8dd-4203-d38c-c3b43422e878"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `gbrt` to predict excess returns\n",
        "def gbrt_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the GBRT model\n",
        "    result_df = gbrt_model(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['gbrt_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using GBRT model (this should generate binary outcomes)\n",
        "crsp_test_lagged = gbrt_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'gbrt_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'gbrt_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_gbrt512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_gbrt512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_gbrt512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_gbrt_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_gbrt512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_gbrt_lag_512.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_gbrt_lag_512.to_csv(\"cumulative_log_returns_gbrt_lag_512.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "IWVRFgGo7Zfa",
        "outputId": "42578be0-6f9a-483c-fdee-1d522c9d3430"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_gbrt512_c = cumulative_log_returns_gbrt_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_gbrt512_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_gbrt512_c = pd.DataFrame(metrics)\n",
        "display(metrics_gbrt512_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_gbrt512_wc = cumulative_log_returns_gbrt_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_gbrt512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_gbrt512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_gbrt512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR9DU3Zu8GdF"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TincfIIZspA"
      },
      "source": [
        "## NN1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "ek51rm95PuLI",
        "outputId": "fc1f5686-275b-433b-a7b3-892124ebc3b4"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "nn1_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn1', lag=[5])\n",
        "print(\"NN1 Results (Lag 5):\")\n",
        "display(nn1_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "yY_HK0TDqgSk",
        "outputId": "2396b871-2a17-492f-dd70-eed2821e0165"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "nn1_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn1', lag=[21])\n",
        "print(\"NN1 Results (Lag 21):\")\n",
        "display(nn1_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "dAbmexWANSol",
        "outputId": "53e7146c-316d-463b-b2a9-a517b961d79c"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "nn1_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn1', lag=[21])\n",
        "print(\"NN1 Results (Lag 21):\")\n",
        "display(nn1_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "RkMBOnqUsODE",
        "outputId": "ccf1ec7e-4029-4a43-adf0-87c3847b2015"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "nn1_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn1', lag=[252])\n",
        "print(\"NN1 Results (Lag 252):\")\n",
        "display(nn1_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "DVWG2r8TsPNI",
        "outputId": "17b6f04e-8a3b-4803-a63e-745427ea341c"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "nn1_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn1', lag=[512])\n",
        "print(\"NN1 Results (Lag 512):\")\n",
        "display(nn1_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQza7vQcr3HL"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "v2TJvr4Mr4RE",
        "outputId": "1e296cfe-a1d3-44f6-c2fb-8575371ed9f7"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Utility: Adjusted R²\n",
        "def adjusted_r2_score(y_true, y_pred, n, p):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Utility: Calculate OOS R² for NN1\n",
        "def calculate_oos_r2_nn1(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return r2_score(y_test, y_pred)\n",
        "\n",
        "# Neural Network (NN1) model with hyperparameter tuning\n",
        "def nn1_model_with_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'hidden_layer_sizes': [(32,)],  # One hidden layer with 32 neurons\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'max_iter': [200, 500],\n",
        "        'learning_rate_init': [0.001, 0.01]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(MLPRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# NN1 model with OOS R² loss and tuning\n",
        "def nn1_model_with_tuning_loss(train_df, test_df, lag=5):\n",
        "    feature = f'lag_{lag}'\n",
        "    df_train = train_df.dropna(subset=[feature, 'directional_target'])\n",
        "    df_test = test_df.dropna(subset=[feature, 'directional_target'])\n",
        "\n",
        "    X_train = df_train[[feature]]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[[feature]]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Tune and train NN1 model\n",
        "    model = nn1_model_with_tuning(X_train, y_train)\n",
        "\n",
        "    # Calculate OOS R²\n",
        "    oos_r2 = calculate_oos_r2_nn1(model, X_test, y_test)\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        'Lag': lag,\n",
        "        'OOS R²': oos_r2\n",
        "    }])\n",
        "\n",
        "# Main function to calculate OOS R² for NN1 with market cap segmentation\n",
        "def calculate_oos_r2_market_cap_nn1(train_df, test_df, valid_lags):\n",
        "    # Calculate the quantiles for the market cap\n",
        "    top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "    bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "    # Filter top 25% and bottom 25% based on market cap\n",
        "    top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "    bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "    # Create empty DataFrames to store results\n",
        "    nn1_oos_r2_all = pd.DataFrame()\n",
        "    nn1_oos_r2_top_25 = pd.DataFrame()\n",
        "    nn1_oos_r2_bottom_25 = pd.DataFrame()\n",
        "\n",
        "    # Loop through each lag and calculate OOS R² for all, top 25% and bottom 25% stocks\n",
        "    for lag in valid_lags:\n",
        "        # For All Stocks\n",
        "        nn1_oos_r2_all = pd.concat([nn1_oos_r2_all, nn1_model_with_tuning_loss(train_df, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Top 25% Stocks\n",
        "        nn1_oos_r2_top_25 = pd.concat([nn1_oos_r2_top_25, nn1_model_with_tuning_loss(top_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Bottom 25% Stocks\n",
        "        nn1_oos_r2_bottom_25 = pd.concat([nn1_oos_r2_bottom_25, nn1_model_with_tuning_loss(bottom_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "    # Merge the results into a single DataFrame with Lag as index\n",
        "    results_df = pd.DataFrame({\n",
        "        'Lag': valid_lags,\n",
        "        'All Stocks R²': nn1_oos_r2_all['OOS R²'].values,\n",
        "        'Top 25% R²': nn1_oos_r2_top_25['OOS R²'].values,\n",
        "        'Bottom 25% R²': nn1_oos_r2_bottom_25['OOS R²'].values\n",
        "    })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Example: Define lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Calculate OOS R² for NN1 with all stocks, top 25%, and bottom 25%\n",
        "nn1_oos_r2_results = calculate_oos_r2_market_cap_nn1(crsp_train_lagged, crsp_test_lagged, valid_lags)\n",
        "\n",
        "# Display the results in the required format\n",
        "display(nn1_oos_r2_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bPkD8aPbd8S"
      },
      "source": [
        "## NN1 Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5vAea3MLvdq"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "danfQKF1LyCi",
        "outputId": "ec02740e-8a52-4bc5-dc65-64d241ff1a12"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN1` to predict excess returns\n",
        "def nn1_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the NN1 model\n",
        "    result_df = neural_network_nn1(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn1_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN1 model\n",
        "crsp_test_lagged = nn1_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn1_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn1_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn15  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_nn15['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn15['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn15['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_nn1_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_nn15)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_nn1_lag_5.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "6MMy4IadP85R",
        "outputId": "238274fc-6ed5-42a8-9fb0-72a36dbac902"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn15_c = cumulative_log_returns_nn1_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn15_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn15_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn15_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn15_wc = cumulative_log_returns_nn1_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn15_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn15_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn15_wc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYNxljHtRSWP"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "4w8iOpBKRWYn",
        "outputId": "5cb773d8-3bfb-4ca3-c75c-5949ae27df52"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `laNN1sso` to predict excess returns\n",
        "def nn1_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the NN1 model\n",
        "    result_df = neural_network_nn1(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    #  use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn1_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN1 model\n",
        "crsp_test_lagged = nn1_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn1_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn1_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn121  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_nn121['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn121['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn121['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_nn1_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_nn121)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_nn1_lag_21.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "-hMnqxJmSLgy",
        "outputId": "f4b3f4c3-d753-4c9e-846a-35c0b341631f"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn121_c = cumulative_log_returns_nn1_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn121_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn121_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn121_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn121_wc = cumulative_log_returns_nn1_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn121_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn121_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn121_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn8tJuO0T7tZ"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "brDtq9LeT9UO",
        "outputId": "a5ee58b0-0a8d-4e5c-a279-6a4802e8baf3"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN1` to predict excess returns\n",
        "def nn1_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the NN1 model\n",
        "    result_df = neural_network_nn1(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn1_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN1 model\n",
        "crsp_test_lagged = nn1_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn1_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn1_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn1252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_nn1252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_nn1_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_nn1252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_nn1_lag_252.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "a6QIYBiSU3P4",
        "outputId": "2234f3bf-abf8-4599-c4a5-527396f4a261"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn1252_c = cumulative_log_returns_nn1_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn1252_c)\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn1252_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn1252_c)\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn1252_wc = cumulative_log_returns_nn1_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn1252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn1252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn1252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10ID2AKhVL_W"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "MQ3xg0N_VNUV",
        "outputId": "60c5c857-8dfe-4545-a0b1-16e9bdcc38ef"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN1` to predict excess returns\n",
        "def nn1_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the NN1 model\n",
        "    result_df = neural_network_nn1(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn1_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN1 model (this should generate binary outcomes)\n",
        "crsp_test_lagged = nn1_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn1_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn1_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Step 6: Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn1512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_nn1512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn1512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_nn1_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_nn1512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_nn1_lag_512.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "VcExAX8fV9wO",
        "outputId": "84ba50e6-3bff-4632-b3a1-6ee208456b29"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn1512_c = cumulative_log_returns_nn1_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn1512_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn1512_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn1512_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn1512_wc = cumulative_log_returns_nn1_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn1512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn1512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn1512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFdMDZcFaB5_"
      },
      "source": [
        "## NN2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "GUPP1g30aDJp",
        "outputId": "531cf8a9-ce12-406c-a732-ec7d33a1b947"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "nn2_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn2', lag=[5])\n",
        "print(\"NN2 Results (Lag 5):\")\n",
        "display(nn2_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "V00x4vIiaIFu",
        "outputId": "76381cc8-8f75-48ba-a33f-8a9fde1edfdd"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "nn2_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn2', lag=[21])\n",
        "print(\"NN2 Results (Lag 21):\")\n",
        "display(nn2_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "CsyJJcnqaMR6",
        "outputId": "742c4d49-a375-47b1-c9e7-562670c69297"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "nn2_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn2', lag=[252])\n",
        "print(\"NN2 Results (Lag 252):\")\n",
        "display(nn2_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "eEZzMNOuaQWO",
        "outputId": "2edca615-f816-4df6-efbf-beda78c0c275"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "nn2_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn2', lag=[512])\n",
        "print(\"NN2 Results (Lag 512):\")\n",
        "display(nn2_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7SVznSsZM9"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwR2jMk6saqL"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Utility: Adjusted R²\n",
        "def adjusted_r2_score(y_true, y_pred, n, p):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Utility: Calculate OOS R² for NN2\n",
        "def calculate_oos_r2_nn2(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return r2_score(y_test, y_pred)\n",
        "\n",
        "# NN2 Model with hyperparameter tuning\n",
        "def nn2_model_with_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'hidden_layer_sizes': [(32, 16)],  # Two hidden layers: 32 and 16 neurons\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'max_iter': [200, 500],\n",
        "        'learning_rate_init': [0.001, 0.01]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(MLPRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# NN2 Model with OOS R² calculation\n",
        "def nn2_model_with_tuning_loss(train_df, test_df, lag=5):\n",
        "    feature = f'lag_{lag}'\n",
        "    df_train = train_df.dropna(subset=[feature, 'directional_target'])\n",
        "    df_test = test_df.dropna(subset=[feature, 'directional_target'])\n",
        "\n",
        "    X_train = df_train[[feature]]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[[feature]]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Tune and train NN2 model\n",
        "    model = nn2_model_with_tuning(X_train, y_train)\n",
        "\n",
        "    # Calculate OOS R²\n",
        "    oos_r2 = calculate_oos_r2_nn2(model, X_test, y_test)\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        'Lag': lag,\n",
        "        'OOS R²': oos_r2\n",
        "    }])\n",
        "\n",
        "# Main function to calculate OOS R² for NN2 with market cap segmentation\n",
        "def calculate_oos_r2_market_cap_nn2(train_df, test_df, valid_lags):\n",
        "    # Calculate the quantiles for the market cap\n",
        "    top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "    bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "    # Filter top 25% and bottom 25% based on market cap\n",
        "    top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "    bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "    # Create empty DataFrames to store results\n",
        "    nn2_oos_r2_all = pd.DataFrame()\n",
        "    nn2_oos_r2_top_25 = pd.DataFrame()\n",
        "    nn2_oos_r2_bottom_25 = pd.DataFrame()\n",
        "\n",
        "    # Loop through each lag and calculate OOS R² for all, top 25% and bottom 25% stocks\n",
        "    for lag in valid_lags:\n",
        "        # For All Stocks\n",
        "        nn2_oos_r2_all = pd.concat([nn2_oos_r2_all, nn2_model_with_tuning_loss(train_df, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Top 25% Stocks\n",
        "        nn2_oos_r2_top_25 = pd.concat([nn2_oos_r2_top_25, nn2_model_with_tuning_loss(top_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Bottom 25% Stocks\n",
        "        nn2_oos_r2_bottom_25 = pd.concat([nn2_oos_r2_bottom_25, nn2_model_with_tuning_loss(bottom_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "    # Merge the results into a single DataFrame with Lag as index\n",
        "    results_df = pd.DataFrame({\n",
        "        'Lag': valid_lags,\n",
        "        'All Stocks R²': nn2_oos_r2_all['OOS R²'].values,\n",
        "        'Top 25% R²': nn2_oos_r2_top_25['OOS R²'].values,\n",
        "        'Bottom 25% R²': nn2_oos_r2_bottom_25['OOS R²'].values\n",
        "    })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Example: Define lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Calculate OOS R² for NN2 with all stocks, top 25%, and bottom 25%\n",
        "nn2_oos_r2_results = calculate_oos_r2_market_cap_nn2(crsp_train_lagged, crsp_test_lagged, valid_lags)\n",
        "\n",
        "# Display the results in the required format\n",
        "display(nn2_oos_r2_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p198M_kbOaf"
      },
      "source": [
        "## NN2 Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96SqfTccbajG"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "B18rD9x3bRFX",
        "outputId": "3cad851e-733c-4c2b-e955-d81833299163"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN2` to predict excess returns\n",
        "def nn2_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the NN2 model\n",
        "    result_df = neural_network_nn2(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn2_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN2 model\n",
        "crsp_test_lagged = nn2_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn2_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn2_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn25  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_nn25['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn25['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn25['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_nn2_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_nn25)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_nn2_lag_5.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "redDYojNb0mT",
        "outputId": "5074c83e-c406-46bb-a42d-a13b41d1a9a3"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn25_c = cumulative_log_returns_nn2_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn25_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn25_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn25_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn25_wc = cumulative_log_returns_nn2_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn25_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn25_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn25_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxffWi4BcE19"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "IyjOQIKrcGjR",
        "outputId": "e81b6343-7540-4fb9-b8bb-6b0058bfbc4c"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN2` to predict excess returns\n",
        "def nn2_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the NN2 model\n",
        "    result_df = neural_network_nn2(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn2_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN2 model\n",
        "crsp_test_lagged = nn2_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn2_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn2_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn221  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_nn221['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn221['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn221['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_nn2_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_nn221)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_nn2_lag_21.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "r9wLe9_udQIX",
        "outputId": "5476aa0e-6ccb-4c97-82eb-e30f45dd003b"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn221_c = cumulative_log_returns_nn2_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn221_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn221_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn221_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn221_wc = cumulative_log_returns_nn2_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn221_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn221_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn221_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP7N1eC2e9gy"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "W4wsOxMAe_2s",
        "outputId": "17229565-acf5-4093-c10b-dc66721084f1"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "#  Use the previously defined `NN2` to predict excess returns\n",
        "def nn2_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the NN2 model\n",
        "    result_df = neural_network_nn2(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn2_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN2 model\n",
        "crsp_test_lagged = nn2_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn2_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn2_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn2252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_nn2252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_nn2_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_nn2252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_nn2_lag_252.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "HZqQDwM2iRQd",
        "outputId": "5eb28f33-7b05-4575-82c9-ca64519a97bb"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn2252_c = cumulative_log_returns_nn2_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn2252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn2252_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn2252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn2252_wc = cumulative_log_returns_nn2_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn2252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn2252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn2252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hWwLT1Kix7H"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "Go5Wr5dsi2RZ",
        "outputId": "475cbd3c-376c-40e4-889a-272d7bc90187"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN2` to predict excess returns\n",
        "def nn2_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the NN2 model\n",
        "    result_df = neural_network_nn2(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result (it’s added to crsp_test_lagged directly)\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn2_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN2 model (this should generate binary outcomes)\n",
        "crsp_test_lagged = nn2_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn2_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn2_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn2512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_nn2512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn2512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_nn2_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_nn2512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_nn2_lag_512.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "1QLMicgekqYi",
        "outputId": "3cd36621-b69b-434c-8fa6-7c47b2c62ee5"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns (for other metrics)\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns (for volatility and standard deviation)\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn2512_c = cumulative_log_returns_nn2_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn2512_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn2512_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn2512_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn2512_wc = cumulative_log_returns_nn2_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn2512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn2512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn2512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S7So_3taLTm"
      },
      "source": [
        "## NN3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "rcjfTsvOaX4O",
        "outputId": "b3a12e05-7a63-4a9c-d501-f13723a10151"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "nn3_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn3', lag=[5])\n",
        "print(\"NN3 Results (Lag 5):\")\n",
        "display(nn3_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "NCXS6a38aeav",
        "outputId": "d697e480-3fba-4bcf-9ae5-53ca73b64972"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "nn3_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn3', lag=[21])\n",
        "print(\"NN3 Results (Lag 21):\")\n",
        "display(nn3_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "RS7uJVZeaibI",
        "outputId": "593a0b79-e759-471a-9268-1785b8ae1e34"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "nn3_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn3', lag=[252])\n",
        "print(\"NN3 Results (Lag 252):\")\n",
        "display(nn3_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "zcQCo1UYamBn",
        "outputId": "0cbcfb21-c6d3-4c2d-eb1b-7136b31af944"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "nn3_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn3', lag=[512])\n",
        "print(\"NN3 Results (Lag 512):\")\n",
        "display(nn3_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW5AMY23sqph"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYNf9Ssfsrzs"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Adjusted R² calculation\n",
        "def adjusted_r2_score(y_true, y_pred, n, p):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Utility: Calculate OOS R² for NN3\n",
        "def calculate_oos_r2_nn3(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return r2_score(y_test, y_pred)\n",
        "\n",
        "# NN3 Model with hyperparameter tuning\n",
        "def nn3_model_with_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'hidden_layer_sizes': [(32, 16, 8)],  # Three hidden layers: 32, 16, and 8 neurons\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'max_iter': [200, 500],\n",
        "        'learning_rate_init': [0.001, 0.01]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(MLPRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# NN3 Model with OOS R² loss and tuning\n",
        "def nn3_model_with_tuning_loss(train_df, test_df, lag=5):\n",
        "    feature = f'lag_{lag}'\n",
        "    df_train = train_df.dropna(subset=[feature, 'directional_target'])\n",
        "    df_test = test_df.dropna(subset=[feature, 'directional_target'])\n",
        "\n",
        "    X_train = df_train[[feature]]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[[feature]]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Tune and train NN3 model\n",
        "    model = nn3_model_with_tuning(X_train, y_train)\n",
        "\n",
        "    # Calculate OOS R²\n",
        "    oos_r2 = calculate_oos_r2_nn3(model, X_test, y_test)\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        'Lag': lag,\n",
        "        'OOS R²': oos_r2\n",
        "    }])\n",
        "\n",
        "# Main function to calculate OOS R² for NN3 with market cap segmentation\n",
        "def calculate_oos_r2_market_cap_nn3(train_df, test_df, valid_lags):\n",
        "    # Calculate the quantiles for the market cap\n",
        "    top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "    bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "    # Filter top 25% and bottom 25% based on market cap\n",
        "    top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "    bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "    # Create empty DataFrames to store results\n",
        "    nn3_oos_r2_all = pd.DataFrame()\n",
        "    nn3_oos_r2_top_25 = pd.DataFrame()\n",
        "    nn3_oos_r2_bottom_25 = pd.DataFrame()\n",
        "\n",
        "    # Loop through each lag and calculate OOS R² for all, top 25% and bottom 25% stocks\n",
        "    for lag in valid_lags:\n",
        "        # For All Stocks\n",
        "        nn3_oos_r2_all = pd.concat([nn3_oos_r2_all, nn3_model_with_tuning_loss(train_df, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Top 25% Stocks\n",
        "        nn3_oos_r2_top_25 = pd.concat([nn3_oos_r2_top_25, nn3_model_with_tuning_loss(top_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Bottom 25% Stocks\n",
        "        nn3_oos_r2_bottom_25 = pd.concat([nn3_oos_r2_bottom_25, nn3_model_with_tuning_loss(bottom_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "    # Merge the results into a single DataFrame with Lag as index\n",
        "    results_df = pd.DataFrame({\n",
        "        'Lag': valid_lags,\n",
        "        'All Stocks R²': nn3_oos_r2_all['OOS R²'].values,\n",
        "        'Top 25% R²': nn3_oos_r2_top_25['OOS R²'].values,\n",
        "        'Bottom 25% R²': nn3_oos_r2_bottom_25['OOS R²'].values\n",
        "    })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Example: Define lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Calculate OOS R² for NN3 with all stocks, top 25%, and bottom 25%\n",
        "nn3_oos_r2_results = calculate_oos_r2_market_cap_nn3(crsp_train_lagged, crsp_test_lagged, valid_lags)\n",
        "\n",
        "# Display the results in the required format\n",
        "display(nn3_oos_r2_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzC6j9Fmk0f4"
      },
      "source": [
        "## NN3 Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV_n1onMk4nq"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "JfiBs874k8a3",
        "outputId": "7319cb56-5f73-4101-fcd0-6f5a8681bc7e"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN3` to predict excess returns\n",
        "def nn3_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the NN3 model\n",
        "    result_df = neural_network_nn3(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn3_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN3 model\n",
        "crsp_test_lagged = nn3_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn3_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn3_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn35  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_nn35['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn35['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn35['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_nn3_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_nn35)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_nn3_lag_5.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "BervfCZclZWE",
        "outputId": "94371434-8d02-41c6-fe17-3ea5ae3ecc34"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn35_c = cumulative_log_returns_nn3_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn35_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn35_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn35_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn35_wc = cumulative_log_returns_nn3_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn35_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn35_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn35_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f98CtirYlmz0"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "tI6rxVqtlpIr",
        "outputId": "202bcc3e-e19d-439b-dd76-b3bb8b81b065"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN3` to predict excess returns\n",
        "def nn3_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the NN3 model\n",
        "    result_df = neural_network_nn3(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    #  use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn3_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN3 model\n",
        "crsp_test_lagged = nn3_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn3_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn3_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn321  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_nn321['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn321['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn321['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_nn3_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_nn321)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_nn3_lag_21.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "cptO_yiMl7FF",
        "outputId": "ab30b34e-05cd-456f-aae3-4b81afc7def6"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn321_c = cumulative_log_returns_nn3_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn321_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn321_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn321_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn321_wc = cumulative_log_returns_nn3_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn321_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn321_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn321_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkyY3gtamWsp"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "kWemB-7vmYdO",
        "outputId": "6ce90a41-a1f9-4a1e-9bca-198c187a97e1"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Step 4: Use the previously defined `NN3` to predict excess returns\n",
        "def nn3_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the NN3 model\n",
        "    result_df = neural_network_nn3(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn3_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN3 model\n",
        "crsp_test_lagged = nn3_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn3_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn3_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn3252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_nn3252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_nn3_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_nn3252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_nn3_lag_252.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "3lWbhrmZm0bu",
        "outputId": "07949f3a-5828-4f1e-9a69-180a69f3626a"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn3252_c = cumulative_log_returns_nn3_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn3252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn3252_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn3252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn3252_wc = cumulative_log_returns_nn3_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn3252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn3252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn3252_wc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgNgsAoHnKde"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "LKVXRUC5nMx-",
        "outputId": "8f05dc18-0103-40af-9948-10690c6de41f"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN3` to predict excess returns\n",
        "def nn3_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the NN3 model\n",
        "    result_df = neural_network_nn3(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn3_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN3 model\n",
        "crsp_test_lagged = nn3_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn3_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn3_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn3512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_nn3512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn3512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_nn3_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_nn3512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_nn3_lag_512.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "w7PIFlUdnWkD",
        "outputId": "6fc5667c-82bf-48e4-84ea-9655b9a7c3ed"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn3512_c = cumulative_log_returns_nn3_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn3512_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn3512_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn3512_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn3512_wc = cumulative_log_returns_nn3_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn3512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn3512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn3512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gis0xTsaapj6"
      },
      "source": [
        "## NN4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "JH6B81ZCaqmn",
        "outputId": "5eb6a42c-abe7-4c6e-c38f-36405b10ab7c"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "nn4_lag5 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn4', lag=[5])\n",
        "print(\"NN4 Results (Lag 5):\")\n",
        "display(nn4_lag5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "psU_4G63at97",
        "outputId": "c76aebcc-80d5-4be8-f48d-5d0bbcbb60aa"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "nn4_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn4', lag=[21])\n",
        "print(\"NN4 Results (Lag 21):\")\n",
        "display(nn4_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "nfHwmQWCay9M",
        "outputId": "4f19fe96-0642-4ff7-a871-466d05f68c07"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "nn4_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn4', lag=[252])\n",
        "print(\"NN4 Results (Lag 252):\")\n",
        "display(nn4_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "JaWEPdWva2DD",
        "outputId": "676be7be-d3f4-4845-e4f7-355fa7c1ad40"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "nn4_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn4', lag=[512])\n",
        "print(\"NN4 Results (Lag 512):\")\n",
        "display(nn4_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQnn3Hgns7KL"
      },
      "source": [
        "### OOS R2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGAVf3-Ss8Qh"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Utility: Adjusted R² calculation\n",
        "def adjusted_r2_score(y_true, y_pred, n, p):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "# Utility: Calculate OOS R² for NN4\n",
        "def calculate_oos_r2_nn4(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    return r2_score(y_test, y_pred)\n",
        "\n",
        "# NN4 Model with hyperparameter tuning (4 hidden layers)\n",
        "def nn4_model_with_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'hidden_layer_sizes': [(32, 16, 8, 4)],  # Four hidden layers: 32, 16, 8, and 4 neurons\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'max_iter': [200, 500],\n",
        "        'learning_rate_init': [0.001, 0.01]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(MLPRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# NN4 Model with OOS R² loss and tuning\n",
        "def nn4_model_with_tuning_loss(train_df, test_df, lag=5):\n",
        "    feature = f'lag_{lag}'\n",
        "    df_train = train_df.dropna(subset=[feature, 'directional_target'])\n",
        "    df_test = test_df.dropna(subset=[feature, 'directional_target'])\n",
        "\n",
        "    X_train = df_train[[feature]]\n",
        "    y_train = df_train['directional_target']\n",
        "    X_test = df_test[[feature]]\n",
        "    y_test = df_test['directional_target']\n",
        "\n",
        "    # Tune and train NN4 model\n",
        "    model = nn4_model_with_tuning(X_train, y_train)\n",
        "\n",
        "    # Calculate OOS R²\n",
        "    oos_r2 = calculate_oos_r2_nn4(model, X_test, y_test)\n",
        "\n",
        "    return pd.DataFrame([{\n",
        "        'Lag': lag,\n",
        "        'OOS R²': oos_r2\n",
        "    }])\n",
        "\n",
        "# Main function to calculate OOS R² for NN4 with market cap segmentation\n",
        "def calculate_oos_r2_market_cap_nn4(train_df, test_df, valid_lags):\n",
        "    # Calculate the quantiles for the market cap\n",
        "    top_25_percentile = merged_df['market_cap'].quantile(0.75)\n",
        "    bottom_25_percentile = merged_df['market_cap'].quantile(0.25)\n",
        "\n",
        "    # Filter top 25% and bottom 25% based on market cap\n",
        "    top_25_stocks = merged_df[merged_df['market_cap'] >= top_25_percentile]\n",
        "    bottom_25_stocks = merged_df[merged_df['market_cap'] <= bottom_25_percentile]\n",
        "\n",
        "    # Create empty DataFrames to store results\n",
        "    nn4_oos_r2_all = pd.DataFrame()\n",
        "    nn4_oos_r2_top_25 = pd.DataFrame()\n",
        "    nn4_oos_r2_bottom_25 = pd.DataFrame()\n",
        "\n",
        "    # Loop through each lag and calculate OOS R² for all, top 25% and bottom 25% stocks\n",
        "    for lag in valid_lags:\n",
        "        # For All Stocks\n",
        "        nn4_oos_r2_all = pd.concat([nn4_oos_r2_all, nn4_model_with_tuning_loss(train_df, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Top 25% Stocks\n",
        "        nn4_oos_r2_top_25 = pd.concat([nn4_oos_r2_top_25, nn4_model_with_tuning_loss(top_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "        # For Bottom 25% Stocks\n",
        "        nn4_oos_r2_bottom_25 = pd.concat([nn4_oos_r2_bottom_25, nn4_model_with_tuning_loss(bottom_25_stocks, test_df, lag)], ignore_index=True)\n",
        "\n",
        "    # Merge the results into a single DataFrame with Lag as index\n",
        "    results_df = pd.DataFrame({\n",
        "        'Lag': valid_lags,\n",
        "        'All Stocks R²': nn4_oos_r2_all['OOS R²'].values,\n",
        "        'Top 25% R²': nn4_oos_r2_top_25['OOS R²'].values,\n",
        "        'Bottom 25% R²': nn4_oos_r2_bottom_25['OOS R²'].values\n",
        "    })\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Example: Define lags\n",
        "valid_lags = [5, 21, 252, 512]\n",
        "\n",
        "# Calculate OOS R² for NN4 with all stocks, top 25%, and bottom 25%\n",
        "nn4_oos_r2_results = calculate_oos_r2_market_cap_nn4(crsp_train_lagged, crsp_test_lagged, valid_lags)\n",
        "\n",
        "# Display the results in the required format\n",
        "display(nn4_oos_r2_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D8lojtMn28N"
      },
      "source": [
        "## NN4 Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIQMEEFsoU_L"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "xlufTndGn4i-",
        "outputId": "709e303f-f4f6-4359-91b8-e7c1daadbd1f"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN4` to predict excess returns\n",
        "def nn4_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the NN4 model\n",
        "    result_df = neural_network_nn4(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn4_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN4 model\n",
        "crsp_test_lagged = nn4_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn4_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn4_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn45  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_nn45['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn45['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn45['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_nn4_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_nn45)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_nn4_lag_5.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "WyV03eeCoIME",
        "outputId": "c4d19744-1b03-4c07-c5da-11edc39400a5"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn45_c = cumulative_log_returns_nn4_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn45_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn45_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn45_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn45_wc = cumulative_log_returns_nn4_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn45_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn45_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn45_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL5FS31UoWy5"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "WjwpLzfIoYGi",
        "outputId": "7a526bde-7104-4955-8465-766fb4f73eb9"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN4` to predict excess returns\n",
        "def nn4_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the NN4 model\n",
        "    result_df = neural_network_nn4(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn4_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN4 model\n",
        "crsp_test_lagged = nn4_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn4_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn4_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn421  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_nn421['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn421['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn421['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_nn4_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_nn421)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_nn4_lag_21.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "OGqo6dzIetG-",
        "outputId": "5139c2f5-797d-4a3a-975d-357a4731c681"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn421_c = cumulative_log_returns_nn4_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn421_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn421_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn421_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn421_wc = cumulative_log_returns_nn4_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn421_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn421_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn421_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxg9Yj9lrmtt"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "YSLImKOTrsJX",
        "outputId": "945ad23b-bad5-4cba-ccbf-33e304b78967"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "#  Use the previously defined `NN4` to predict excess returns\n",
        "def nn4_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the NN4 model\n",
        "    result_df = neural_network_nn4(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn4_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN4 model\n",
        "crsp_test_lagged = nn4_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn4_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn4_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn4252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_nn4252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_nn4_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_nn4252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_nn4_lag_252.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "9iF-JY7wsGtk",
        "outputId": "702a8b7b-8ad4-4c41-edb1-12f3e0ce2ec0"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn4252_c = cumulative_log_returns_nn4_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn4252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn4252_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn4252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn4252_wc = cumulative_log_returns_nn4_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn4252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn4252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn4252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPWt2iXqsSlU"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "13clHxz4sUHX",
        "outputId": "224e958f-b2fa-4a07-fbd2-0e8e08725c0d"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Step 4: Use the previously defined `NN4` to predict excess returns\n",
        "def nn4_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the NN4 model\n",
        "    result_df = neural_network_nn4(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # Now you can use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn4_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN4 model\n",
        "crsp_test_lagged = nn4_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn4_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn4_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn4512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_nn4512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn4512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_nn4_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_nn4512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_nn4_lag_512.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "iEfJ4TOwsYmD",
        "outputId": "3ad0cb8a-cce5-4760-deec-317b0510e4b7"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn4512_c = cumulative_log_returns_nn4_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn4512_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn4512_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn4512_c)\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn4512_wc = cumulative_log_returns_nn4_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn4512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn4512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn4512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJngN6fXa4sg"
      },
      "source": [
        "## NN5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "WPEVpLiOCxlf",
        "outputId": "bd73af14-6e7a-4e2d-b1bb-687084d8de26"
      },
      "outputs": [],
      "source": [
        "# lag 5\n",
        "nn5_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn5', lag=[5])\n",
        "print(\"NN5 Results (Lag 5):\")\n",
        "display(nn5_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "xxnoB37UbAv0",
        "outputId": "84574301-444b-435c-8799-0ddaec0a20cd"
      },
      "outputs": [],
      "source": [
        "# lag 21\n",
        "nn5_lag21 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn5', lag=[21])\n",
        "print(\"NN5 Results (Lag 21):\")\n",
        "display(nn5_lag21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "YiaR-es2bC8R",
        "outputId": "c63afd08-79a1-4512-d5c2-fea1dace306b"
      },
      "outputs": [],
      "source": [
        "# lag 252\n",
        "nn5_lag252 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn5', lag=[252])\n",
        "print(\"NN5 Results (Lag 252):\")\n",
        "display(nn5_lag252)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "9tv1wp5rcKZo",
        "outputId": "d405ead0-4b98-4e66-bbb0-a79ee7208beb"
      },
      "outputs": [],
      "source": [
        "# lag 512\n",
        "nn5_lag512 = rolling_forecast_model(crsp_train_lagged, crsp_test_lagged, model_type='nn5', lag=[512])\n",
        "print(\"NN5 Results (Lag 512):\")\n",
        "display(nn5_lag512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et7Eqg7nLo9o"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIIrDMmisxOq"
      },
      "source": [
        "## NN5 Portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egNlSB3lsy8a"
      },
      "source": [
        "### Lag 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "9uMVoSj8s0-T",
        "outputId": "cc30f2c9-b857-401c-9b44-d7cf4da570f8"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN5` to predict excess returns\n",
        "def nn5_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[5]):\n",
        "    # Get the predicted excess returns using the NN5 model\n",
        "    result_df = neural_network_nn5(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn5_5_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN5 model\n",
        "crsp_test_lagged = nn5_5_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn5_5_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn5_5_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn55  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_5_with_cost': [],\n",
        "    'cum_ES_return_5_with_cost': [],\n",
        "    'cum_ELS_return_5_with_cost': [],\n",
        "    'cum_VL_return_5_with_cost': [],\n",
        "    'cum_VS_return_5_with_cost': [],\n",
        "    'cum_VLS_return_5_with_cost': [],\n",
        "    'cum_EL_return_5_without_cost': [],\n",
        "    'cum_ES_return_5_without_cost': [],\n",
        "    'cum_ELS_return_5_without_cost': [],\n",
        "    'cum_VL_return_5_without_cost': [],\n",
        "    'cum_VS_return_5_without_cost': [],\n",
        "    'cum_VLS_return_5_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 5\n",
        "cum_EL_return_5_with_cost = 0\n",
        "cum_ES_return_5_with_cost = 0\n",
        "cum_ELS_return_5_with_cost = 0\n",
        "cum_VL_return_5_with_cost = 0\n",
        "cum_VS_return_5_with_cost = 0\n",
        "cum_VLS_return_5_with_cost = 0\n",
        "\n",
        "cum_EL_return_5_without_cost = 0\n",
        "cum_ES_return_5_without_cost = 0\n",
        "cum_ELS_return_5_without_cost = 0\n",
        "cum_VL_return_5_without_cost = 0\n",
        "cum_VS_return_5_without_cost = 0\n",
        "cum_VLS_return_5_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 5 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 5\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 5\n",
        "    cum_EL_return_5_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_5_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_5_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_5_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_5_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_5_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_5_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_5_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_5_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_5_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_5_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_5_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 5 portfolios\n",
        "    cumulative_log_returns_by_date_nn55['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn55['cum_EL_return_5_with_cost'].append(cum_EL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_ES_return_5_with_cost'].append(cum_ES_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_ELS_return_5_with_cost'].append(cum_ELS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_VL_return_5_with_cost'].append(cum_VL_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_VS_return_5_with_cost'].append(cum_VS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_VLS_return_5_with_cost'].append(cum_VLS_return_5_with_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_EL_return_5_without_cost'].append(cum_EL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_ES_return_5_without_cost'].append(cum_ES_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_ELS_return_5_without_cost'].append(cum_ELS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_VL_return_5_without_cost'].append(cum_VL_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_VS_return_5_without_cost'].append(cum_VS_return_5_without_cost)\n",
        "    cumulative_log_returns_by_date_nn55['cum_VLS_return_5_without_cost'].append(cum_VLS_return_5_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 5\n",
        "cumulative_log_returns_nn5_lag_5 = pd.DataFrame(cumulative_log_returns_by_date_nn55)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 5\n",
        "display(cumulative_log_returns_nn5_lag_5.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_nn5_lag_5.to_csv(\"cumulative_log_returns_nn5_lag_5.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "vufcHvPY5_SE",
        "outputId": "b3879d1f-d81b-4384-dec8-30f0046aa60b"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_5_with_cost', 'cum_ES_return_5_with_cost', 'cum_ELS_return_5_with_cost',\n",
        "    'cum_VL_return_5_with_cost', 'cum_VS_return_5_with_cost', 'cum_VLS_return_5_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn55_c = cumulative_log_returns_nn5_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn55_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn55_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn55_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_5_without_cost', 'cum_ES_return_5_without_cost', 'cum_ELS_return_5_without_cost',\n",
        "    'cum_VL_return_5_without_cost', 'cum_VS_return_5_without_cost', 'cum_VLS_return_5_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn55_wc = cumulative_log_returns_nn5_lag_5[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn55_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn55_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn55_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHsV5p7y6KVH"
      },
      "source": [
        "### Lag 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "mvZ7SeCdhbR1",
        "outputId": "9d44d45d-40ac-46b8-925e-361f83955f48"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN5` to predict excess returns\n",
        "def nn5_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[21]):\n",
        "    # Get the predicted excess returns using the NN5 model\n",
        "    result_df = neural_network_nn5(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn5_21_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN5 model\n",
        "crsp_test_lagged = nn5_21_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn5_21_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn5_21_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn521  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_21_with_cost': [],\n",
        "    'cum_ES_return_21_with_cost': [],\n",
        "    'cum_ELS_return_21_with_cost': [],\n",
        "    'cum_VL_return_21_with_cost': [],\n",
        "    'cum_VS_return_21_with_cost': [],\n",
        "    'cum_VLS_return_21_with_cost': [],\n",
        "    'cum_EL_return_21_without_cost': [],\n",
        "    'cum_ES_return_21_without_cost': [],\n",
        "    'cum_ELS_return_21_without_cost': [],\n",
        "    'cum_VL_return_21_without_cost': [],\n",
        "    'cum_VS_return_21_without_cost': [],\n",
        "    'cum_VLS_return_21_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 21\n",
        "cum_EL_return_21_with_cost = 0\n",
        "cum_ES_return_21_with_cost = 0\n",
        "cum_ELS_return_21_with_cost = 0\n",
        "cum_VL_return_21_with_cost = 0\n",
        "cum_VS_return_21_with_cost = 0\n",
        "cum_VLS_return_21_with_cost = 0\n",
        "\n",
        "cum_EL_return_21_without_cost = 0\n",
        "cum_ES_return_21_without_cost = 0\n",
        "cum_ELS_return_21_without_cost = 0\n",
        "cum_VL_return_21_without_cost = 0\n",
        "cum_VS_return_21_without_cost = 0\n",
        "cum_VLS_return_21_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 21 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 21\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 21\n",
        "    cum_EL_return_21_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_21_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_21_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_21_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_21_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_21_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_21_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_21_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_21_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_21_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_21_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_21_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 21 portfolios\n",
        "    cumulative_log_returns_by_date_nn521['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn521['cum_EL_return_21_with_cost'].append(cum_EL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_ES_return_21_with_cost'].append(cum_ES_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_ELS_return_21_with_cost'].append(cum_ELS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_VL_return_21_with_cost'].append(cum_VL_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_VS_return_21_with_cost'].append(cum_VS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_VLS_return_21_with_cost'].append(cum_VLS_return_21_with_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_EL_return_21_without_cost'].append(cum_EL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_ES_return_21_without_cost'].append(cum_ES_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_ELS_return_21_without_cost'].append(cum_ELS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_VL_return_21_without_cost'].append(cum_VL_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_VS_return_21_without_cost'].append(cum_VS_return_21_without_cost)\n",
        "    cumulative_log_returns_by_date_nn521['cum_VLS_return_21_without_cost'].append(cum_VLS_return_21_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 21\n",
        "cumulative_log_returns_nn5_lag_21 = pd.DataFrame(cumulative_log_returns_by_date_nn521)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 21\n",
        "display(cumulative_log_returns_nn5_lag_21.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_nn5_lag_21.to_csv(\"cumulative_log_returns_nn5_lag_21.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "hKU0NctM6WkE",
        "outputId": "38a7da7f-e4c3-4d9c-e947-622cfd9badf0"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_21_with_cost', 'cum_ES_return_21_with_cost', 'cum_ELS_return_21_with_cost',\n",
        "    'cum_VL_return_21_with_cost', 'cum_VS_return_21_with_cost', 'cum_VLS_return_21_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn521_c = cumulative_log_returns_nn5_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn521_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn521_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn521_c)\n",
        "\n",
        "#  same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_21_without_cost', 'cum_ES_return_21_without_cost', 'cum_ELS_return_21_without_cost',\n",
        "    'cum_VL_return_21_without_cost', 'cum_VS_return_21_without_cost', 'cum_VLS_return_21_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn521_wc = cumulative_log_returns_nn5_lag_21[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn521_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn521_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn521_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM4I4dhG6LsC"
      },
      "source": [
        "### Lag 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "CH1JwS7m6uh0",
        "outputId": "10ad3ccc-ee9e-48da-e188-f6c18ba13676"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN5` to predict excess returns\n",
        "def nn5_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[252]):\n",
        "    # Get the predicted excess returns using the NN5 model\n",
        "    result_df = neural_network_nn5(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn5_252_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN5 model\n",
        "crsp_test_lagged = nn5_252_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn5_252_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn5_252_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn5252  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_252_with_cost': [],\n",
        "    'cum_ES_return_252_with_cost': [],\n",
        "    'cum_ELS_return_252_with_cost': [],\n",
        "    'cum_VL_return_252_with_cost': [],\n",
        "    'cum_VS_return_252_with_cost': [],\n",
        "    'cum_VLS_return_252_with_cost': [],\n",
        "    'cum_EL_return_252_without_cost': [],\n",
        "    'cum_ES_return_252_without_cost': [],\n",
        "    'cum_ELS_return_252_without_cost': [],\n",
        "    'cum_VL_return_252_without_cost': [],\n",
        "    'cum_VS_return_252_without_cost': [],\n",
        "    'cum_VLS_return_252_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 252\n",
        "cum_EL_return_252_with_cost = 0\n",
        "cum_ES_return_252_with_cost = 0\n",
        "cum_ELS_return_252_with_cost = 0\n",
        "cum_VL_return_252_with_cost = 0\n",
        "cum_VS_return_252_with_cost = 0\n",
        "cum_VLS_return_252_with_cost = 0\n",
        "\n",
        "cum_EL_return_252_without_cost = 0\n",
        "cum_ES_return_252_without_cost = 0\n",
        "cum_ELS_return_252_without_cost = 0\n",
        "cum_VL_return_252_without_cost = 0\n",
        "cum_VS_return_252_without_cost = 0\n",
        "cum_VLS_return_252_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 252\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 252\n",
        "    cum_EL_return_252_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_252_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_252_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_252_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_252_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_252_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_252_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_252_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_252_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_252_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_252_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_252_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 252 portfolios\n",
        "    cumulative_log_returns_by_date_nn5252['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_EL_return_252_with_cost'].append(cum_EL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_ES_return_252_with_cost'].append(cum_ES_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_ELS_return_252_with_cost'].append(cum_ELS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_VL_return_252_with_cost'].append(cum_VL_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_VS_return_252_with_cost'].append(cum_VS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_VLS_return_252_with_cost'].append(cum_VLS_return_252_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_EL_return_252_without_cost'].append(cum_EL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_ES_return_252_without_cost'].append(cum_ES_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_ELS_return_252_without_cost'].append(cum_ELS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_VL_return_252_without_cost'].append(cum_VL_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_VS_return_252_without_cost'].append(cum_VS_return_252_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5252['cum_VLS_return_252_without_cost'].append(cum_VLS_return_252_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 252\n",
        "cumulative_log_returns_nn5_lag_252 = pd.DataFrame(cumulative_log_returns_by_date_nn5252)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 252\n",
        "display(cumulative_log_returns_nn5_lag_252.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_nn5_lag_252.to_csv(\"cumulative_log_returns_nn5_lag_252.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "OrqI14qQ6_iP",
        "outputId": "cdecd9f1-6cab-4d87-870d-7c7e52cf0f62"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_252_with_cost', 'cum_ES_return_252_with_cost', 'cum_ELS_return_252_with_cost',\n",
        "    'cum_VL_return_252_with_cost', 'cum_VS_return_252_with_cost', 'cum_VLS_return_252_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn5252_c = cumulative_log_returns_nn5_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn5252_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn5252_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn5252_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_252_without_cost', 'cum_ES_return_252_without_cost', 'cum_ELS_return_252_without_cost',\n",
        "    'cum_VL_return_252_without_cost', 'cum_VS_return_252_without_cost', 'cum_VLS_return_252_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn5252_wc = cumulative_log_returns_nn5_lag_252[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn5252_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn5252_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn5252_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEOi6YPT6My1"
      },
      "source": [
        "### Lag 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "fx9n_fth7agl",
        "outputId": "a4e5c6ee-5a38-4e40-a7a8-8e4852760b27"
      },
      "outputs": [],
      "source": [
        "# Add transaction cost (10bps = 0.001)\n",
        "def calculate_transaction_cost(row):\n",
        "    return 0.001  # 10 bps for both small and large cap stocks\n",
        "\n",
        "crsp_test_lagged['transaction_cost'] = crsp_test_lagged.apply(calculate_transaction_cost, axis=1)\n",
        "\n",
        "# Use the previously defined `NN5` to predict excess returns\n",
        "def nn5_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged, lags=[512]):\n",
        "    # Get the predicted excess returns using the NN5 model\n",
        "    result_df = neural_network_nn5(crsp_train_lagged, crsp_test_lagged, lags)\n",
        "\n",
        "    # Extract the predicted excess returns from the result\n",
        "    predicted_excess_returns = crsp_test_lagged['predicted_excess_returns'].values\n",
        "\n",
        "    # use these predicted returns for portfolio construction\n",
        "    crsp_test_lagged['nn5_512_predicted_excess_returns'] = predicted_excess_returns\n",
        "\n",
        "    return crsp_test_lagged\n",
        "\n",
        "# Get predicted excess returns using NN5 model\n",
        "crsp_test_lagged = nn5_512_predicted_excess_returns(crsp_train_lagged, crsp_test_lagged)\n",
        "\n",
        "# Portfolio Construction (Top 10% Long, Bottom 10% Short)\n",
        "def compute_returns(group):\n",
        "    # Long position (Top 10% based on predicted returns)\n",
        "    top_positive = group.nlargest(int(0.1 * len(group)), 'nn5_512_predicted_excess_returns')\n",
        "    # Short position (Bottom 10% based on predicted returns)\n",
        "    top_negative = group.nsmallest(int(0.1 * len(group)), 'nn5_512_predicted_excess_returns')\n",
        "\n",
        "    # Equal-Weighted Long position return (Top 10%)\n",
        "    equal_long_log_return_with_cost = np.log1p(top_positive['adjusted_ret']).mean() - top_positive['transaction_cost'].mean()\n",
        "    equal_long_log_return_without_cost = np.log1p(top_positive['adjusted_ret']).mean()\n",
        "\n",
        "    # Equal-Weighted Short position return (Bottom 10%)\n",
        "    equal_short_log_return_with_cost = -np.log1p(top_negative['adjusted_ret']).mean() - top_negative['transaction_cost'].mean()\n",
        "    equal_short_log_return_without_cost = -np.log1p(top_negative['adjusted_ret']).mean()\n",
        "\n",
        "    # Value-Weighted Long position return (Top 10%) based on market cap\n",
        "    total_market_cap_positive = top_positive['market_cap_merged'].sum()\n",
        "    value_long_log_return_with_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive - top_positive['transaction_cost'].mean()\n",
        "    value_long_log_return_without_cost = (np.log1p(top_positive['adjusted_ret']) * top_positive['market_cap_merged']).sum() / total_market_cap_positive\n",
        "\n",
        "    # Value-Weighted Short position return (Bottom 10%) based on market cap\n",
        "    total_market_cap_negative = top_negative['market_cap_merged'].sum()\n",
        "    value_short_log_return_with_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative + top_negative['transaction_cost'].mean())\n",
        "    value_short_log_return_without_cost = -((np.log1p(top_negative['adjusted_ret']) * top_negative['market_cap_merged']).sum() / total_market_cap_negative)\n",
        "\n",
        "    # Combine Long and Short to get Long-Short return\n",
        "    equal_long_short_log_return_with_cost = equal_long_log_return_with_cost + equal_short_log_return_with_cost\n",
        "    equal_long_short_log_return_without_cost = equal_long_log_return_without_cost + equal_short_log_return_without_cost\n",
        "\n",
        "    value_long_short_log_return_with_cost = value_long_log_return_with_cost + value_short_log_return_with_cost\n",
        "    value_long_short_log_return_without_cost = value_long_log_return_without_cost + value_short_log_return_without_cost\n",
        "\n",
        "    return {\n",
        "        'equal_long_log_return_with_cost': equal_long_log_return_with_cost,\n",
        "        'equal_short_log_return_with_cost': equal_short_log_return_with_cost,\n",
        "        'equal_long_short_log_return_with_cost': equal_long_short_log_return_with_cost,\n",
        "        'equal_long_log_return_without_cost': equal_long_log_return_without_cost,\n",
        "        'equal_short_log_return_without_cost': equal_short_log_return_without_cost,\n",
        "        'equal_long_short_log_return_without_cost': equal_long_short_log_return_without_cost,\n",
        "        'value_long_log_return_with_cost': value_long_log_return_with_cost,\n",
        "        'value_short_log_return_with_cost': value_short_log_return_with_cost,\n",
        "        'value_long_short_log_return_with_cost': value_long_short_log_return_with_cost,\n",
        "        'value_long_log_return_without_cost': value_long_log_return_without_cost,\n",
        "        'value_short_log_return_without_cost': value_short_log_return_without_cost,\n",
        "        'value_long_short_log_return_without_cost': value_long_short_log_return_without_cost\n",
        "    }\n",
        "\n",
        "# Compute cumulative returns for each date with daily rebalancing\n",
        "cumulative_log_returns_by_date_nn5512  = {\n",
        "    'date': [],\n",
        "    'cum_EL_return_512_with_cost': [],\n",
        "    'cum_ES_return_512_with_cost': [],\n",
        "    'cum_ELS_return_512_with_cost': [],\n",
        "    'cum_VL_return_512_with_cost': [],\n",
        "    'cum_VS_return_512_with_cost': [],\n",
        "    'cum_VLS_return_512_with_cost': [],\n",
        "    'cum_EL_return_512_without_cost': [],\n",
        "    'cum_ES_return_512_without_cost': [],\n",
        "    'cum_ELS_return_512_without_cost': [],\n",
        "    'cum_VL_return_512_without_cost': [],\n",
        "    'cum_VS_return_512_without_cost': [],\n",
        "    'cum_VLS_return_512_without_cost': []\n",
        "}\n",
        "\n",
        "# Initialize cumulative returns for lag 512\n",
        "cum_EL_return_512_with_cost = 0\n",
        "cum_ES_return_512_with_cost = 0\n",
        "cum_ELS_return_512_with_cost = 0\n",
        "cum_VL_return_512_with_cost = 0\n",
        "cum_VS_return_512_with_cost = 0\n",
        "cum_VLS_return_512_with_cost = 0\n",
        "\n",
        "cum_EL_return_512_without_cost = 0\n",
        "cum_ES_return_512_without_cost = 0\n",
        "cum_ELS_return_512_without_cost = 0\n",
        "cum_VL_return_512_without_cost = 0\n",
        "cum_VS_return_512_without_cost = 0\n",
        "cum_VLS_return_512_without_cost = 0\n",
        "\n",
        "# Iterate over each date to compute returns for lag 252 portfolios\n",
        "for date in crsp_test_lagged['date'].unique():\n",
        "    group = crsp_test_lagged[crsp_test_lagged['date'] == date]\n",
        "\n",
        "    # Compute returns for lag 512\n",
        "    returns = compute_returns(group)\n",
        "\n",
        "    # Update cumulative returns with daily values for lag 512\n",
        "    cum_EL_return_512_with_cost += returns['equal_long_log_return_with_cost']\n",
        "    cum_ES_return_512_with_cost += returns['equal_short_log_return_with_cost']\n",
        "    cum_ELS_return_512_with_cost += returns['equal_long_short_log_return_with_cost']\n",
        "    cum_VL_return_512_with_cost += returns['value_long_log_return_with_cost']\n",
        "    cum_VS_return_512_with_cost += returns['value_short_log_return_with_cost']\n",
        "    cum_VLS_return_512_with_cost += returns['value_long_short_log_return_with_cost']\n",
        "\n",
        "    cum_EL_return_512_without_cost += returns['equal_long_log_return_without_cost']\n",
        "    cum_ES_return_512_without_cost += returns['equal_short_log_return_without_cost']\n",
        "    cum_ELS_return_512_without_cost += returns['equal_long_short_log_return_without_cost']\n",
        "    cum_VL_return_512_without_cost += returns['value_long_log_return_without_cost']\n",
        "    cum_VS_return_512_without_cost += returns['value_short_log_return_without_cost']\n",
        "    cum_VLS_return_512_without_cost += returns['value_long_short_log_return_without_cost']\n",
        "\n",
        "    # Append results for the day for lag 512 portfolios\n",
        "    cumulative_log_returns_by_date_nn5512['date'].append(date)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_EL_return_512_with_cost'].append(cum_EL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_ES_return_512_with_cost'].append(cum_ES_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_ELS_return_512_with_cost'].append(cum_ELS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_VL_return_512_with_cost'].append(cum_VL_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_VS_return_512_with_cost'].append(cum_VS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_VLS_return_512_with_cost'].append(cum_VLS_return_512_with_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_EL_return_512_without_cost'].append(cum_EL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_ES_return_512_without_cost'].append(cum_ES_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_ELS_return_512_without_cost'].append(cum_ELS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_VL_return_512_without_cost'].append(cum_VL_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_VS_return_512_without_cost'].append(cum_VS_return_512_without_cost)\n",
        "    cumulative_log_returns_by_date_nn5512['cum_VLS_return_512_without_cost'].append(cum_VLS_return_512_without_cost)\n",
        "\n",
        "# Convert to DataFrame for lag 512\n",
        "cumulative_log_returns_nn5_lag_512 = pd.DataFrame(cumulative_log_returns_by_date_nn5512)\n",
        "\n",
        "# Display the cumulative returns DataFrame for lag 512\n",
        "display(cumulative_log_returns_nn5_lag_512.head())\n",
        "\n",
        "# Saving the DataFrame as a CSV file\n",
        "cumulative_log_returns_nn5_lag_512.to_csv(\"cumulative_log_returns_nn5_lag_512.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "xG_PK9mK7eOy",
        "outputId": "c91984c3-024b-4080-82e4-1297bbe0d82e"
      },
      "outputs": [],
      "source": [
        "# Cumulative returns\n",
        "def cumulative_return(daily_returns):\n",
        "    return np.prod(1 + daily_returns) - 1\n",
        "\n",
        "# Annualized returns\n",
        "def annualized_return(daily_returns, periods=252):\n",
        "    cumulative_return_value = np.prod(1 + daily_returns) - 1\n",
        "    return (1 + cumulative_return_value) ** (periods / len(daily_returns)) - 1\n",
        "\n",
        "# Sharpe ratio\n",
        "def sharpe_ratio(daily_returns, risk_free_rate=0.01, periods=252):\n",
        "    daily_rf = risk_free_rate / periods  # Assuming 252 trading days\n",
        "    excess_returns = daily_returns - daily_rf\n",
        "    return np.sqrt(periods) * excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "# Calculate volatility (standard deviation) of daily returns\n",
        "def calculate_volatility(daily_returns, periods=252):\n",
        "    return np.std(daily_returns) * np.sqrt(periods)\n",
        "\n",
        "# Calculate maximum drawdown\n",
        "def maximum_drawdown(daily_returns):\n",
        "    cum_returns = np.cumprod(1 + daily_returns)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Apply fixed transaction cost to the daily returns\n",
        "def apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001):\n",
        "    # Subtract the transaction cost from each daily return\n",
        "    return daily_returns - transaction_cost\n",
        "\n",
        "# Apply percentage-based transaction cost to the daily returns\n",
        "def apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001):\n",
        "    # Apply transaction cost as a percentage of the return\n",
        "    return daily_returns * (1 - transaction_cost_percentage)\n",
        "\n",
        "# Prepare portfolio names (with transaction cost)\n",
        "portfolios_with_cost = [\n",
        "    'cum_EL_return_512_with_cost', 'cum_ES_return_512_with_cost', 'cum_ELS_return_512_with_cost',\n",
        "    'cum_VL_return_512_with_cost', 'cum_VS_return_512_with_cost', 'cum_VLS_return_512_with_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container\n",
        "metrics = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio with transaction cost\n",
        "for portfolio in portfolios_with_cost:\n",
        "    cumulative_returns_nn5512_c = cumulative_log_returns_nn5_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn5512_c)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Apply fixed transaction cost for other metrics (e.g., cumulative returns, Sharpe ratio)\n",
        "    daily_returns_after_cost_fixed = apply_transaction_cost_fixed(daily_returns, transaction_cost=0.001)\n",
        "\n",
        "    # Apply percentage-based transaction cost for volatility and standard deviation\n",
        "    daily_returns_after_cost_percentage = apply_transaction_cost_percentage(daily_returns, transaction_cost_percentage=0.001)\n",
        "\n",
        "    # Calculate cumulative returns after fixed transaction cost\n",
        "    cum_return_after_cost = cumulative_return(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns_after_cost_fixed)\n",
        "    sharpe = sharpe_ratio(daily_returns_after_cost_fixed)\n",
        "    vol = calculate_volatility(daily_returns_after_cost_percentage)  # Using percentage-based cost for volatility\n",
        "    max_draw = maximum_drawdown(daily_returns_after_cost_fixed)\n",
        "\n",
        "    # Standard Deviation of daily returns after cost (using percentage-based cost for standard deviation)\n",
        "    std_dev = np.std(daily_returns_after_cost_percentage)\n",
        "\n",
        "    # Store results\n",
        "    metrics['Portfolio'].append(portfolio)\n",
        "    metrics['Annualized Return'].append(ann_return)\n",
        "    metrics['Sharpe Ratio'].append(sharpe)\n",
        "    metrics['Volatility'].append(vol)\n",
        "    metrics['Standard Deviation'].append(std_dev)\n",
        "    metrics['Max Drawdown'].append(max_draw)\n",
        "    metrics['Cumulative Return'].append(cum_return_after_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn5512_c = pd.DataFrame(metrics)\n",
        "display(metrics_nn5512_c)\n",
        "\n",
        "\n",
        "# same calculations for portfolios without transaction cost\n",
        "portfolios_without_cost = [\n",
        "    'cum_EL_return_512_without_cost', 'cum_ES_return_512_without_cost', 'cum_ELS_return_512_without_cost',\n",
        "    'cum_VL_return_512_without_cost', 'cum_VS_return_512_without_cost', 'cum_VLS_return_512_without_cost',\n",
        "]\n",
        "\n",
        "# Initialize metrics container for portfolios without transaction cost\n",
        "metrics_wc = {\n",
        "    'Portfolio': [],\n",
        "    'Annualized Return': [],\n",
        "    'Sharpe Ratio': [],\n",
        "    'Volatility': [],\n",
        "    'Standard Deviation': [],\n",
        "    'Max Drawdown': [],\n",
        "    'Cumulative Return': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each portfolio without transaction cost\n",
        "for portfolio in portfolios_without_cost:\n",
        "    cumulative_returns_nn5512_wc = cumulative_log_returns_nn5_lag_512[portfolio].values\n",
        "\n",
        "    # Calculate daily returns from cumulative returns\n",
        "    daily_returns = np.diff(cumulative_returns_nn5512_wc)  # Compute daily returns from cumulative log returns\n",
        "\n",
        "    # Calculate cumulative returns without transaction cost\n",
        "    cum_return_without_cost = cumulative_return(daily_returns)\n",
        "\n",
        "    # Metrics calculations\n",
        "    ann_return = annualized_return(daily_returns)\n",
        "    sharpe = sharpe_ratio(daily_returns)\n",
        "    vol = calculate_volatility(daily_returns)\n",
        "    max_draw = maximum_drawdown(daily_returns)\n",
        "\n",
        "    # Standard Deviation of daily returns without cost\n",
        "    std_dev = np.std(daily_returns)\n",
        "\n",
        "    # Store results\n",
        "    metrics_wc['Portfolio'].append(portfolio)\n",
        "    metrics_wc['Annualized Return'].append(ann_return)\n",
        "    metrics_wc['Sharpe Ratio'].append(sharpe)\n",
        "    metrics_wc['Volatility'].append(vol)\n",
        "    metrics_wc['Standard Deviation'].append(std_dev)\n",
        "    metrics_wc['Max Drawdown'].append(max_draw)\n",
        "    metrics_wc['Cumulative Return'].append(cum_return_without_cost)\n",
        "\n",
        "# Convert the results into a DataFrame for analysis\n",
        "metrics_nn5512_wc = pd.DataFrame(metrics_wc)\n",
        "display(metrics_nn5512_wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_YfMW_bwaGo"
      },
      "source": [
        "# **08. Cumulative Log Return Plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## S&P 500 Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqp4gF4kxE0l",
        "outputId": "839ae2f0-ba9c-4cde-ba40-f7b5c444cd71"
      },
      "outputs": [],
      "source": [
        "pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LvM5hQQxHrK",
        "outputId": "d58b74d5-2455-4a67-f903-1fd37d80f321"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "# Define benchmark date range\n",
        "start_date = '2016-01-01'\n",
        "end_date = '2024-12-31'\n",
        "\n",
        "# Download S&P 500 index with default multi-index columns\n",
        "sp500 = yf.download('^GSPC', start=start_date, end=end_date, auto_adjust=False)\n",
        "\n",
        "# Flatten MultiIndex columns if necessary\n",
        "sp500.columns = [f\"{col[0]}_{col[1]}\" for col in sp500.columns]\n",
        "\n",
        "#  Choose the correct close price column (using 'Adj Close' which is typically the adjusted close price)\n",
        "price_col = 'Adj Close_^GSPC' if 'Adj Close_^GSPC' in sp500.columns else 'Close_^GSPC'\n",
        "\n",
        "#  Calculate log returns\n",
        "sp500['log_return'] = np.log(sp500[price_col] / sp500[price_col].shift(1))\n",
        "sp500 = sp500.dropna()  # Drop any rows with NaN values\n",
        "\n",
        "# Calculate cumulative log returns for S&P 500\n",
        "sp500['cum_SP500_log_return'] = np.cumsum(sp500['log_return'])\n",
        "\n",
        "# Ensure 'date' column is in datetime format\n",
        "sp500['date'] = pd.to_datetime(sp500.index)\n",
        "\n",
        "# Clean up and keep relevant columns\n",
        "sp500 = sp500[[price_col, 'log_return', 'cum_SP500_log_return']].dropna()\n",
        "sp500.reset_index(inplace=True)\n",
        "sp500.rename(columns={'Date': 'date', price_col: 'close'}, inplace=True)\n",
        "\n",
        "# Final result preview\n",
        "print(sp500.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XSflao32kVJ",
        "outputId": "c8dda54a-0397-4157-d901-fbe691e13e0a"
      },
      "outputs": [],
      "source": [
        "sp500['log_return'] = np.log(sp500['close'] / sp500['close'].shift(1))\n",
        "sp500 = sp500.dropna()\n",
        "\n",
        "daily_returns = sp500['log_return'].values  # These are daily log returns\n",
        "\n",
        "# Convert log returns to simple returns\n",
        "simple_returns = np.expm1(daily_returns)  # exp(log_return) - 1 ≈ simple return\n",
        "\n",
        "# Risk-free rate assumption\n",
        "risk_free_rate = 0.01  # 1% annualized\n",
        "periods = 252  # trading days\n",
        "\n",
        "# Metric Functions\n",
        "def cumulative_return(r):\n",
        "    return np.prod(1 + r) - 1\n",
        "\n",
        "def annualized_return(r):\n",
        "    return (1 + cumulative_return(r)) ** (periods / len(r)) - 1\n",
        "\n",
        "def sharpe_ratio(r):\n",
        "    excess = r - risk_free_rate / periods\n",
        "    return np.sqrt(periods) * np.mean(excess) / np.std(excess)\n",
        "\n",
        "def calculate_volatility(r):\n",
        "    return np.std(r) * np.sqrt(periods)\n",
        "\n",
        "def maximum_drawdown(r):\n",
        "    cum_returns = np.cumprod(1 + r)\n",
        "    peak = np.maximum.accumulate(cum_returns)\n",
        "    drawdown = (cum_returns - peak) / peak\n",
        "    return np.min(drawdown)\n",
        "\n",
        "# Compute Metrics\n",
        "sp500_metrics = {\n",
        "    'Cumulative Return': cumulative_return(simple_returns),\n",
        "    'Annualized Return': annualized_return(simple_returns),\n",
        "    'Sharpe Ratio': sharpe_ratio(simple_returns),\n",
        "    'Volatility': calculate_volatility(simple_returns),\n",
        "    'Standard Deviation': np.std(simple_returns),\n",
        "    'Max Drawdown': maximum_drawdown(simple_returns)\n",
        "}\n",
        "\n",
        "# Display as DataFrame\n",
        "sp500_metrics_df = pd.DataFrame(sp500_metrics, index=['S&P 500'])\n",
        "display(sp500_metrics_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGa0W4lfxik-",
        "outputId": "db39ce36-93cb-482a-8975-4bbdd30656a4"
      },
      "outputs": [],
      "source": [
        "# Get the number of observations (rows) in the sp500 dataframe\n",
        "num_observations = sp500.shape[0]\n",
        "\n",
        "print(f\"Number of observations in the S&P 500 data: {num_observations}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Size 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSEThb5cIxnp"
      },
      "source": [
        "### Plot EW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "jbsmOuhiRk8d",
        "outputId": "af8c2238-07ac-4b3d-a94d-a065353c0a67"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import seaborn as sns\n",
        "\n",
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols5,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso5,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge5,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet5,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr5,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm5,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf5,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt5,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn15,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn25,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn35,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn45,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn55\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_EL_return_5_without_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_ES_return_5_without_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Lag 5 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Equal-Weighted_Portfolio_Performance_Lag5_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa5NY7YrJR16"
      },
      "source": [
        "### Plot VW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "Z52hyujvJUWx",
        "outputId": "84f172ad-ef66-4717-9a8f-0ba03c5e5277"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols5,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso5,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge5,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet5,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr5,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm5,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf5,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt5,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn15,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn25,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn35,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn45,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn55\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_VL_return_5_without_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_VS_return_5_without_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Lag 5 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Value-Weighted_Portfolio_Performance_Lag5_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmO_zY43FNlW"
      },
      "source": [
        "### Plot EW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "ZORXoIM2FNQs",
        "outputId": "1f226ddc-a793-4c27-d22d-dc0d1849b26f"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols5,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso5,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge5,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet5,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr5,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm5,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf5,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt5,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn15,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn25,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn35,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn45,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn55\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_EL_return_5_with_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_ES_return_5_with_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Lag 5 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Equal-Weighted_Portfolio_Performance_Lag5_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoZu-WlLeWip"
      },
      "source": [
        "### Plot VW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "YVnABe4MmmkT",
        "outputId": "15e168da-f620-4acd-fe9b-d0d5179535e0"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols5,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso5,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge5,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet5,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr5,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm5,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf5,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt5,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn15,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn25,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn35,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn45,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn55\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_VL_return_5_with_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_VS_return_5_with_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Lag 5 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Value-Weighted_Portfolio_Performance_Lag5_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Rx3JA4Jn6P"
      },
      "source": [
        "### Long-short Plot EW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "sNBGS5hPJpz0",
        "outputId": "bbe44f6b-4df8-4c00-fd8e-feeda03fc58f"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols5,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso5,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge5,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet5,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr5,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm5,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf5,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt5,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn15,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn25,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn35,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn45,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn55\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_5_without_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_ELS_return_5_without_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Long-Short) (Lag 5 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag5_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCPJgeajJkmQ"
      },
      "source": [
        "### Long-short Plot EW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "uphZgdJuJqWJ",
        "outputId": "41531ddc-f2e9-41b9-ac85-6ca3a2d3dcd4"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols5,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso5,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge5,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet5,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr5,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm5,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf5,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt5,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn15,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn25,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn35,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn45,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn55\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_5_with_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_ELS_return_5_with_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Long-Short) (Lag 5 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag5_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnyOljNxnMq2"
      },
      "source": [
        "### Long-short Plot VW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "nC7lcJqDnPAq",
        "outputId": "b4575818-dc15-4122-f76d-70deb97972aa"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols5,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso5,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge5,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet5,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr5,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm5,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf5,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt5,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn15,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn25,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn35,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn45,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn55\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_VLS_return_5_without_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_VLS_return_5_without_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Long-Short) (Lag 5 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag5_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSF0Jh78nPhw"
      },
      "source": [
        "### Long-short Plot VW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "my187lQSnRY7",
        "outputId": "33311fa9-6c9b-4f9a-a4fd-ba2d9a917b62"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols5,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso5,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge5,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet5,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr5,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm5,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf5,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt5,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn15,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn25,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn35,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn45,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn55\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_VLS_return_5_with_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_VLS_return_5_with_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Long-Short) (Lag 5 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag5_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Size 21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSeb4YI-v9s2"
      },
      "source": [
        "### Plot EW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "Xn_gYq9NxTPz",
        "outputId": "bc2ffe62-2936-46e3-83e7-3a6658226974"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols21,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso21,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge21,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet21,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr21,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm21,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf21,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt21,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn121,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn221,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn321,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn421,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn521\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_EL_return_21_without_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_ES_return_21_without_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Lag 21 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Equal-Weighted_Portfolio_Performance_Lag21_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ5ROop5vmMX"
      },
      "source": [
        "### Plot VW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "4TDPMHfFxkcD",
        "outputId": "96f09e65-024a-431b-cd2d-7996c812b005"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols21,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso21,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge21,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet21,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr21,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm21,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf21,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt21,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn121,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn221,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn321,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn421,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn521\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_VL_return_21_without_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_VS_return_21_without_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Lag 21 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles (upper right)\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Value-Weighted_Portfolio_Performance_Lag21_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIxKEwDywEsz"
      },
      "source": [
        "### Plot EW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "na28LLD2zpxk",
        "outputId": "83c732a4-f4fb-4a37-885e-87943ec149b5"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols21,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso21,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge21,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet21,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr21,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm21,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf21,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt21,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn121,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn221,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn321,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn421,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn521\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_EL_return_21_with_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_ES_return_21_with_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Lag 21 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)  # Add this legend inside the plot area\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Equal-Weighted_Portfolio_Performance_Lag21_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLeU32QEv0j_"
      },
      "source": [
        "### Plot VW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "eLUK0JaRz0iu",
        "outputId": "0f031621-4314-46bb-c1cc-de29c26d92c2"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols21,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso21,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge21,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet21,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr21,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm21,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf21,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt21,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn121,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn221,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn321,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn421,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn521\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_VL_return_21_with_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_VS_return_21_with_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Lag 21 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Value-Weighted_Portfolio_Performance_Lag21_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0beEGfRxMZ3"
      },
      "source": [
        "### Long-short Plot EW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "rBYXlaxl0OwQ",
        "outputId": "a7ca8b28-624d-4f94-e33d-80840eed47be"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols21,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso21,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge21,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet21,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr21,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm21,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf21,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt21,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn121,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn221,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn321,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn421,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn521\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_21_without_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_ELS_return_21_without_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Long-Short) (Lag 21 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag21_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG9Sh5niwB3P"
      },
      "source": [
        "### Long-short Plot EW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "Ew_fvxA71GVc",
        "outputId": "0490a7ea-bdde-43c7-ac04-e21423482ee9"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols21,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso21,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge21,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet21,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr21,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm21,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf21,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt21,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn121,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn221,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn321,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn421,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn521\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_21_with_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_ELS_return_21_with_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Long-Short) (Lag 21 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag21_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bIoR0t1xQHK"
      },
      "source": [
        "### Long-short Plot VW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "pSCTlJeG1Uis",
        "outputId": "36202840-16a8-4ac0-d457-7e015e900274"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols21,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso21,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge21,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet21,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr21,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm21,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf21,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt21,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn121,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn221,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn321,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn421,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn521\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_21_without_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_VLS_return_21_without_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Long-Short) (Lag 21 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag21_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgwPzHvNwZ-t"
      },
      "source": [
        "### Long-short Plot VW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "bJM5Suv11iWd",
        "outputId": "68173618-1bfd-411b-8a42-cd1fc499f39a"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols21,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso21,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge21,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet21,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr21,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm21,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf21,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt21,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn121,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn221,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn321,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn421,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn521\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_21_with_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_VLS_return_21_with_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark (optional, you can remove this if not needed)\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Long-Short) (Lag 21 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag21_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Size 252"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwZAFsVphlO_"
      },
      "source": [
        "### Plot EW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "1GBkpHQchrCr",
        "outputId": "2b487d7d-725b-40b3-ed26-d10e89baa182"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols252,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso252,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge252,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet252,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr252,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm252,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf252,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt252,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1252,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2252,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3252,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4252,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5252\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_EL_return_252_without_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_ES_return_252_without_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Lag 252 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Equal-Weighted_Portfolio_Performance_Lag252_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQfnQTD3wnhA"
      },
      "source": [
        "### Plot VW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "Zx4h7D_Ih3l6",
        "outputId": "74658afa-2e36-43a5-ebd5-4522fe31f49d"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols252,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso252,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge252,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet252,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr252,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm252,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf252,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt252,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1252,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2252,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3252,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4252,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5252\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_VL_return_252_without_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_VS_return_252_without_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Lag 252 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Value-Weighted_Portfolio_Performance_Lag252_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQv0B6LZo7sx"
      },
      "source": [
        "### Plot EW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "5vEedvDPo9Rk",
        "outputId": "3049ba36-26fc-4676-a64f-24924a593c4e"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols252,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso252,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge252,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet252,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr252,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm252,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf252,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt252,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1252,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2252,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3252,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4252,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5252\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_EL_return_252_with_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_ES_return_252_with_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Lag 252 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Equal-Weighted_Portfolio_Performance_Lag252_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmQE4agbw0Nd"
      },
      "source": [
        "### Plot VW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "pdNIUPF2pQSN",
        "outputId": "0a605a44-db37-4efb-a769-ef9e21b265d8"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols252,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso252,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge252,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet252,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr252,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm252,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf252,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt252,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1252,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2252,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3252,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4252,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5252\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_VL_return_252_with_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_VS_return_252_with_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Lag 252 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Value-Weighted_Portfolio_Performance_Lag252_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6DVbCABpm-A"
      },
      "source": [
        "### Long-Short Plot EW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "GoAZXnJsprLs",
        "outputId": "8383d804-7caa-4e4c-f61a-1bc0519a5b23"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols252,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso252,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge252,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet252,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr252,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm252,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf252,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt252,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1252,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2252,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3252,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4252,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5252\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_252_without_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_ELS_return_252_without_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark (optional, you can remove this if not needed)\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Long-Short) (Lag 252 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag252_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24iTXWJqxG2Q"
      },
      "source": [
        "### Long-Short Plot (EW) (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "wpDXaQ-uqFEV",
        "outputId": "2ad47553-ebbc-49e2-eb6e-2e2575f935ce"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols252,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso252,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge252,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet252,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr252,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm252,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf252,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt252,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1252,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2252,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3252,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4252,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5252\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_252_with_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_ELS_return_252_with_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Long-Short) (Lag 252 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag252_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeU0LlbMqRXc"
      },
      "source": [
        "### Long-Short Plot VW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "AkdAb04kqajm",
        "outputId": "d7c113b2-b034-4630-bcd6-9fe8609bf05b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols252,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso252,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge252,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet252,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr252,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm252,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf252,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt252,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1252,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2252,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3252,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4252,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5252\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_252_without_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_VLS_return_252_without_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Long-Short) (Lag 252 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag252_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ8AzorJq06H"
      },
      "source": [
        "### Long-Short Plot VW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "rM77wl_Lq1t9",
        "outputId": "4ff36f01-8f82-4307-dcd7-fb4047a0815f"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols252,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso252,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge252,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet252,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr252,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm252,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf252,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt252,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1252,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2252,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3252,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4252,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5252\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_252_with_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_VLS_return_252_with_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark (optional, you can remove this if not needed)\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Long-Short) (Lag 252 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag252_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Window Size 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqe8-WSArWRE"
      },
      "source": [
        "### Plot EW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "tCeApfLxmgUQ",
        "outputId": "cc0a605e-9da8-4bf3-ac55-70aaa18d7104"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols512,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso512,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge512,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet512,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr512,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm512,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf512,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt512,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1512,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2512,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3512,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4512,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5512\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_EL_return_512_without_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_ES_return_512_without_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Lag 512 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Equal-Weighted_Portfolio_Performance_Lag512_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmCVsEVixi6E"
      },
      "source": [
        "### Plot VW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "I0p7LPMWmwcR",
        "outputId": "feea32ca-e1a9-42af-c5e2-8d2ed9eedc1c"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols512,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso512,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge512,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet512,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr512,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm512,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf512,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt512,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1512,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2512,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3512,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4512,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5512\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_VL_return_512_without_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_VS_return_512_without_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Lag 512 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Value-Weighted_Portfolio_Performance_Lag512_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw2GdUf4m-MH"
      },
      "source": [
        "### Plot EW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "kGhACoeum_0R",
        "outputId": "899ee5e1-4a18-4eb9-d32d-839247effd90"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols512,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso512,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge512,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet512,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr512,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm512,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf512,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt512,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1512,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2512,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3512,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4512,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5512\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_EL_return_512_with_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_ES_return_512_with_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Lag 512 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Legend for line styles\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Equal-Weighted_Portfolio_Performance_Lag512_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY3KyWh0xyGu"
      },
      "source": [
        "### Plot VW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "LSTiygh7nPSH",
        "outputId": "88a407b5-dd9a-4e95-94ef-4d28d5ed8a51"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols512,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso512,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge512,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet512,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr512,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm512,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf512,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt512,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1512,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2512,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3512,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4512,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5512\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))  # Increased height for better spacing\n",
        "\n",
        "    # Plot each model's long and short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Long (solid)\n",
        "        ax.plot(model_data['date'], model_data['cum_VL_return_512_with_cost'],\n",
        "                color=color, linestyle='solid')\n",
        "        # Short (dashed)\n",
        "        ax.plot(model_data['date'], model_data['cum_VS_return_512_with_cost'],\n",
        "                color=color, linestyle='dashed')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Lag 512 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for model colors (placed inside the plot on the bottom right)\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)  # Add this legend inside the plot area\n",
        "\n",
        "    # Legend for line styles (upper right)\n",
        "    style_lines = [\n",
        "        Line2D([0], [0], color='black', linestyle='solid', linewidth=2),\n",
        "        Line2D([0], [0], color='black', linestyle='dashed', linewidth=2)\n",
        "    ]\n",
        "    ax.legend(style_lines, ['Long', 'Short'], loc='upper right', fontsize=9,\n",
        "              frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Value-Weighted_Portfolio_Performance_Lag512_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly8AQ9zFnZp8"
      },
      "source": [
        "### Long-short Plot EW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "2eeZgUfnnjDR",
        "outputId": "e950bafa-f6cf-48a9-93d2-23f80f6f7511"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols512,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso512,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge512,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet512,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr512,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm512,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf512,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt512,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1512,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2512,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3512,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4512,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5512\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))  # Increased height for better spacing\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_512_without_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_ELS_return_512_without_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark (optional, you can remove this if not needed)\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Long-Short) (Lag 512 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)  # Add this legend inside the plot area\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag512_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxhQ6ru1x7f9"
      },
      "source": [
        "### Long-short Plot EW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "rRtFbhV4nw8Q",
        "outputId": "84034109-a551-4af0-f8be-a5e22aa1df57"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols512,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso512,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge512,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet512,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr512,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm512,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf512,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt512,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1512,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2512,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3512,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4512,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5512\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_ELS_return_512_with_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_ELS_return_512_with_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark (optional, you can remove this if not needed)\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Equal-Weighted Portfolio Performance (Long-Short) (Lag 512 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)  # Add this legend inside the plot area\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag512_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDNTgNIJniFI"
      },
      "source": [
        "### Long-short Plot VW (Without TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "O57OKLz3n3eA",
        "outputId": "b71df80e-e158-405c-939f-bd686a71b0b0"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols512,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso512,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge512,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet512,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr512,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm512,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf512,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt512,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1512,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2512,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3512,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4512,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5512\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_VLS_return_512_without_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_VLS_return_512_without_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Long-Short) (Lag 512 - No Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag512_NoCost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwBygPV_yGVd"
      },
      "source": [
        "### Long-short Plot VW (With TC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "M1ruEGqHoF8e",
        "outputId": "133e563d-8a58-4c21-ef47-e82ccb658543"
      },
      "outputs": [],
      "source": [
        "# Define models dictionary\n",
        "models = {\n",
        "    \"OLS\": cumulative_log_returns_by_date_ols512,\n",
        "    \"Lasso\": cumulative_log_returns_by_date_lasso512,\n",
        "    \"Ridge\": cumulative_log_returns_by_date_ridge512,\n",
        "    \"ElasticNet\": cumulative_log_returns_by_date_enet512,\n",
        "    \"PCR\": cumulative_log_returns_by_date_pcr512,\n",
        "    \"GLM\": cumulative_log_returns_by_date_glm512,\n",
        "    \"Random Forest\": cumulative_log_returns_by_date_rf512,\n",
        "    \"GBRT\": cumulative_log_returns_by_date_gbrt512,\n",
        "    \"NN1\": cumulative_log_returns_by_date_nn1512,\n",
        "    \"NN2\": cumulative_log_returns_by_date_nn2512,\n",
        "    \"NN3\": cumulative_log_returns_by_date_nn3512,\n",
        "    \"NN4\": cumulative_log_returns_by_date_nn4512,\n",
        "    \"NN5\": cumulative_log_returns_by_date_nn5512\n",
        "}\n",
        "\n",
        "# Color palette\n",
        "color_palette = sns.color_palette(\"tab20\", n_colors=len(models))\n",
        "\n",
        "def plot_portfolio_performance(models, sp500_data, save_path=None):\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "    # Plot each model's combined long-short returns\n",
        "    for i, (name, model_data) in enumerate(models.items()):\n",
        "        color = color_palette[i]\n",
        "\n",
        "        # Plot the combined Long-Short returns (cum_VLS_return_512_with_cost)\n",
        "        ax.plot(model_data['date'], model_data['cum_VLS_return_512_with_cost'],\n",
        "                color=color, linestyle='solid', label=f'{name} (Long-Short)')\n",
        "\n",
        "    # S&P 500 benchmark\n",
        "    sp500_line, = ax.plot(sp500_data['date'], sp500_data['cum_SP500_log_return'],\n",
        "                          color='black', linewidth=2, label='S&P 500')\n",
        "\n",
        "    # Horizontal zero line\n",
        "    ax.axhline(0, color='black', linestyle=':', linewidth=1)\n",
        "\n",
        "    # Y-axis: numeric values\n",
        "    ax.set_ylabel(\"Cumulative Log Return\", fontsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=10)\n",
        "\n",
        "    # X-axis label\n",
        "    ax.set_xlabel(\"Date\", fontsize=12)\n",
        "\n",
        "    # Title\n",
        "    ax.set_title(\"Value-Weighted Portfolio Performance (Long-Short) (Lag 512 - Transaction Cost)\", fontsize=14)\n",
        "\n",
        "    # Grid\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Legend for models and S&P 500\n",
        "    model_handles = [Line2D([0], [0], color=color_palette[i], linewidth=3) for i in range(len(models))]\n",
        "    model_labels = list(models.keys())\n",
        "\n",
        "    # Add the S&P 500 to the model legend\n",
        "    model_handles.append(sp500_line)\n",
        "    model_labels.append('S&P 500')\n",
        "\n",
        "    # Model legend in the upper left\n",
        "    legend1 = ax.legend(model_handles, model_labels, loc='upper left', fontsize=10,\n",
        "                        frameon=True, fancybox=True, facecolor='white', edgecolor='black', framealpha=0.8)\n",
        "    ax.add_artist(legend1)\n",
        "\n",
        "    # Layout adjustment for bottom space\n",
        "    plt.tight_layout(pad=5.0, rect=[0, 0, 1, 0.9])\n",
        "\n",
        "    # Save or show\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "save_path = \"Long-Short_Portfolio_Performance_Lag512_Cost.png\"\n",
        "plot_portfolio_performance(models, sp500, save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5ph1EzpeJJiV",
        "_jabpJqYeBNM",
        "ymbuQhmYpoAe",
        "CO8mfrXfC63P",
        "jKFIgChhBf7i",
        "Nj7Z6BUxBkFC",
        "6xABfRPS16w3",
        "YqwUR9v2Bwhs",
        "UkuK1ztgCCDX",
        "xnZrU9HpCH-l",
        "9O3RXsKRvb49",
        "ib6sKn64BN2B",
        "2D3k98PD96Ot",
        "blP4QT7fQWny",
        "eJHH1_NCrR8u",
        "_cjLVHOSADrs",
        "lB2UN7FCYrrC",
        "T9tmsSYGAGOd",
        "CwtHNmY4RdOu",
        "Kbi-6Dc__3fS",
        "VVqfPnYNAB6P",
        "pEOZeZBtQxdl",
        "BOn8b_shAnig",
        "_ATq-LJ700B8",
        "R38gneeHF0fI",
        "lEk26NF7GFAf",
        "NQqMMcC5Gf-x",
        "heMnYrFCD-Hn",
        "ddHmDWzKG1aI",
        "otN9eLyIOLxq",
        "VEosYqNk0uOf",
        "5BEjzPtKHFHG",
        "93-uaoLAuueB",
        "5TincfIIZspA",
        "OQza7vQcr3HL",
        "4bPkD8aPbd8S",
        "uFdMDZcFaB5_",
        "4p7SVznSsZM9",
        "0p198M_kbOaf",
        "0S7So_3taLTm",
        "rzC6j9Fmk0f4",
        "gis0xTsaapj6",
        "0D8lojtMn28N",
        "YJngN6fXa4sg",
        "BEUqMYfHtF6z",
        "DIIrDMmisxOq",
        "j_YfMW_bwaGo",
        "WmO_zY43FNlW",
        "27Rx3JA4Jn6P",
        "HnyOljNxnMq2",
        "lSeb4YI-v9s2",
        "yIxKEwDywEsz",
        "L0beEGfRxMZ3",
        "0bIoR0t1xQHK",
        "nwZAFsVphlO_",
        "PQv0B6LZo7sx",
        "R6DVbCABpm-A",
        "PeU0LlbMqRXc",
        "Eqe8-WSArWRE",
        "iw2GdUf4m-MH",
        "Ly8AQ9zFnZp8",
        "hDNTgNIJniFI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-benchmark",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
