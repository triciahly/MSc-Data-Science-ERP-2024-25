{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOG7HtcxHwgC"
      },
      "source": [
        "# Window Size 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GzhhOUkGSiKM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import os\n",
        "\n",
        "def diebold_mariano_test(actual, pred1, pred2, h=1):\n",
        "    \"\"\"\n",
        "    Diebold-Mariano test using Mean Squared Error loss function with Newey-West\n",
        "\n",
        "    H0: E[dt] = 0 (equal prediction accuracy)\n",
        "    H1: E[dt] ≠ 0 (unequal prediction accuracy)\n",
        "\n",
        "    Test statistic: DM = d̄ / σ̂_d̄ ~ N(0,1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Align arrays\n",
        "    min_len = min(len(actual), len(pred1), len(pred2))\n",
        "    actual = np.array(actual)[:min_len]\n",
        "    pred1 = np.array(pred1)[:min_len]\n",
        "    pred2 = np.array(pred2)[:min_len]\n",
        "\n",
        "    # Remove missing values\n",
        "    mask = ~(np.isnan(actual) | np.isnan(pred1) | np.isnan(pred2))\n",
        "    actual = actual[mask]\n",
        "    pred1 = pred1[mask]\n",
        "    pred2 = pred2[mask]\n",
        "\n",
        "    if len(actual) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Calculate squared forecast errors\n",
        "    e1_squared = (actual - pred1)**2\n",
        "    e2_squared = (actual - pred2)**2\n",
        "\n",
        "    # Loss differential: dt = e²1,t - e²2,t\n",
        "    d = e1_squared - e2_squared\n",
        "    d_mean = np.mean(d)\n",
        "    n = len(d)\n",
        "\n",
        "    # Newey-West variance estimator\n",
        "    def newey_west_variance(d, h):\n",
        "        n = len(d)\n",
        "        d_centered = d - np.mean(d)\n",
        "\n",
        "        # Bandwidth selection\n",
        "        bandwidth = max(1, int(4 * (n/100)**(2/9)))\n",
        "\n",
        "        # Calculate autocovariances with Bartlett kernel\n",
        "        gamma_0 = np.mean(d_centered**2)\n",
        "        gamma_sum = 0\n",
        "\n",
        "        for k in range(1, min(bandwidth + 1, n)):\n",
        "            if k < n:\n",
        "                weight = 1 - k / (bandwidth + 1)\n",
        "                gamma_k = np.mean(d_centered[k:] * d_centered[:-k])\n",
        "                gamma_sum += 2 * weight * gamma_k\n",
        "\n",
        "        return gamma_0 + gamma_sum\n",
        "\n",
        "    # Calculate long-run variance\n",
        "    long_run_var = newey_west_variance(d, h)\n",
        "\n",
        "    if long_run_var <= 0:\n",
        "        long_run_var = np.var(d, ddof=1)\n",
        "        if long_run_var <= 0:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "    # DM statistic\n",
        "    dm_stat = d_mean / np.sqrt(long_run_var / n)\n",
        "\n",
        "    if np.isnan(dm_stat) or np.isinf(dm_stat):\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Two-tailed p-value\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
        "\n",
        "    return dm_stat, p_value\n",
        "\n",
        "\n",
        "def run_pairwise_dm_tests(actual, predictions_dict):\n",
        "    \"\"\"\n",
        "    Run pairwise DM tests for all model combinations\n",
        "    \"\"\"\n",
        "    model_names = list(predictions_dict.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Initialize matrices\n",
        "    dm_stats = np.full((n_models, n_models), np.nan)\n",
        "    p_values = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    print(f\"Running pairwise DM tests...\")\n",
        "\n",
        "    # Pairwise comparisons\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:  # Skip diagonal\n",
        "                pred1 = predictions_dict[model_a]\n",
        "                pred2 = predictions_dict[model_b]\n",
        "\n",
        "                dm_stat, p_value = diebold_mariano_test(actual, pred1, pred2)\n",
        "\n",
        "                dm_stats[i, j] = dm_stat\n",
        "                p_values[i, j] = p_value\n",
        "\n",
        "    # Create DataFrames\n",
        "    dm_df = pd.DataFrame(dm_stats, index=model_names, columns=model_names)\n",
        "    p_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
        "\n",
        "    return dm_df, p_df\n",
        "\n",
        "\n",
        "def format_pvalue_table(p_df):\n",
        "    \"\"\"Format p-values with significance indicators\"\"\"\n",
        "    formatted = p_df.copy().astype(object)\n",
        "\n",
        "    for i in range(len(p_df)):\n",
        "        for j in range(len(p_df.columns)):\n",
        "            if i == j:\n",
        "                formatted.iloc[i, j] = \"-\"\n",
        "            elif i > j:  # Lower triangle - leave empty\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "            elif not np.isnan(p_df.iloc[i, j]):\n",
        "                p_val = p_df.iloc[i, j]\n",
        "\n",
        "                if p_val < 0.001:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}***\"\n",
        "                elif p_val < 0.01:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}**\"\n",
        "                elif p_val < 0.05:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}*\"\n",
        "                else:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}\"\n",
        "            else:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def format_dm_table(dm_df, p_df):\n",
        "    \"\"\"Format DM statistics table with significance indicators based on p-values\"\"\"\n",
        "    formatted = dm_df.copy().astype(object)\n",
        "\n",
        "    for i in range(len(dm_df)):\n",
        "        for j in range(len(dm_df.columns)):\n",
        "            if i == j:\n",
        "                formatted.iloc[i, j] = \"-\"\n",
        "            elif i > j:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "            elif not (np.isnan(dm_df.iloc[i, j]) or np.isnan(p_df.iloc[i, j])):\n",
        "                dm_val = dm_df.iloc[i, j]\n",
        "                p_val = p_df.iloc[i, j]\n",
        "\n",
        "                # Add significance stars based on p-values\n",
        "                if p_val < 0.001:\n",
        "                    stars = \"***\"\n",
        "                elif p_val < 0.01:\n",
        "                    stars = \"**\"\n",
        "                elif p_val < 0.05:\n",
        "                    stars = \"*\"\n",
        "                else:\n",
        "                    stars = \"\"\n",
        "\n",
        "                formatted.iloc[i, j] = f\"{dm_val:.4f}{stars}\"\n",
        "            else:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def create_dm_heatmap(p_matrix, title=\"Diebold-Mariano Test P-Values\"):\n",
        "    \"\"\"\n",
        "    Create heatmap with black diagonal X marks and pure black for p >= 0.05\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Prepare data matrix\n",
        "    n_models = len(p_matrix)\n",
        "    heatmap_data = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    # Fill data: 0.051 for diagonal and p>=0.05, actual p-values for p<0.05\n",
        "    for i in range(n_models):\n",
        "        for j in range(n_models):\n",
        "            if i == j:\n",
        "                heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "            else:\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "                if np.isnan(p_val) or p_val >= 0.05:\n",
        "                    heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "                else:\n",
        "                    heatmap_data[i, j] = p_val  # Keep actual p-value for coloring\n",
        "\n",
        "    # Create discrete colormap with explicit black at the end\n",
        "    colors = ['#003300', '#006600', '#009900', '#00CC00', '#33FF33', '#66FF66', '#99FF99',\n",
        "              '#CCFF99', '#FFFF66', '#FFCC33', '#FF9900', '#FF6600', '#FF3300', '#CC0000', '#000000']\n",
        "\n",
        "    # Create boundaries for discrete colors\n",
        "    boundaries = np.linspace(0, 0.05, len(colors)-1).tolist() + [0.051, 1.0]\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "    norm = mcolors.BoundaryNorm(boundaries, len(colors))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm, aspect='equal', interpolation='nearest')\n",
        "\n",
        "    # Add white X marks on diagonal\n",
        "    for i in range(n_models):\n",
        "        ax.text(i, i, 'X', ha='center', va='center',\n",
        "                color='white', fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Add white grid lines\n",
        "    for i in range(n_models + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=1.0)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=1.0)\n",
        "\n",
        "    # Set labels\n",
        "    model_names = p_matrix.columns.tolist()\n",
        "    ax.set_xticks(range(n_models))\n",
        "    ax.set_yticks(range(n_models))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11, color='black')\n",
        "    ax.set_yticklabels(model_names, fontsize=11, color='black')\n",
        "\n",
        "    # Add title\n",
        "    ax.set_title(title + '\\n(Window 5)', fontsize=18, fontweight='bold', color='black', pad=30)\n",
        "\n",
        "    # Create colorbar with discrete ticks including black\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.7, aspect=25, pad=0.02, boundaries=boundaries, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.051])\n",
        "    cbar.set_label('P-Value', fontsize=14, color='black', labelpad=15)\n",
        "    cbar.ax.tick_params(labelsize=12, colors='black')\n",
        "\n",
        "    # Set colorbar labels with black indicator\n",
        "    cbar_labels = ['0.00', '0.01', '0.02', '0.03', '0.04', '0.05', '≥0.05']\n",
        "    cbar.set_ticklabels(cbar_labels)\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove tick marks but keep labels\n",
        "    ax.tick_params(length=0, colors='black')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(bottom=0.08, top=0.92, left=0.1, right=0.88)\n",
        "\n",
        "    plt.savefig('dm_test_heatmap_fixed.png', dpi=300, bbox_inches='tight',\n",
        "                facecolor='white', edgecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def run_dm_test_analysis():\n",
        "    \"\"\"\n",
        "    Load data and run DM tests for all model pairs\n",
        "    \"\"\"\n",
        "\n",
        "    # Input folder: DM_Test_Data\n",
        "    DATA_DIR = os.path.join(os.getcwd(), \"DM_Test_Data\")\n",
        "\n",
        "    # Model files\n",
        "    model_files = {\n",
        "        \"OLS\": \"ols_results_lag5_full.csv\",\n",
        "        \"LASSO\": \"lasso_results_lag5_full.csv\",\n",
        "        \"Ridge\": \"ridge_results_lag5_full.csv\",\n",
        "        \"ElasticNet\": \"enet_results_lag5_full.csv\",\n",
        "        \"PCR\": \"pcr_results_lag5_full.csv\",\n",
        "        \"GLM\": \"glm_results_lag5_full.csv\",\n",
        "        \"RF\": \"rf_results_lag5_full.csv\",\n",
        "        \"GBRT\": \"gbrt_results_lag5_full.csv\",\n",
        "        \"NN1\": \"nn1_results_lag5_full.csv\",\n",
        "        \"NN2\": \"nn2_results_lag5_full.csv\",\n",
        "        \"NN3\": \"nn3_results_lag5_full.csv\",\n",
        "        \"NN4\": \"nn4_results_lag5_full.csv\",\n",
        "        \"NN5\": \"nn5_results_lag5_full.csv\",\n",
        "        \"Chronos-T5 Tiny\": \"chronost5tiny_results_lag5_full.csv\",\n",
        "        \"Chronos-Bolt Tiny\": \"chronosbolttiny_results_lag5_full.csv\",\n",
        "        \"Chronos-Bolt Mini\": \"chronosboltmini_results_lag5_full.csv\",\n",
        "        \"Chronos-Bolt Small\": \"chronosboltsmall_results_lag5_full.csv\",\n",
        "        \"TimesFM 1.0-200M\": \"timesfm1_results_lag5_full.csv\",\n",
        "        \"TimesFM 2.0-500M\": \"timesfm2_results_lag5_full.csv\",\n",
        "        \"Moirai Small\": \"uni2tssmall_results_lag5_full.csv\",\n",
        "        \"Moirai-Moe Small\": \"uni2tssmallmoe_results_lag5_full.csv\",\n",
        "        \"Moirai-Moe Base\": \"uni2tsbasemoe_results_lag5_full.csv\"\n",
        "    }\n",
        "\n",
        "    print(\"DIEBOLD-MARIANO TEST\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Loss Function: Mean Squared Error (MSE)\")\n",
        "    print(\"H₀: E[dₜ] = 0 (equal prediction accuracy)\")\n",
        "    print(\"H₁: E[dₜ] ≠ 0 (unequal prediction accuracy)\")\n",
        "    print(\"Variance Estimator: Newey-West with automatic bandwidth\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load and align data\n",
        "    dfs = []\n",
        "    model_names = []\n",
        "\n",
        "    for model_name, file_name in model_files.items():\n",
        "        file_path = os.path.join(DATA_DIR, file_name)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Standardize column names\n",
        "            if 'excess_ret' in df.columns:\n",
        "                df = df.rename(columns={'excess_ret': 'actual'})\n",
        "            elif 'y_true' in df.columns:\n",
        "                df = df.rename(columns={'y_true': 'actual'})\n",
        "\n",
        "            if 'y_pred' in df.columns:\n",
        "                df = df.rename(columns={'y_pred': 'predicted'})\n",
        "            elif 'predicted_excess_returns_lag5' in df.columns:\n",
        "                df = df.rename(columns={'predicted_excess_returns_lag5': 'predicted'})\n",
        "\n",
        "            dfs.append(df[['actual', 'predicted']])\n",
        "            model_names.append(model_name)\n",
        "            print(f\"Loaded: {model_name}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: {file_path} not found, skipping {model_name}\")\n",
        "\n",
        "    # Align to common length\n",
        "    min_length = min(len(df) for df in dfs)\n",
        "    print(f\"\\nSample size after alignment: {min_length}\")\n",
        "\n",
        "    actual_values = dfs[0]['actual'].iloc[:min_length].values\n",
        "    predictions = {}\n",
        "\n",
        "    for df, name in zip(dfs, model_names):\n",
        "        predictions[name] = df['predicted'].iloc[:min_length].values\n",
        "\n",
        "    print(f\"Models included: {len(model_names)}\")\n",
        "\n",
        "    # Run DM tests\n",
        "    dm_matrix, p_matrix = run_pairwise_dm_tests(actual_values, predictions)\n",
        "\n",
        "    # Format results\n",
        "    formatted_dm = format_dm_table(dm_matrix, p_matrix)\n",
        "    formatted_p = format_pvalue_table(p_matrix)\n",
        "\n",
        "    print(\"\\nDM TEST STATISTICS:\")\n",
        "    print(\"Interpretation:\")\n",
        "    print(\"- DM > 0: Row model has HIGHER MSE (worse) than Column model\")\n",
        "    print(\"- DM < 0: Row model has LOWER MSE (better) than Column model\")\n",
        "    print(\"- Significance: *** p<0.001, ** p<0.01, * p<0.05\")\n",
        "    print(\"- p > 0.05: No significant difference (accept H₀)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(formatted_dm.to_string())\n",
        "\n",
        "    print(\"\\n\\nP-VALUES:\")\n",
        "    print(\"- p < 0.05: Reject H₀ (significant difference in accuracy)\")\n",
        "    print(\"- p ≥ 0.05: Accept H₀ (no significant difference)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(formatted_p.to_string())\n",
        "\n",
        "    # Save the DM test statistics results to a CSV file\n",
        "    formatted_dm.to_csv(\"dm_statistics_formatted.csv\", index=True)\n",
        "    p_matrix.to_csv(\"dm_pvalues_formatted.csv\", index=True)\n",
        "\n",
        "    # Create summary of best performing models\n",
        "    mse_performance = {}\n",
        "    for name, preds in predictions.items():\n",
        "        mse = np.mean((actual_values - preds)**2)\n",
        "        mse_performance[name] = mse\n",
        "\n",
        "    # Summary of significant outperformances\n",
        "    print(\"\\n\\nSUMMARY OF SIGNIFICANT OUTPERFORMANCES:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    significant_better = {}\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        significant_better[model_a] = []\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:\n",
        "                dm_stat = dm_matrix.iloc[i, j]\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "\n",
        "                # Model A significantly better than Model B (DM < 0 and p < 0.05)\n",
        "                if not np.isnan(p_val) and p_val < 0.05 and dm_stat < 0:\n",
        "                    significant_better[model_a].append(model_b)\n",
        "\n",
        "    for model, beaten_models in significant_better.items():\n",
        "        if beaten_models:\n",
        "            print(f\"{model} significantly outperforms: {', '.join(beaten_models[:5])}\")\n",
        "            if len(beaten_models) > 5:\n",
        "                print(f\"   ... and {len(beaten_models)-5} more models\")\n",
        "\n",
        "    # Create and display heatmap\n",
        "    print(\"\\nGenerating DM test heatmap...\")\n",
        "    create_dm_heatmap(p_matrix)\n",
        "\n",
        "    print(f\"\\nResults saved:\")\n",
        "    print(\"- dm_statistics5.csv\")\n",
        "    print(\"- dm_pvalues.csv\")\n",
        "    print(\"- dm_statistics_formatted5.csv\")\n",
        "    print(\"- dm_pvalues_formatted.csv\")\n",
        "    print(\"- dm_test_heatmap_fixed.png\")\n",
        "\n",
        "    return dm_matrix, p_matrix\n",
        "\n",
        "def run_pairwise_dm_tests(actual, predictions_dict):\n",
        "    \"\"\"\n",
        "    Run pairwise DM tests for all model combinations\n",
        "    \"\"\"\n",
        "    model_names = list(predictions_dict.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Initialize matrices\n",
        "    dm_stats = np.full((n_models, n_models), np.nan)\n",
        "    p_values = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    print(f\"Running pairwise DM tests...\")\n",
        "\n",
        "    # Pairwise comparisons\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:  # Skip diagonal\n",
        "                pred1 = predictions_dict[model_a]\n",
        "                pred2 = predictions_dict[model_b]\n",
        "\n",
        "                dm_stat, p_value = diebold_mariano_test(actual, pred1, pred2)\n",
        "\n",
        "                dm_stats[i, j] = dm_stat\n",
        "                p_values[i, j] = p_value\n",
        "\n",
        "    # Create DataFrames\n",
        "    dm_df = pd.DataFrame(dm_stats, index=model_names, columns=model_names)\n",
        "    p_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
        "\n",
        "    return dm_df, p_df\n",
        "\n",
        "def create_dm_heatmap(p_matrix, title=\"Diebold-Mariano Test P-Values\"):\n",
        "    \"\"\"\n",
        "    Create heatmap with black diagonal X marks and pure black for p >= 0.05\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Prepare data matrix\n",
        "    n_models = len(p_matrix)\n",
        "    heatmap_data = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    # Fill data: 0.051 for diagonal and p>=0.05, actual p-values for p<0.05\n",
        "    for i in range(n_models):\n",
        "        for j in range(n_models):\n",
        "            if i == j:\n",
        "                heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "            else:\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "                if np.isnan(p_val) or p_val >= 0.05:\n",
        "                    heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "                else:\n",
        "                    heatmap_data[i, j] = p_val  # Keep actual p-value for coloring\n",
        "\n",
        "    # Create discrete colormap with explicit black at the end\n",
        "    colors = ['#006400', '#007400', '#009900', '#00CC00', '#33FF33', '#66FF66', '#99FF99',\n",
        "              '#CCFF99', '#FFFF66', '#FFCC33', '#FF9900', '#FF6600', '#FF3300', '#CC0000', '#000000']\n",
        "\n",
        "    # Create boundaries for discrete colors\n",
        "    boundaries = np.linspace(0, 0.05, len(colors)-1).tolist() + [0.051, 1.0]\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "    norm = mcolors.BoundaryNorm(boundaries, len(colors))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm, aspect='equal', interpolation='nearest')\n",
        "\n",
        "    # Add white X marks on diagonal\n",
        "    for i in range(n_models):\n",
        "        ax.text(i, i, 'X', ha='center', va='center',\n",
        "                color='white', fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Add white grid lines\n",
        "    for i in range(n_models + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=1.0)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=1.0)\n",
        "\n",
        "    # Set labels\n",
        "    model_names = p_matrix.columns.tolist()\n",
        "    ax.set_xticks(range(n_models))\n",
        "    ax.set_yticks(range(n_models))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11, color='black')\n",
        "    ax.set_yticklabels(model_names, fontsize=11, color='black')\n",
        "\n",
        "    # Add title\n",
        "    ax.set_title(title + '\\n(Window 5)', fontsize=18, fontweight='bold', color='black', pad=30)\n",
        "\n",
        "    # Create colorbar with discrete ticks including black\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.7, aspect=25, pad=0.02, boundaries=boundaries, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.051])\n",
        "    cbar.set_label('P-Value', fontsize=14, color='black', labelpad=15)\n",
        "    cbar.ax.tick_params(labelsize=12, colors='black')\n",
        "\n",
        "    # Set colorbar labels with black indicator\n",
        "    cbar_labels = ['0.00', '0.01', '0.02', '0.03', '0.04', '0.05', '≥0.05']\n",
        "    cbar.set_ticklabels(cbar_labels)\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove tick marks but keep labels\n",
        "    ax.tick_params(length=0, colors='black')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(bottom=0.08, top=0.92, left=0.1, right=0.88)\n",
        "\n",
        "    plt.savefig('dm_test_heatmap_fixed.png', dpi=300, bbox_inches='tight',\n",
        "                facecolor='white', edgecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    dm_df, p_df = run_dm_test_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnQoZyUOr-Bz"
      },
      "source": [
        "# Window Size 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N-6JbrhYoArG"
      },
      "outputs": [],
      "source": [
        "def diebold_mariano_test(actual, pred1, pred2, h=1):\n",
        "    \"\"\"\n",
        "    Diebold-Mariano test using Mean Squared Error loss function with Newey-West\n",
        "\n",
        "    H0: E[dt] = 0 (equal prediction accuracy)\n",
        "    H1: E[dt] ≠ 0 (unequal prediction accuracy)\n",
        "\n",
        "    Test statistic: DM = d̄ / σ̂_d̄ ~ N(0,1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Align arrays\n",
        "    min_len = min(len(actual), len(pred1), len(pred2))\n",
        "    actual = np.array(actual)[:min_len]\n",
        "    pred1 = np.array(pred1)[:min_len]\n",
        "    pred2 = np.array(pred2)[:min_len]\n",
        "\n",
        "    # Remove missing values\n",
        "    mask = ~(np.isnan(actual) | np.isnan(pred1) | np.isnan(pred2))\n",
        "    actual = actual[mask]\n",
        "    pred1 = pred1[mask]\n",
        "    pred2 = pred2[mask]\n",
        "\n",
        "    if len(actual) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Calculate squared forecast errors\n",
        "    e1_squared = (actual - pred1)**2\n",
        "    e2_squared = (actual - pred2)**2\n",
        "\n",
        "    # Loss differential: dt = e²1,t - e²2,t\n",
        "    d = e1_squared - e2_squared\n",
        "    d_mean = np.mean(d)\n",
        "    n = len(d)\n",
        "\n",
        "    # Newey-West variance estimator\n",
        "    def newey_west_variance(d, h):\n",
        "        n = len(d)\n",
        "        d_centered = d - np.mean(d)\n",
        "\n",
        "        # Bandwidth selection\n",
        "        bandwidth = max(1, int(4 * (n/100)**(2/9)))\n",
        "\n",
        "        # Calculate autocovariances with Bartlett kernel\n",
        "        gamma_0 = np.mean(d_centered**2)\n",
        "        gamma_sum = 0\n",
        "\n",
        "        for k in range(1, min(bandwidth + 1, n)):\n",
        "            if k < n:\n",
        "                weight = 1 - k / (bandwidth + 1)\n",
        "                gamma_k = np.mean(d_centered[k:] * d_centered[:-k])\n",
        "                gamma_sum += 2 * weight * gamma_k\n",
        "\n",
        "        return gamma_0 + gamma_sum\n",
        "\n",
        "    # Calculate long-run variance\n",
        "    long_run_var = newey_west_variance(d, h)\n",
        "\n",
        "    if long_run_var <= 0:\n",
        "        long_run_var = np.var(d, ddof=1)\n",
        "        if long_run_var <= 0:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "    # DM statistic\n",
        "    dm_stat = d_mean / np.sqrt(long_run_var / n)\n",
        "\n",
        "    if np.isnan(dm_stat) or np.isinf(dm_stat):\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Two-tailed p-value\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
        "\n",
        "    return dm_stat, p_value\n",
        "\n",
        "\n",
        "def run_pairwise_dm_tests(actual, predictions_dict):\n",
        "    \"\"\"\n",
        "    Run pairwise DM tests for all model combinations\n",
        "    \"\"\"\n",
        "    model_names = list(predictions_dict.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Initialize matrices\n",
        "    dm_stats = np.full((n_models, n_models), np.nan)\n",
        "    p_values = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    print(f\"Running pairwise DM tests...\")\n",
        "\n",
        "    # Pairwise comparisons\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:  # Skip diagonal\n",
        "                pred1 = predictions_dict[model_a]\n",
        "                pred2 = predictions_dict[model_b]\n",
        "\n",
        "                dm_stat, p_value = diebold_mariano_test(actual, pred1, pred2)\n",
        "\n",
        "                dm_stats[i, j] = dm_stat\n",
        "                p_values[i, j] = p_value\n",
        "\n",
        "    # Create DataFrames\n",
        "    dm_df = pd.DataFrame(dm_stats, index=model_names, columns=model_names)\n",
        "    p_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
        "\n",
        "    return dm_df, p_df\n",
        "\n",
        "\n",
        "def format_pvalue_table(p_df):\n",
        "    \"\"\"Format p-values with significance indicators\"\"\"\n",
        "    formatted = p_df.copy().astype(object)\n",
        "\n",
        "    for i in range(len(p_df)):\n",
        "        for j in range(len(p_df.columns)):\n",
        "            if i == j:\n",
        "                formatted.iloc[i, j] = \"-\"\n",
        "            elif i > j:  # Lower triangle - leave empty\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "            elif not np.isnan(p_df.iloc[i, j]):\n",
        "                p_val = p_df.iloc[i, j]\n",
        "\n",
        "                if p_val < 0.001:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}***\"\n",
        "                elif p_val < 0.01:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}**\"\n",
        "                elif p_val < 0.05:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}*\"\n",
        "                else:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}\"\n",
        "            else:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def format_dm_table(dm_df, p_df):\n",
        "    \"\"\"Format DM statistics table with significance indicators based on p-values\"\"\"\n",
        "    formatted = dm_df.copy().astype(object)\n",
        "\n",
        "    for i in range(len(dm_df)):\n",
        "        for j in range(len(dm_df.columns)):\n",
        "            if i == j:\n",
        "                formatted.iloc[i, j] = \"-\"\n",
        "            elif i > j:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "            elif not (np.isnan(dm_df.iloc[i, j]) or np.isnan(p_df.iloc[i, j])):\n",
        "                dm_val = dm_df.iloc[i, j]\n",
        "                p_val = p_df.iloc[i, j]\n",
        "\n",
        "                # Add significance stars based on p-values\n",
        "                if p_val < 0.001:\n",
        "                    stars = \"***\"\n",
        "                elif p_val < 0.01:\n",
        "                    stars = \"**\"\n",
        "                elif p_val < 0.05:\n",
        "                    stars = \"*\"\n",
        "                else:\n",
        "                    stars = \"\"\n",
        "\n",
        "                formatted.iloc[i, j] = f\"{dm_val:.4f}{stars}\"\n",
        "            else:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def create_dm_heatmap(p_matrix, title=\"Diebold-Mariano Test P-Values\"):\n",
        "    \"\"\"\n",
        "    Create heatmap with black diagonal X marks and pure black for p >= 0.05\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Prepare data matrix\n",
        "    n_models = len(p_matrix)\n",
        "    heatmap_data = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    # Fill data: 0.051 for diagonal and p>=0.05, actual p-values for p<0.05\n",
        "    for i in range(n_models):\n",
        "        for j in range(n_models):\n",
        "            if i == j:\n",
        "                heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "            else:\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "                if np.isnan(p_val) or p_val >= 0.05:\n",
        "                    heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "                else:\n",
        "                    heatmap_data[i, j] = p_val  # Keep actual p-value for coloring\n",
        "\n",
        "    # Create discrete colormap with explicit black at the end\n",
        "    colors = ['#003300', '#006600', '#009900', '#00CC00', '#33FF33', '#66FF66', '#99FF99',\n",
        "              '#CCFF99', '#FFFF66', '#FFCC33', '#FF9900', '#FF6600', '#FF3300', '#CC0000', '#000000']\n",
        "\n",
        "    # Create boundaries for discrete colors\n",
        "    boundaries = np.linspace(0, 0.05, len(colors)-1).tolist() + [0.051, 1.0]\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "    norm = mcolors.BoundaryNorm(boundaries, len(colors))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm, aspect='equal', interpolation='nearest')\n",
        "\n",
        "    # Add white X marks on diagonal\n",
        "    for i in range(n_models):\n",
        "        ax.text(i, i, 'X', ha='center', va='center',\n",
        "                color='white', fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Add white grid lines\n",
        "    for i in range(n_models + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=1.0)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=1.0)\n",
        "\n",
        "    # Set labels\n",
        "    model_names = p_matrix.columns.tolist()\n",
        "    ax.set_xticks(range(n_models))\n",
        "    ax.set_yticks(range(n_models))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11, color='black')\n",
        "    ax.set_yticklabels(model_names, fontsize=11, color='black')\n",
        "\n",
        "    # Add title\n",
        "    ax.set_title(title + '\\n(Window 21)', fontsize=18, fontweight='bold', color='black', pad=30)\n",
        "\n",
        "    # Create colorbar with discrete ticks including black\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.7, aspect=25, pad=0.02, boundaries=boundaries, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.051])\n",
        "    cbar.set_label('P-Value', fontsize=14, color='black', labelpad=15)\n",
        "    cbar.ax.tick_params(labelsize=12, colors='black')\n",
        "\n",
        "    # Set colorbar labels with black indicator\n",
        "    cbar_labels = ['0.00', '0.01', '0.02', '0.03', '0.04', '0.05', '≥0.05']\n",
        "    cbar.set_ticklabels(cbar_labels)\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove tick marks but keep labels\n",
        "    ax.tick_params(length=0, colors='black')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(bottom=0.08, top=0.92, left=0.1, right=0.88)\n",
        "\n",
        "    plt.savefig('dm_test_heatmap_fixed.png', dpi=300, bbox_inches='tight',\n",
        "                facecolor='white', edgecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def run_dm_test_analysis():\n",
        "    \"\"\"\n",
        "    Load data and run DM tests for all model pairs\n",
        "    \"\"\"\n",
        "\n",
        "    # Input folder: DM_Test_Data\n",
        "    DATA_DIR = os.path.join(os.getcwd(), \"DM_Test_Data\")\n",
        "\n",
        "    # Model files\n",
        "    model_files = {\n",
        "        \"OLS\": \"ols_results_lag21_full.csv\",\n",
        "        \"LASSO\": \"lasso_results_lag21_full.csv\",\n",
        "        \"Ridge\": \"ridge_results_lag21_full.csv\",\n",
        "        \"ElasticNet\": \"enet_results_lag21_full.csv\",\n",
        "        \"PCR\": \"pcr_results_lag21_full.csv\",\n",
        "        \"GLM\": \"glm_results_lag21_full.csv\",\n",
        "        \"RF\": \"rf_results_lag21_full.csv\",\n",
        "        \"GBRT\": \"gbrt_results_lag21_full.csv\",\n",
        "        \"NN1\": \"nn1_results_lag21_full.csv\",\n",
        "        \"NN2\": \"nn2_results_lag21_full.csv\",\n",
        "        \"NN3\": \"nn3_results_lag21_full.csv\",\n",
        "        \"NN4\": \"nn4_results_lag21_full.csv\",\n",
        "        \"NN5\": \"nn5_results_lag21_full.csv\",\n",
        "        \"Chronos-T5 Tiny\": \"chronost5tiny_results_lag21_full.csv\",\n",
        "        \"Chronos-Bolt Tiny\": \"chronosbolttiny_results_lag21_full.csv\",\n",
        "        \"Chronos-Bolt Mini\": \"chronosboltmini_results_lag21_full.csv\",\n",
        "        \"Chronos-Bolt Small\": \"chronosboltsmall_results_lag21_full.csv\",\n",
        "        \"TimesFM 1.0-200M\": \"timesfm1_results_lag21_full.csv\",\n",
        "        \"TimesFM 2.0-500M\": \"timesfm2_results_lag21_full.csv\",\n",
        "        \"Moirai Small\": \"uni2tssmall_results_lag21_full.csv\",\n",
        "        \"Moirai-Moe Small\": \"uni2tssmallmoe_results_lag21_full.csv\",\n",
        "        \"Moirai-Moe Base\": \"uni2tsbasemoe_results_lag21_full.csv\"\n",
        "    }\n",
        "\n",
        "    print(\"DIEBOLD-MARIANO TEST\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Loss Function: Mean Squared Error (MSE)\")\n",
        "    print(\"H₀: E[dₜ] = 0 (equal prediction accuracy)\")\n",
        "    print(\"H₁: E[dₜ] ≠ 0 (unequal prediction accuracy)\")\n",
        "    print(\"Variance Estimator: Newey-West with automatic bandwidth\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load and align data\n",
        "    dfs = []\n",
        "    model_names = []\n",
        "\n",
        "    for model_name, file_name in model_files.items():\n",
        "        file_path = os.path.join(DATA_DIR, file_name)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Standardize column names\n",
        "            if 'excess_ret' in df.columns:\n",
        "                df = df.rename(columns={'excess_ret': 'actual'})\n",
        "            elif 'y_true' in df.columns:\n",
        "                df = df.rename(columns={'y_true': 'actual'})\n",
        "\n",
        "            if 'y_pred' in df.columns:\n",
        "                df = df.rename(columns={'y_pred': 'predicted'})\n",
        "            elif 'predicted_excess_returns_lag21' in df.columns:\n",
        "                df = df.rename(columns={'predicted_excess_returns_lag21': 'predicted'})\n",
        "\n",
        "            dfs.append(df[['actual', 'predicted']])\n",
        "            model_names.append(model_name)\n",
        "            print(f\"Loaded: {model_name}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: {file_path} not found, skipping {model_name}\")\n",
        "\n",
        "    # Align to common length\n",
        "    min_length = min(len(df) for df in dfs)\n",
        "    print(f\"\\nSample size after alignment: {min_length}\")\n",
        "\n",
        "    actual_values = dfs[0]['actual'].iloc[:min_length].values\n",
        "    predictions = {}\n",
        "\n",
        "    for df, name in zip(dfs, model_names):\n",
        "        predictions[name] = df['predicted'].iloc[:min_length].values\n",
        "\n",
        "    print(f\"Models included: {len(model_names)}\")\n",
        "\n",
        "    # Run DM tests\n",
        "    dm_matrix, p_matrix = run_pairwise_dm_tests(actual_values, predictions)\n",
        "\n",
        "    # Format results\n",
        "    formatted_dm = format_dm_table(dm_matrix, p_matrix)\n",
        "    formatted_p = format_pvalue_table(p_matrix)\n",
        "\n",
        "    print(\"\\nDM TEST STATISTICS:\")\n",
        "    print(\"Interpretation:\")\n",
        "    print(\"- DM > 0: Row model has HIGHER MSE (worse) than Column model\")\n",
        "    print(\"- DM < 0: Row model has LOWER MSE (better) than Column model\")\n",
        "    print(\"- Significance: *** p<0.001, ** p<0.01, * p<0.05\")\n",
        "    print(\"- p > 0.05: No significant difference (accept H₀)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(formatted_dm.to_string())\n",
        "\n",
        "    print(\"\\n\\nP-VALUES:\")\n",
        "    print(\"- p < 0.05: Reject H₀ (significant difference in accuracy)\")\n",
        "    print(\"- p ≥ 0.05: Accept H₀ (no significant difference)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(formatted_p.to_string())\n",
        "\n",
        "    # Save the DM test statistics results to a CSV file\n",
        "    formatted_dm.to_csv(\"dm_statistics_formatted.csv\", index=True)\n",
        "    p_matrix.to_csv(\"dm_pvalues_formatted.csv\", index=True)\n",
        "\n",
        "    # Create summary of best performing models\n",
        "    mse_performance = {}\n",
        "    for name, preds in predictions.items():\n",
        "        mse = np.mean((actual_values - preds)**2)\n",
        "        mse_performance[name] = mse\n",
        "\n",
        "    # Summary of significant outperformances\n",
        "    print(\"\\n\\nSUMMARY OF SIGNIFICANT OUTPERFORMANCES:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    significant_better = {}\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        significant_better[model_a] = []\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:\n",
        "                dm_stat = dm_matrix.iloc[i, j]\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "\n",
        "                # Model A significantly better than Model B (DM < 0 and p < 0.05)\n",
        "                if not np.isnan(p_val) and p_val < 0.05 and dm_stat < 0:\n",
        "                    significant_better[model_a].append(model_b)\n",
        "\n",
        "    for model, beaten_models in significant_better.items():\n",
        "        if beaten_models:\n",
        "            print(f\"{model} significantly outperforms: {', '.join(beaten_models[:5])}\")\n",
        "            if len(beaten_models) > 5:\n",
        "                print(f\"   ... and {len(beaten_models)-5} more models\")\n",
        "\n",
        "    # Create and display heatmap\n",
        "    print(\"\\nGenerating DM test heatmap...\")\n",
        "    create_dm_heatmap(p_matrix)\n",
        "\n",
        "    print(f\"\\nResults saved:\")\n",
        "    print(\"- dm_statistics21.csv\")\n",
        "    print(\"- dm_pvalues.csv\")\n",
        "    print(\"- dm_statistics_formatted21.csv\")\n",
        "    print(\"- dm_pvalues_formatted.csv\")\n",
        "    print(\"- dm_test_heatmap_fixed.png\")\n",
        "\n",
        "    return dm_matrix, p_matrix\n",
        "\n",
        "def run_pairwise_dm_tests(actual, predictions_dict):\n",
        "    \"\"\"\n",
        "    Run pairwise DM tests for all model combinations\n",
        "    \"\"\"\n",
        "    model_names = list(predictions_dict.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Initialize matrices\n",
        "    dm_stats = np.full((n_models, n_models), np.nan)\n",
        "    p_values = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    print(f\"Running pairwise DM tests...\")\n",
        "\n",
        "    # Pairwise comparisons\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:  # Skip diagonal\n",
        "                pred1 = predictions_dict[model_a]\n",
        "                pred2 = predictions_dict[model_b]\n",
        "\n",
        "                dm_stat, p_value = diebold_mariano_test(actual, pred1, pred2)\n",
        "\n",
        "                dm_stats[i, j] = dm_stat\n",
        "                p_values[i, j] = p_value\n",
        "\n",
        "    # Create DataFrames\n",
        "    dm_df = pd.DataFrame(dm_stats, index=model_names, columns=model_names)\n",
        "    p_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
        "\n",
        "    return dm_df, p_df\n",
        "\n",
        "def create_dm_heatmap(p_matrix, title=\"Diebold-Mariano Test P-Values\"):\n",
        "    \"\"\"\n",
        "    Create heatmap with black diagonal X marks and pure black for p >= 0.05\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Prepare data matrix\n",
        "    n_models = len(p_matrix)\n",
        "    heatmap_data = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    # Fill data: 0.051 for diagonal and p>=0.05, actual p-values for p<0.05\n",
        "    for i in range(n_models):\n",
        "        for j in range(n_models):\n",
        "            if i == j:\n",
        "                heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "            else:\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "                if np.isnan(p_val) or p_val >= 0.05:\n",
        "                    heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "                else:\n",
        "                    heatmap_data[i, j] = p_val\n",
        "\n",
        "    # Create discrete colormap with explicit black at the end\n",
        "    colors = ['#006400', '#007400', '#009900', '#00CC00', '#33FF33', '#66FF66', '#99FF99',\n",
        "              '#CCFF99', '#FFFF66', '#FFCC33', '#FF9900', '#FF6600', '#FF3300', '#CC0000', '#000000']\n",
        "\n",
        "    # Create boundaries for discrete colors\n",
        "    boundaries = np.linspace(0, 0.05, len(colors)-1).tolist() + [0.051, 1.0]\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "    norm = mcolors.BoundaryNorm(boundaries, len(colors))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm, aspect='equal', interpolation='nearest')\n",
        "\n",
        "    # Add white X marks on diagonal\n",
        "    for i in range(n_models):\n",
        "        ax.text(i, i, 'X', ha='center', va='center',\n",
        "                color='white', fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Add white grid lines\n",
        "    for i in range(n_models + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=1.0)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=1.0)\n",
        "\n",
        "    # Set labels\n",
        "    model_names = p_matrix.columns.tolist()\n",
        "    ax.set_xticks(range(n_models))\n",
        "    ax.set_yticks(range(n_models))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11, color='black')\n",
        "    ax.set_yticklabels(model_names, fontsize=11, color='black')\n",
        "\n",
        "    # Add title\n",
        "    ax.set_title(title + '\\n(Window 21)', fontsize=18, fontweight='bold', color='black', pad=30)\n",
        "\n",
        "    # Create colorbar with discrete ticks including black\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.7, aspect=25, pad=0.02, boundaries=boundaries, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.051])\n",
        "    cbar.set_label('P-Value', fontsize=14, color='black', labelpad=15)\n",
        "    cbar.ax.tick_params(labelsize=12, colors='black')\n",
        "\n",
        "    # Set colorbar labels with black indicator\n",
        "    cbar_labels = ['0.00', '0.01', '0.02', '0.03', '0.04', '0.05', '≥0.05']\n",
        "    cbar.set_ticklabels(cbar_labels)\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove tick marks but keep labels\n",
        "    ax.tick_params(length=0, colors='black')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(bottom=0.08, top=0.92, left=0.1, right=0.88)\n",
        "\n",
        "    plt.savefig('dm_test_heatmap_fixed.png', dpi=300, bbox_inches='tight',\n",
        "                facecolor='white', edgecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    dm_df, p_df = run_dm_test_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNbi4FHD_H_Q"
      },
      "source": [
        "# Window Size 252"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W-fqJ345_JC-"
      },
      "outputs": [],
      "source": [
        "def diebold_mariano_test(actual, pred1, pred2, h=1):\n",
        "    \"\"\"\n",
        "    Diebold-Mariano test using Mean Squared Error loss function with Newey-West\n",
        "\n",
        "    H0: E[dt] = 0 (equal prediction accuracy)\n",
        "    H1: E[dt] ≠ 0 (unequal prediction accuracy)\n",
        "\n",
        "    Test statistic: DM = d̄ / σ̂_d̄ ~ N(0,1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Align arrays\n",
        "    min_len = min(len(actual), len(pred1), len(pred2))\n",
        "    actual = np.array(actual)[:min_len]\n",
        "    pred1 = np.array(pred1)[:min_len]\n",
        "    pred2 = np.array(pred2)[:min_len]\n",
        "\n",
        "    # Remove missing values\n",
        "    mask = ~(np.isnan(actual) | np.isnan(pred1) | np.isnan(pred2))\n",
        "    actual = actual[mask]\n",
        "    pred1 = pred1[mask]\n",
        "    pred2 = pred2[mask]\n",
        "\n",
        "    if len(actual) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Calculate squared forecast errors\n",
        "    e1_squared = (actual - pred1)**2\n",
        "    e2_squared = (actual - pred2)**2\n",
        "\n",
        "    # Loss differential: dt = e²1,t - e²2,t\n",
        "    d = e1_squared - e2_squared\n",
        "    d_mean = np.mean(d)\n",
        "    n = len(d)\n",
        "\n",
        "    # Newey-West variance estimator\n",
        "    def newey_west_variance(d, h):\n",
        "        n = len(d)\n",
        "        d_centered = d - np.mean(d)\n",
        "\n",
        "        # Bandwidth selection\n",
        "        bandwidth = max(1, int(4 * (n/100)**(2/9)))\n",
        "\n",
        "        # Calculate autocovariances with Bartlett kernel\n",
        "        gamma_0 = np.mean(d_centered**2)\n",
        "        gamma_sum = 0\n",
        "\n",
        "        for k in range(1, min(bandwidth + 1, n)):\n",
        "            if k < n:\n",
        "                weight = 1 - k / (bandwidth + 1)\n",
        "                gamma_k = np.mean(d_centered[k:] * d_centered[:-k])\n",
        "                gamma_sum += 2 * weight * gamma_k\n",
        "\n",
        "        return gamma_0 + gamma_sum\n",
        "\n",
        "    # Calculate long-run variance\n",
        "    long_run_var = newey_west_variance(d, h)\n",
        "\n",
        "    if long_run_var <= 0:\n",
        "        long_run_var = np.var(d, ddof=1)\n",
        "        if long_run_var <= 0:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "    # DM statistic\n",
        "    dm_stat = d_mean / np.sqrt(long_run_var / n)\n",
        "\n",
        "    if np.isnan(dm_stat) or np.isinf(dm_stat):\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Two-tailed p-value\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
        "\n",
        "    return dm_stat, p_value\n",
        "\n",
        "\n",
        "def run_pairwise_dm_tests(actual, predictions_dict):\n",
        "    \"\"\"\n",
        "    Run pairwise DM tests for all model combinations\n",
        "    \"\"\"\n",
        "    model_names = list(predictions_dict.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Initialize matrices\n",
        "    dm_stats = np.full((n_models, n_models), np.nan)\n",
        "    p_values = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    print(f\"Running pairwise DM tests...\")\n",
        "\n",
        "    # Pairwise comparisons\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:  # Skip diagonal\n",
        "                pred1 = predictions_dict[model_a]\n",
        "                pred2 = predictions_dict[model_b]\n",
        "\n",
        "                dm_stat, p_value = diebold_mariano_test(actual, pred1, pred2)\n",
        "\n",
        "                dm_stats[i, j] = dm_stat\n",
        "                p_values[i, j] = p_value\n",
        "\n",
        "    # Create DataFrames\n",
        "    dm_df = pd.DataFrame(dm_stats, index=model_names, columns=model_names)\n",
        "    p_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
        "\n",
        "    return dm_df, p_df\n",
        "\n",
        "\n",
        "def format_pvalue_table(p_df):\n",
        "    \"\"\"Format p-values with significance indicators\"\"\"\n",
        "    formatted = p_df.copy().astype(object)\n",
        "\n",
        "    for i in range(len(p_df)):\n",
        "        for j in range(len(p_df.columns)):\n",
        "            if i == j:\n",
        "                formatted.iloc[i, j] = \"-\"\n",
        "            elif i > j:  # Lower triangle - leave empty\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "            elif not np.isnan(p_df.iloc[i, j]):\n",
        "                p_val = p_df.iloc[i, j]\n",
        "\n",
        "                if p_val < 0.001:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}***\"\n",
        "                elif p_val < 0.01:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}**\"\n",
        "                elif p_val < 0.05:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}*\"\n",
        "                else:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}\"\n",
        "            else:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def format_dm_table(dm_df, p_df):\n",
        "    \"\"\"Format DM statistics table with significance indicators based on p-values\"\"\"\n",
        "    formatted = dm_df.copy().astype(object)\n",
        "\n",
        "    for i in range(len(dm_df)):\n",
        "        for j in range(len(dm_df.columns)):\n",
        "            if i == j:\n",
        "                formatted.iloc[i, j] = \"-\"\n",
        "            elif i > j:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "            elif not (np.isnan(dm_df.iloc[i, j]) or np.isnan(p_df.iloc[i, j])):\n",
        "                dm_val = dm_df.iloc[i, j]\n",
        "                p_val = p_df.iloc[i, j]\n",
        "\n",
        "                # Add significance stars based on p-values\n",
        "                if p_val < 0.001:\n",
        "                    stars = \"***\"\n",
        "                elif p_val < 0.01:\n",
        "                    stars = \"**\"\n",
        "                elif p_val < 0.05:\n",
        "                    stars = \"*\"\n",
        "                else:\n",
        "                    stars = \"\"\n",
        "\n",
        "                formatted.iloc[i, j] = f\"{dm_val:.4f}{stars}\"\n",
        "            else:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def create_dm_heatmap(p_matrix, title=\"Diebold-Mariano Test P-Values\"):\n",
        "    \"\"\"\n",
        "    Create heatmap with black diagonal X marks and pure black for p >= 0.05\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Prepare data matrix\n",
        "    n_models = len(p_matrix)\n",
        "    heatmap_data = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    # Fill data: 0.051 for diagonal and p>=0.05, actual p-values for p<0.05\n",
        "    for i in range(n_models):\n",
        "        for j in range(n_models):\n",
        "            if i == j:\n",
        "                heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "            else:\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "                if np.isnan(p_val) or p_val >= 0.05:\n",
        "                    heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "                else:\n",
        "                    heatmap_data[i, j] = p_val  # Keep actual p-value for coloring\n",
        "\n",
        "    # Create discrete colormap with explicit black at the end\n",
        "    colors = ['#003300', '#006600', '#009900', '#00CC00', '#33FF33', '#66FF66', '#99FF99',\n",
        "              '#CCFF99', '#FFFF66', '#FFCC33', '#FF9900', '#FF6600', '#FF3300', '#CC0000', '#000000']\n",
        "\n",
        "    # Create boundaries for discrete colors\n",
        "    boundaries = np.linspace(0, 0.05, len(colors)-1).tolist() + [0.051, 1.0]\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "    norm = mcolors.BoundaryNorm(boundaries, len(colors))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm, aspect='equal', interpolation='nearest')\n",
        "\n",
        "    # Add white X marks on diagonal\n",
        "    for i in range(n_models):\n",
        "        ax.text(i, i, 'X', ha='center', va='center',\n",
        "                color='white', fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Add white grid lines\n",
        "    for i in range(n_models + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=1.0)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=1.0)\n",
        "\n",
        "    # Set labels\n",
        "    model_names = p_matrix.columns.tolist()\n",
        "    ax.set_xticks(range(n_models))\n",
        "    ax.set_yticks(range(n_models))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11, color='black')\n",
        "    ax.set_yticklabels(model_names, fontsize=11, color='black')\n",
        "\n",
        "    # Add title\n",
        "    ax.set_title(title + '\\n(Window 252)', fontsize=18, fontweight='bold', color='black', pad=30)\n",
        "\n",
        "    # Create colorbar with discrete ticks including black\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.7, aspect=25, pad=0.02, boundaries=boundaries, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.051])\n",
        "    cbar.set_label('P-Value', fontsize=14, color='black', labelpad=15)\n",
        "    cbar.ax.tick_params(labelsize=12, colors='black')\n",
        "\n",
        "    # Set colorbar labels with black indicator\n",
        "    cbar_labels = ['0.00', '0.01', '0.02', '0.03', '0.04', '0.05', '≥0.05']\n",
        "    cbar.set_ticklabels(cbar_labels)\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove tick marks but keep labels\n",
        "    ax.tick_params(length=0, colors='black')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(bottom=0.08, top=0.92, left=0.1, right=0.88)\n",
        "\n",
        "    plt.savefig('dm_test_heatmap_fixed.png', dpi=300, bbox_inches='tight',\n",
        "                facecolor='white', edgecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def run_dm_test_analysis():\n",
        "    \"\"\"\n",
        "    Load data and run DM tests for all model pairs\n",
        "    \"\"\"\n",
        "    \n",
        "    # Input folder: DM_Test_Data\n",
        "    DATA_DIR = os.path.join(os.getcwd(), \"DM_Test_Data\")\n",
        "\n",
        "    # Model files\n",
        "    model_files = {\n",
        "        \"OLS\": \"ols_results_lag252_full.csv\",\n",
        "        \"LASSO\": \"lasso_results_lag252_full.csv\",\n",
        "        \"Ridge\": \"ridge_results_lag252_full.csv\",\n",
        "        \"ElasticNet\": \"enet_results_lag252_full.csv\",\n",
        "        \"PCR\": \"pcr_results_lag252_full.csv\",\n",
        "        \"GLM\": \"glm_results_lag252_full.csv\",\n",
        "        \"RF\": \"rf_results_lag252_full.csv\",\n",
        "        \"GBRT\": \"gbrt_results_lag252_full.csv\",\n",
        "        \"NN1\": \"nn1_results_lag252_full.csv\",\n",
        "        \"NN2\": \"nn2_results_lag252_full.csv\",\n",
        "        \"NN3\": \"nn3_results_lag252_full.csv\",\n",
        "        \"NN4\": \"nn4_results_lag252_full.csv\",\n",
        "        \"NN5\": \"nn5_results_lag252_full.csv\",\n",
        "        \"Chronos-T5 Tiny\": \"chronost5tiny_results_lag252_full.csv\",\n",
        "        \"Chronos-Bolt Tiny\": \"chronosbolttiny_results_lag252_full.csv\",\n",
        "        \"Chronos-Bolt Mini\": \"chronosboltmini_results_lag252_full.csv\",\n",
        "        \"Chronos-Bolt Small\": \"chronosboltsmall_results_lag252_full.csv\",\n",
        "        \"TimesFM 1.0-200M\": \"timesfm1_results_lag252_full.csv\",\n",
        "        \"TimesFM 2.0-500M\": \"timesfm2_results_lag252_full.csv\",\n",
        "        \"Moirai Small\": \"uni2tssmall_results_lag252_full.csv\",\n",
        "        \"Moirai-Moe Small\": \"uni2tssmallmoe_results_lag252_full.csv\",\n",
        "        \"Moirai-Moe Base\": \"uni2tsbasemoe_results_lag252_full.csv\"\n",
        "    }\n",
        "\n",
        "    print(\"DIEBOLD-MARIANO TEST\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Loss Function: Mean Squared Error (MSE)\")\n",
        "    print(\"H₀: E[dₜ] = 0 (equal prediction accuracy)\")\n",
        "    print(\"H₁: E[dₜ] ≠ 0 (unequal prediction accuracy)\")\n",
        "    print(\"Variance Estimator: Newey-West with automatic bandwidth\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load and align data\n",
        "    dfs = []\n",
        "    model_names = []\n",
        "\n",
        "    for model_name, file_name in model_files.items():\n",
        "        file_path = os.path.join(DATA_DIR, file_name)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Standardize column names\n",
        "            if 'excess_ret' in df.columns:\n",
        "                df = df.rename(columns={'excess_ret': 'actual'})\n",
        "            elif 'y_true' in df.columns:\n",
        "                df = df.rename(columns={'y_true': 'actual'})\n",
        "\n",
        "            if 'y_pred' in df.columns:\n",
        "                df = df.rename(columns={'y_pred': 'predicted'})\n",
        "            elif 'predicted_excess_returns_lag252' in df.columns:\n",
        "                df = df.rename(columns={'predicted_excess_returns_lag252': 'predicted'})\n",
        "\n",
        "            dfs.append(df[['actual', 'predicted']])\n",
        "            model_names.append(model_name)\n",
        "            print(f\"Loaded: {model_name}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: {file_path} not found, skipping {model_name}\")\n",
        "\n",
        "    # Align to common length\n",
        "    min_length = min(len(df) for df in dfs)\n",
        "    print(f\"\\nSample size after alignment: {min_length}\")\n",
        "\n",
        "    actual_values = dfs[0]['actual'].iloc[:min_length].values\n",
        "    predictions = {}\n",
        "\n",
        "    for df, name in zip(dfs, model_names):\n",
        "        predictions[name] = df['predicted'].iloc[:min_length].values\n",
        "\n",
        "    print(f\"Models included: {len(model_names)}\")\n",
        "\n",
        "    # Run DM tests\n",
        "    dm_matrix, p_matrix = run_pairwise_dm_tests(actual_values, predictions)\n",
        "\n",
        "    # Format results\n",
        "    formatted_dm = format_dm_table(dm_matrix, p_matrix)\n",
        "    formatted_p = format_pvalue_table(p_matrix)\n",
        "\n",
        "    print(\"\\nDM TEST STATISTICS:\")\n",
        "    print(\"Interpretation:\")\n",
        "    print(\"- DM > 0: Row model has HIGHER MSE (worse) than Column model\")\n",
        "    print(\"- DM < 0: Row model has LOWER MSE (better) than Column model\")\n",
        "    print(\"- Significance: *** p<0.001, ** p<0.01, * p<0.05\")\n",
        "    print(\"- p > 0.05: No significant difference (accept H₀)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(formatted_dm.to_string())\n",
        "\n",
        "    print(\"\\n\\nP-VALUES:\")\n",
        "    print(\"- p < 0.05: Reject H₀ (significant difference in accuracy)\")\n",
        "    print(\"- p ≥ 0.05: Accept H₀ (no significant difference)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(formatted_p.to_string())\n",
        "\n",
        "    # Save the DM test statistics results to a CSV file\n",
        "    formatted_dm.to_csv(\"dm_statistics_formatted.csv\", index=True)\n",
        "    p_matrix.to_csv(\"dm_pvalues_formatted.csv\", index=True)\n",
        "\n",
        "    # Create summary of best performing models\n",
        "    mse_performance = {}\n",
        "    for name, preds in predictions.items():\n",
        "        mse = np.mean((actual_values - preds)**2)\n",
        "        mse_performance[name] = mse\n",
        "\n",
        "    # Summary of significant outperformances\n",
        "    print(\"\\n\\nSUMMARY OF SIGNIFICANT OUTPERFORMANCES:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    significant_better = {}\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        significant_better[model_a] = []\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:\n",
        "                dm_stat = dm_matrix.iloc[i, j]\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "\n",
        "                # Model A significantly better than Model B (DM < 0 and p < 0.05)\n",
        "                if not np.isnan(p_val) and p_val < 0.05 and dm_stat < 0:\n",
        "                    significant_better[model_a].append(model_b)\n",
        "\n",
        "    for model, beaten_models in significant_better.items():\n",
        "        if beaten_models:\n",
        "            print(f\"{model} significantly outperforms: {', '.join(beaten_models[:5])}\")\n",
        "            if len(beaten_models) > 5:\n",
        "                print(f\"   ... and {len(beaten_models)-5} more models\")\n",
        "\n",
        "    # Create and display heatmap\n",
        "    print(\"\\nGenerating DM test heatmap...\")\n",
        "    create_dm_heatmap(p_matrix)\n",
        "\n",
        "    print(f\"\\nResults saved:\")\n",
        "    print(\"- dm_statistics252.csv\")\n",
        "    print(\"- dm_pvalues.csv\")\n",
        "    print(\"- dm_statistics_formatted252.csv\")\n",
        "    print(\"- dm_pvalues_formatted.csv\")\n",
        "    print(\"- dm_test_heatmap_fixed.png\")\n",
        "\n",
        "    return dm_matrix, p_matrix\n",
        "\n",
        "def run_pairwise_dm_tests(actual, predictions_dict):\n",
        "    \"\"\"\n",
        "    Run pairwise DM tests for all model combinations\n",
        "    \"\"\"\n",
        "    model_names = list(predictions_dict.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Initialize matrices\n",
        "    dm_stats = np.full((n_models, n_models), np.nan)\n",
        "    p_values = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    print(f\"Running pairwise DM tests...\")\n",
        "\n",
        "    # Pairwise comparisons\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:  # Skip diagonal\n",
        "                pred1 = predictions_dict[model_a]\n",
        "                pred2 = predictions_dict[model_b]\n",
        "\n",
        "                dm_stat, p_value = diebold_mariano_test(actual, pred1, pred2)\n",
        "\n",
        "                dm_stats[i, j] = dm_stat\n",
        "                p_values[i, j] = p_value\n",
        "\n",
        "    # Create DataFrames\n",
        "    dm_df = pd.DataFrame(dm_stats, index=model_names, columns=model_names)\n",
        "    p_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
        "\n",
        "    return dm_df, p_df\n",
        "\n",
        "def create_dm_heatmap(p_matrix, title=\"Diebold-Mariano Test P-Values\"):\n",
        "    \"\"\"\n",
        "    Create heatmap with black diagonal X marks and pure black for p >= 0.05\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Prepare data matrix\n",
        "    n_models = len(p_matrix)\n",
        "    heatmap_data = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    # Fill data: 0.051 for diagonal and p>=0.05, actual p-values for p<0.05\n",
        "    for i in range(n_models):\n",
        "        for j in range(n_models):\n",
        "            if i == j:\n",
        "                heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "            else:\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "                if np.isnan(p_val) or p_val >= 0.05:\n",
        "                    heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "                else:\n",
        "                    heatmap_data[i, j] = p_val\n",
        "\n",
        "    # Create discrete colormap with explicit black at the end\n",
        "    colors = ['#006400', '#007400', '#009900', '#00CC00', '#33FF33', '#66FF66', '#99FF99',\n",
        "              '#CCFF99', '#FFFF66', '#FFCC33', '#FF9900', '#FF6600', '#FF3300', '#CC0000', '#000000']\n",
        "\n",
        "    # Create boundaries for discrete colors\n",
        "    boundaries = np.linspace(0, 0.05, len(colors)-1).tolist() + [0.051, 1.0]\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "    norm = mcolors.BoundaryNorm(boundaries, len(colors))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm, aspect='equal', interpolation='nearest')\n",
        "\n",
        "    # Add white X marks on diagonal\n",
        "    for i in range(n_models):\n",
        "        ax.text(i, i, 'X', ha='center', va='center',\n",
        "                color='white', fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Add white grid lines\n",
        "    for i in range(n_models + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=1.0)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=1.0)\n",
        "\n",
        "    # Set labels\n",
        "    model_names = p_matrix.columns.tolist()\n",
        "    ax.set_xticks(range(n_models))\n",
        "    ax.set_yticks(range(n_models))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11, color='black')\n",
        "    ax.set_yticklabels(model_names, fontsize=11, color='black')\n",
        "\n",
        "    # Add title\n",
        "    ax.set_title(title + '\\n(Window 252)', fontsize=18, fontweight='bold', color='black', pad=30)\n",
        "\n",
        "    # Create colorbar with discrete ticks including black\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.7, aspect=25, pad=0.02, boundaries=boundaries, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.051])\n",
        "    cbar.set_label('P-Value', fontsize=14, color='black', labelpad=15)\n",
        "    cbar.ax.tick_params(labelsize=12, colors='black')\n",
        "\n",
        "    # Set colorbar labels with black indicator\n",
        "    cbar_labels = ['0.00', '0.01', '0.02', '0.03', '0.04', '0.05', '≥0.05']\n",
        "    cbar.set_ticklabels(cbar_labels)\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove tick marks but keep labels\n",
        "    ax.tick_params(length=0, colors='black')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(bottom=0.08, top=0.92, left=0.1, right=0.88)\n",
        "\n",
        "    plt.savefig('dm_test_heatmap_fixed.png', dpi=300, bbox_inches='tight',\n",
        "                facecolor='white', edgecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    dm_df, p_df = run_dm_test_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYA7xsnD_4wj"
      },
      "source": [
        "# Window Size 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nDGQoi0f_7Hi"
      },
      "outputs": [],
      "source": [
        "def diebold_mariano_test(actual, pred1, pred2, h=1):\n",
        "    \"\"\"\n",
        "    Diebold-Mariano test using Mean Squared Error loss function with Newey-West\n",
        "\n",
        "    H0: E[dt] = 0 (equal prediction accuracy)\n",
        "    H1: E[dt] ≠ 0 (unequal prediction accuracy)\n",
        "\n",
        "    Test statistic: DM = d̄ / σ̂_d̄ ~ N(0,1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Align arrays\n",
        "    min_len = min(len(actual), len(pred1), len(pred2))\n",
        "    actual = np.array(actual)[:min_len]\n",
        "    pred1 = np.array(pred1)[:min_len]\n",
        "    pred2 = np.array(pred2)[:min_len]\n",
        "\n",
        "    # Remove missing values\n",
        "    mask = ~(np.isnan(actual) | np.isnan(pred1) | np.isnan(pred2))\n",
        "    actual = actual[mask]\n",
        "    pred1 = pred1[mask]\n",
        "    pred2 = pred2[mask]\n",
        "\n",
        "    if len(actual) < 10:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Calculate squared forecast errors\n",
        "    e1_squared = (actual - pred1)**2\n",
        "    e2_squared = (actual - pred2)**2\n",
        "\n",
        "    # Loss differential: dt = e²1,t - e²2,t\n",
        "    d = e1_squared - e2_squared\n",
        "    d_mean = np.mean(d)\n",
        "    n = len(d)\n",
        "\n",
        "    # Newey-West variance estimator\n",
        "    def newey_west_variance(d, h):\n",
        "        n = len(d)\n",
        "        d_centered = d - np.mean(d)\n",
        "\n",
        "        # Bandwidth selection\n",
        "        bandwidth = max(1, int(4 * (n/100)**(2/9)))\n",
        "\n",
        "        # Calculate autocovariances with Bartlett kernel\n",
        "        gamma_0 = np.mean(d_centered**2)\n",
        "        gamma_sum = 0\n",
        "\n",
        "        for k in range(1, min(bandwidth + 1, n)):\n",
        "            if k < n:\n",
        "                weight = 1 - k / (bandwidth + 1)\n",
        "                gamma_k = np.mean(d_centered[k:] * d_centered[:-k])\n",
        "                gamma_sum += 2 * weight * gamma_k\n",
        "\n",
        "        return gamma_0 + gamma_sum\n",
        "\n",
        "    # Calculate long-run variance\n",
        "    long_run_var = newey_west_variance(d, h)\n",
        "\n",
        "    if long_run_var <= 0:\n",
        "        long_run_var = np.var(d, ddof=1)\n",
        "        if long_run_var <= 0:\n",
        "            return np.nan, np.nan\n",
        "\n",
        "    # DM statistic\n",
        "    dm_stat = d_mean / np.sqrt(long_run_var / n)\n",
        "\n",
        "    if np.isnan(dm_stat) or np.isinf(dm_stat):\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Two-tailed p-value\n",
        "    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
        "\n",
        "    return dm_stat, p_value\n",
        "\n",
        "\n",
        "def run_pairwise_dm_tests(actual, predictions_dict):\n",
        "    \"\"\"\n",
        "    Run pairwise DM tests for all model combinations\n",
        "    \"\"\"\n",
        "    model_names = list(predictions_dict.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Initialize matrices\n",
        "    dm_stats = np.full((n_models, n_models), np.nan)\n",
        "    p_values = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    print(f\"Running pairwise DM tests...\")\n",
        "\n",
        "    # Pairwise comparisons\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:  # Skip diagonal\n",
        "                pred1 = predictions_dict[model_a]\n",
        "                pred2 = predictions_dict[model_b]\n",
        "\n",
        "                dm_stat, p_value = diebold_mariano_test(actual, pred1, pred2)\n",
        "\n",
        "                dm_stats[i, j] = dm_stat\n",
        "                p_values[i, j] = p_value\n",
        "\n",
        "    # Create DataFrames\n",
        "    dm_df = pd.DataFrame(dm_stats, index=model_names, columns=model_names)\n",
        "    p_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
        "\n",
        "    return dm_df, p_df\n",
        "\n",
        "\n",
        "def format_pvalue_table(p_df):\n",
        "    \"\"\"Format p-values with significance indicators\"\"\"\n",
        "    formatted = p_df.copy().astype(object)\n",
        "\n",
        "    for i in range(len(p_df)):\n",
        "        for j in range(len(p_df.columns)):\n",
        "            if i == j:\n",
        "                formatted.iloc[i, j] = \"-\"\n",
        "            elif i > j:  # Lower triangle - leave empty\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "            elif not np.isnan(p_df.iloc[i, j]):\n",
        "                p_val = p_df.iloc[i, j]\n",
        "\n",
        "                if p_val < 0.001:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}***\"\n",
        "                elif p_val < 0.01:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}**\"\n",
        "                elif p_val < 0.05:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}*\"\n",
        "                else:\n",
        "                    formatted.iloc[i, j] = f\"{p_val:.6f}\"\n",
        "            else:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def format_dm_table(dm_df, p_df):\n",
        "    \"\"\"Format DM statistics table with significance indicators based on p-values\"\"\"\n",
        "    formatted = dm_df.copy().astype(object)\n",
        "\n",
        "    for i in range(len(dm_df)):\n",
        "        for j in range(len(dm_df.columns)):\n",
        "            if i == j:\n",
        "                formatted.iloc[i, j] = \"-\"\n",
        "            elif i > j:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "            elif not (np.isnan(dm_df.iloc[i, j]) or np.isnan(p_df.iloc[i, j])):\n",
        "                dm_val = dm_df.iloc[i, j]\n",
        "                p_val = p_df.iloc[i, j]\n",
        "\n",
        "                # Add significance stars based on p-values\n",
        "                if p_val < 0.001:\n",
        "                    stars = \"***\"\n",
        "                elif p_val < 0.01:\n",
        "                    stars = \"**\"\n",
        "                elif p_val < 0.05:\n",
        "                    stars = \"*\"\n",
        "                else:\n",
        "                    stars = \"\"\n",
        "\n",
        "                formatted.iloc[i, j] = f\"{dm_val:.4f}{stars}\"\n",
        "            else:\n",
        "                formatted.iloc[i, j] = \"\"\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def create_dm_heatmap(p_matrix, title=\"Diebold-Mariano Test P-Values\"):\n",
        "    \"\"\"\n",
        "    Create heatmap with black diagonal X marks and pure black for p >= 0.05\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Prepare data matrix\n",
        "    n_models = len(p_matrix)\n",
        "    heatmap_data = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    # Fill data: 0.051 for diagonal and p>=0.05, actual p-values for p<0.05\n",
        "    for i in range(n_models):\n",
        "        for j in range(n_models):\n",
        "            if i == j:\n",
        "                heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "            else:\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "                if np.isnan(p_val) or p_val >= 0.05:\n",
        "                    heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "                else:\n",
        "                    heatmap_data[i, j] = p_val  # Keep actual p-value for coloring\n",
        "\n",
        "    # Create discrete colormap with explicit black at the end\n",
        "    colors = ['#003300', '#006600', '#009900', '#00CC00', '#33FF33', '#66FF66', '#99FF99',\n",
        "              '#CCFF99', '#FFFF66', '#FFCC33', '#FF9900', '#FF6600', '#FF3300', '#CC0000', '#000000']\n",
        "\n",
        "    # Create boundaries for discrete colors\n",
        "    boundaries = np.linspace(0, 0.05, len(colors)-1).tolist() + [0.051, 1.0]\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "    norm = mcolors.BoundaryNorm(boundaries, len(colors))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm, aspect='equal', interpolation='nearest')\n",
        "\n",
        "    # Add white X marks on diagonal\n",
        "    for i in range(n_models):\n",
        "        ax.text(i, i, 'X', ha='center', va='center',\n",
        "                color='white', fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Add white grid lines\n",
        "    for i in range(n_models + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=1.0)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=1.0)\n",
        "\n",
        "    # Set labels\n",
        "    model_names = p_matrix.columns.tolist()\n",
        "    ax.set_xticks(range(n_models))\n",
        "    ax.set_yticks(range(n_models))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11, color='black')\n",
        "    ax.set_yticklabels(model_names, fontsize=11, color='black')\n",
        "\n",
        "    # Add title\n",
        "    ax.set_title(title + '\\n(Window 512)', fontsize=18, fontweight='bold', color='black', pad=30)\n",
        "\n",
        "    # Create colorbar with discrete ticks including black\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.7, aspect=25, pad=0.02, boundaries=boundaries, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.051])\n",
        "    cbar.set_label('P-Value', fontsize=14, color='black', labelpad=15)\n",
        "    cbar.ax.tick_params(labelsize=12, colors='black')\n",
        "\n",
        "    # Set colorbar labels with black indicator\n",
        "    cbar_labels = ['0.00', '0.01', '0.02', '0.03', '0.04', '0.05', '≥0.05']\n",
        "    cbar.set_ticklabels(cbar_labels)\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove tick marks but keep labels\n",
        "    ax.tick_params(length=0, colors='black')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(bottom=0.08, top=0.92, left=0.1, right=0.88)\n",
        "\n",
        "    plt.savefig('dm_test_heatmap_fixed.png', dpi=300, bbox_inches='tight',\n",
        "                facecolor='white', edgecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def run_dm_test_analysis():\n",
        "    \"\"\"\n",
        "    Load data and run DM tests for all model pairs\n",
        "    \"\"\"\n",
        "\n",
        "    # Input folder: DM_Test_Data\n",
        "    DATA_DIR = os.path.join(os.getcwd(), \"DM_Test_Data\")\n",
        "    \n",
        "    # Model files\n",
        "    model_files = {\n",
        "        \"OLS\": \"ols_results_lag512_full.csv\",\n",
        "        \"LASSO\": \"lasso_results_lag512_full.csv\",\n",
        "        \"Ridge\": \"ridge_results_lag512_full.csv\",\n",
        "        \"ElasticNet\": \"enet_results_lag512_full.csv\",\n",
        "        \"PCR\": \"pcr_results_lag512_full.csv\",\n",
        "        \"GLM\": \"glm_results_lag512_full.csv\",\n",
        "        \"RF\": \"rf_results_lag512_full.csv\",\n",
        "        \"GBRT\": \"gbrt_results_lag512_full.csv\",\n",
        "        \"NN1\": \"nn1_results_lag512_full.csv\",\n",
        "        \"NN2\": \"nn2_results_lag512_full.csv\",\n",
        "        \"NN3\": \"nn3_results_lag512_full.csv\",\n",
        "        \"NN4\": \"nn4_results_lag512_full.csv\",\n",
        "        \"NN5\": \"nn5_results_lag512_full.csv\",\n",
        "        \"Chronos-T5 Tiny\": \"chronost5tiny_results_lag512_full.csv\",\n",
        "        \"Chronos-Bolt Tiny\": \"chronosbolttiny_results_lag512_full.csv\",\n",
        "        \"Chronos-Bolt Mini\": \"chronosboltmini_results_lag512_full.csv\",\n",
        "        \"Chronos-Bolt Small\": \"chronosboltsmall_results_lag512_full.csv\",\n",
        "        \"TimesFM 1.0-200M\": \"timesfm1_results_lag512_full.csv\",\n",
        "        \"TimesFM 2.0-500M\": \"timesfm2_results_lag512_full.csv\",\n",
        "        \"Moirai Small\": \"uni2tssmall_results_lag512_full.csv\",\n",
        "        \"Moirai-Moe Small\": \"uni2tssmallmoe_results_lag512_full.csv\",\n",
        "        \"Moirai-Moe Base\": \"uni2tsbasemoe_results_lag512_full.csv\"\n",
        "    }\n",
        "\n",
        "    print(\"DIEBOLD-MARIANO TEST\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Loss Function: Mean Squared Error (MSE)\")\n",
        "    print(\"H₀: E[dₜ] = 0 (equal prediction accuracy)\")\n",
        "    print(\"H₁: E[dₜ] ≠ 0 (unequal prediction accuracy)\")\n",
        "    print(\"Variance Estimator: Newey-West with automatic bandwidth\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load and align data\n",
        "    dfs = []\n",
        "    model_names = []\n",
        "\n",
        "    for model_name, file_name in model_files.items():\n",
        "        file_path = os.path.join(DATA_DIR, file_name)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Standardize column names\n",
        "            if 'excess_ret' in df.columns:\n",
        "                df = df.rename(columns={'excess_ret': 'actual'})\n",
        "            elif 'y_true' in df.columns:\n",
        "                df = df.rename(columns={'y_true': 'actual'})\n",
        "\n",
        "            if 'y_pred' in df.columns:\n",
        "                df = df.rename(columns={'y_pred': 'predicted'})\n",
        "            elif 'predicted_excess_returns_lag512' in df.columns:\n",
        "                df = df.rename(columns={'predicted_excess_returns_lag512': 'predicted'})\n",
        "\n",
        "            dfs.append(df[['actual', 'predicted']])\n",
        "            model_names.append(model_name)\n",
        "            print(f\"Loaded: {model_name}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: {file_path} not found, skipping {model_name}\")\n",
        "\n",
        "    # Align to common length\n",
        "    min_length = min(len(df) for df in dfs)\n",
        "    print(f\"\\nSample size after alignment: {min_length}\")\n",
        "\n",
        "    actual_values = dfs[0]['actual'].iloc[:min_length].values\n",
        "    predictions = {}\n",
        "\n",
        "    for df, name in zip(dfs, model_names):\n",
        "        predictions[name] = df['predicted'].iloc[:min_length].values\n",
        "\n",
        "    print(f\"Models included: {len(model_names)}\")\n",
        "\n",
        "    # Run DM tests\n",
        "    dm_matrix, p_matrix = run_pairwise_dm_tests(actual_values, predictions)\n",
        "\n",
        "    # Format results\n",
        "    formatted_dm = format_dm_table(dm_matrix, p_matrix)\n",
        "    formatted_p = format_pvalue_table(p_matrix)\n",
        "\n",
        "    print(\"\\nDM TEST STATISTICS:\")\n",
        "    print(\"Interpretation:\")\n",
        "    print(\"- DM > 0: Row model has HIGHER MSE (worse) than Column model\")\n",
        "    print(\"- DM < 0: Row model has LOWER MSE (better) than Column model\")\n",
        "    print(\"- Significance: *** p<0.001, ** p<0.01, * p<0.05\")\n",
        "    print(\"- p > 0.05: No significant difference (accept H₀)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(formatted_dm.to_string())\n",
        "\n",
        "    print(\"\\n\\nP-VALUES:\")\n",
        "    print(\"- p < 0.05: Reject H₀ (significant difference in accuracy)\")\n",
        "    print(\"- p ≥ 0.05: Accept H₀ (no significant difference)\")\n",
        "    print(\"-\" * 80)\n",
        "    print(formatted_p.to_string())\n",
        "\n",
        "    # Save the DM test statistics results to a CSV file\n",
        "    formatted_dm.to_csv(\"dm_statistics_formatted.csv\", index=True)\n",
        "    p_matrix.to_csv(\"dm_pvalues_formatted.csv\", index=True)\n",
        "\n",
        "    # Create summary of best performing models\n",
        "    mse_performance = {}\n",
        "    for name, preds in predictions.items():\n",
        "        mse = np.mean((actual_values - preds)**2)\n",
        "        mse_performance[name] = mse\n",
        "\n",
        "    # Summary of significant outperformances\n",
        "    print(\"\\n\\nSUMMARY OF SIGNIFICANT OUTPERFORMANCES:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    significant_better = {}\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        significant_better[model_a] = []\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:\n",
        "                dm_stat = dm_matrix.iloc[i, j]\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "\n",
        "                # Model A significantly better than Model B (DM < 0 and p < 0.05)\n",
        "                if not np.isnan(p_val) and p_val < 0.05 and dm_stat < 0:\n",
        "                    significant_better[model_a].append(model_b)\n",
        "\n",
        "    for model, beaten_models in significant_better.items():\n",
        "        if beaten_models:\n",
        "            print(f\"{model} significantly outperforms: {', '.join(beaten_models[:5])}\")\n",
        "            if len(beaten_models) > 5:\n",
        "                print(f\"   ... and {len(beaten_models)-5} more models\")\n",
        "\n",
        "    # Create and display heatmap\n",
        "    print(\"\\nGenerating DM test heatmap...\")\n",
        "    create_dm_heatmap(p_matrix)\n",
        "\n",
        "    print(f\"\\nResults saved:\")\n",
        "    print(\"- dm_statistics512.csv\")\n",
        "    print(\"- dm_pvalues.csv\")\n",
        "    print(\"- dm_statistics_formatted512.csv\")\n",
        "    print(\"- dm_pvalues_formatted.csv\")\n",
        "    print(\"- dm_test_heatmap_fixed.png\")\n",
        "\n",
        "    return dm_matrix, p_matrix\n",
        "\n",
        "def run_pairwise_dm_tests(actual, predictions_dict):\n",
        "    \"\"\"\n",
        "    Run pairwise DM tests for all model combinations\n",
        "    \"\"\"\n",
        "    model_names = list(predictions_dict.keys())\n",
        "    n_models = len(model_names)\n",
        "\n",
        "    # Initialize matrices\n",
        "    dm_stats = np.full((n_models, n_models), np.nan)\n",
        "    p_values = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    print(f\"Running pairwise DM tests...\")\n",
        "\n",
        "    # Pairwise comparisons\n",
        "    for i, model_a in enumerate(model_names):\n",
        "        for j, model_b in enumerate(model_names):\n",
        "            if i != j:  # Skip diagonal\n",
        "                pred1 = predictions_dict[model_a]\n",
        "                pred2 = predictions_dict[model_b]\n",
        "\n",
        "                dm_stat, p_value = diebold_mariano_test(actual, pred1, pred2)\n",
        "\n",
        "                dm_stats[i, j] = dm_stat\n",
        "                p_values[i, j] = p_value\n",
        "\n",
        "    # Create DataFrames\n",
        "    dm_df = pd.DataFrame(dm_stats, index=model_names, columns=model_names)\n",
        "    p_df = pd.DataFrame(p_values, index=model_names, columns=model_names)\n",
        "\n",
        "    return dm_df, p_df\n",
        "\n",
        "def create_dm_heatmap(p_matrix, title=\"Diebold-Mariano Test P-Values\"):\n",
        "    \"\"\"\n",
        "    Create heatmap with black diagonal X marks and pure black for p >= 0.05\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.colors as mcolors\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    fig.patch.set_facecolor('white')\n",
        "    ax.set_facecolor('white')\n",
        "\n",
        "    # Prepare data matrix\n",
        "    n_models = len(p_matrix)\n",
        "    heatmap_data = np.full((n_models, n_models), np.nan)\n",
        "\n",
        "    # Fill data: 0.051 for diagonal and p>=0.05, actual p-values for p<0.05\n",
        "    for i in range(n_models):\n",
        "        for j in range(n_models):\n",
        "            if i == j:\n",
        "                heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "            else:\n",
        "                p_val = p_matrix.iloc[i, j]\n",
        "                if np.isnan(p_val) or p_val >= 0.05:\n",
        "                    heatmap_data[i, j] = 0.051  # Slightly above 0.05 for black\n",
        "                else:\n",
        "                    heatmap_data[i, j] = p_val\n",
        "\n",
        "    # Create discrete colormap with explicit black at the end\n",
        "    colors = ['#006400', '#007400', '#009900', '#00CC00', '#33FF33', '#66FF66', '#99FF99',\n",
        "              '#CCFF99', '#FFFF66', '#FFCC33', '#FF9900', '#FF6600', '#FF3300', '#CC0000', '#000000']\n",
        "\n",
        "    # Create boundaries for discrete colors\n",
        "    boundaries = np.linspace(0, 0.05, len(colors)-1).tolist() + [0.051, 1.0]\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "    norm = mcolors.BoundaryNorm(boundaries, len(colors))\n",
        "\n",
        "    # Plot heatmap\n",
        "    im = ax.imshow(heatmap_data, cmap=cmap, norm=norm, aspect='equal', interpolation='nearest')\n",
        "\n",
        "    # Add white X marks on diagonal\n",
        "    for i in range(n_models):\n",
        "        ax.text(i, i, 'X', ha='center', va='center',\n",
        "                color='white', fontsize=18, fontweight='bold')\n",
        "\n",
        "    # Add white grid lines\n",
        "    for i in range(n_models + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=1.0)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=1.0)\n",
        "\n",
        "    # Set labels\n",
        "    model_names = p_matrix.columns.tolist()\n",
        "    ax.set_xticks(range(n_models))\n",
        "    ax.set_yticks(range(n_models))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right', fontsize=11, color='black')\n",
        "    ax.set_yticklabels(model_names, fontsize=11, color='black')\n",
        "\n",
        "    # Add title\n",
        "    ax.set_title(title + '\\n(Window 512)', fontsize=18, fontweight='bold', color='black', pad=30)\n",
        "\n",
        "    # Create colorbar with discrete ticks including black\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.7, aspect=25, pad=0.02, boundaries=boundaries, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.051])\n",
        "    cbar.set_label('P-Value', fontsize=14, color='black', labelpad=15)\n",
        "    cbar.ax.tick_params(labelsize=12, colors='black')\n",
        "\n",
        "    # Set colorbar labels with black indicator\n",
        "    cbar_labels = ['0.00', '0.01', '0.02', '0.03', '0.04', '0.05', '≥0.05']\n",
        "    cbar.set_ticklabels(cbar_labels)\n",
        "\n",
        "    # Remove spines\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "\n",
        "    # Remove tick marks but keep labels\n",
        "    ax.tick_params(length=0, colors='black')\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.subplots_adjust(bottom=0.08, top=0.92, left=0.1, right=0.88)\n",
        "\n",
        "    plt.savefig('dm_test_heatmap_fixed.png', dpi=300, bbox_inches='tight',\n",
        "                facecolor='white', edgecolor='none')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    dm_df, p_df = run_dm_test_analysis()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GnQoZyUOr-Bz",
        "JNbi4FHD_H_Q",
        "wYA7xsnD_4wj"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
